<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Minicurso de verano de Aprendizaje Máquina</title>
  <meta name="description" content="Minicurso de Aprendizaje Máquina, ITAM 2018.">
  <meta name="generator" content="bookdown 0.7.12 and GitBook 2.6.7">

  <meta property="og:title" content="Minicurso de verano de Aprendizaje Máquina" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Minicurso de Aprendizaje Máquina, ITAM 2018." />
  <meta name="github-repo" content="felipegonzalez/aprendizaje-maquina-verano-2018" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Minicurso de verano de Aprendizaje Máquina" />
  
  <meta name="twitter:description" content="Minicurso de Aprendizaje Máquina, ITAM 2018." />
  

<meta name="author" content="Felipe González">


<meta name="date" content="2018-06-11">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="regresion-lineal.html">
<link rel="next" href="regresion-regularizada.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.2/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.7.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotlyjs-1.29.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotlyjs-1.29.2/plotly-latest.min.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="toc.css" type="text/css" />
<link rel="stylesheet" href="font-awesome.min.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Minicurso aprendizaje máquina</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Temario</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#referencias"><i class="fa fa-check"></i>Referencias</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#software"><i class="fa fa-check"></i>Software</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduccion.html"><a href="introduccion.html"><i class="fa fa-check"></i><b>1</b> Introducción</a><ul>
<li class="chapter" data-level="1.1" data-path="introduccion.html"><a href="introduccion.html#que-es-aprendizaje-de-maquina-machine-learning"><i class="fa fa-check"></i><b>1.1</b> ¿Qué es aprendizaje de máquina (machine learning)?</a></li>
<li class="chapter" data-level="1.2" data-path="introduccion.html"><a href="introduccion.html#aprendizaje-supervisado"><i class="fa fa-check"></i><b>1.2</b> Aprendizaje Supervisado</a><ul>
<li class="chapter" data-level="1.2.1" data-path="introduccion.html"><a href="introduccion.html#proceso-generador-de-datos-modelo-teorico"><i class="fa fa-check"></i><b>1.2.1</b> Proceso generador de datos (modelo teórico)</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="introduccion.html"><a href="introduccion.html#predicciones"><i class="fa fa-check"></i><b>1.3</b> Predicciones</a></li>
<li class="chapter" data-level="1.4" data-path="introduccion.html"><a href="introduccion.html#cuantificacion-de-error-o-precision"><i class="fa fa-check"></i><b>1.4</b> Cuantificación de error o precisión</a></li>
<li class="chapter" data-level="1.5" data-path="introduccion.html"><a href="introduccion.html#aprendizaje"><i class="fa fa-check"></i><b>1.5</b> Tarea de aprendizaje supervisado</a><ul>
<li class="chapter" data-level="" data-path="introduccion.html"><a href="introduccion.html#observaciones"><i class="fa fa-check"></i>Observaciones</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="introduccion.html"><a href="introduccion.html#por-que-tenemos-errores"><i class="fa fa-check"></i><b>1.6</b> ¿Por qué tenemos errores?</a></li>
<li class="chapter" data-level="1.7" data-path="introduccion.html"><a href="introduccion.html#resumen"><i class="fa fa-check"></i><b>1.7</b> Resumen</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="regresion-lineal.html"><a href="regresion-lineal.html"><i class="fa fa-check"></i><b>2</b> Regresión lineal</a><ul>
<li class="chapter" data-level="2.1" data-path="regresion-lineal.html"><a href="regresion-lineal.html#introduccion-1"><i class="fa fa-check"></i><b>2.1</b> Introducción</a></li>
<li class="chapter" data-level="2.2" data-path="regresion-lineal.html"><a href="regresion-lineal.html#aprendizaje-de-coeficientes-ajuste"><i class="fa fa-check"></i><b>2.2</b> Aprendizaje de coeficientes (ajuste)</a></li>
<li class="chapter" data-level="2.3" data-path="regresion-lineal.html"><a href="regresion-lineal.html#descenso-en-gradiente"><i class="fa fa-check"></i><b>2.3</b> Descenso en gradiente</a><ul>
<li class="chapter" data-level="2.3.1" data-path="regresion-lineal.html"><a href="regresion-lineal.html#seleccion-de-tamano-de-paso-eta"><i class="fa fa-check"></i><b>2.3.1</b> Selección de tamaño de paso <span class="math inline">\(\eta\)</span></a></li>
<li class="chapter" data-level="2.3.2" data-path="regresion-lineal.html"><a href="regresion-lineal.html#funciones-de-varias-variables"><i class="fa fa-check"></i><b>2.3.2</b> Funciones de varias variables</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="regresion-lineal.html"><a href="regresion-lineal.html#descenso-en-gradiente-para-regresion-lineal"><i class="fa fa-check"></i><b>2.4</b> Descenso en gradiente para regresión lineal</a></li>
<li class="chapter" data-level="2.5" data-path="regresion-lineal.html"><a href="regresion-lineal.html#normalizacion-de-entradas"><i class="fa fa-check"></i><b>2.5</b> Normalización de entradas</a></li>
<li class="chapter" data-level="2.6" data-path="regresion-lineal.html"><a href="regresion-lineal.html#interpretacion-de-modelos-lineales"><i class="fa fa-check"></i><b>2.6</b> Interpretación de modelos lineales</a></li>
<li class="chapter" data-level="2.7" data-path="regresion-lineal.html"><a href="regresion-lineal.html#por-que-el-modelo-lineal-funciona-bien-muchas-veces"><i class="fa fa-check"></i><b>2.7</b> ¿Por qué el modelo lineal funciona bien (muchas veces)?</a><ul>
<li class="chapter" data-level="2.7.1" data-path="regresion-lineal.html"><a href="regresion-lineal.html#k-vecinos-mas-cercanos"><i class="fa fa-check"></i><b>2.7.1</b> k vecinos más cercanos</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="regresion-lineal.html"><a href="regresion-lineal.html#ejercicio-1"><i class="fa fa-check"></i>Ejercicio</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="regresion-logistica.html"><a href="regresion-logistica.html"><i class="fa fa-check"></i><b>3</b> Regresión logística</a><ul>
<li class="chapter" data-level="3.1" data-path="regresion-logistica.html"><a href="regresion-logistica.html#el-problema-de-clasificacion"><i class="fa fa-check"></i><b>3.1</b> El problema de clasificación</a><ul>
<li class="chapter" data-level="" data-path="regresion-logistica.html"><a href="regresion-logistica.html#que-estimar-en-problemas-de-clasificacion"><i class="fa fa-check"></i>¿Qué estimar en problemas de clasificación?</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="regresion-logistica.html"><a href="regresion-logistica.html#estimacion-de-probabilidades-de-clase"><i class="fa fa-check"></i><b>3.2</b> Estimación de probabilidades de clase</a><ul>
<li class="chapter" data-level="" data-path="regresion-logistica.html"><a href="regresion-logistica.html#ejemplo-9"><i class="fa fa-check"></i>Ejemplo</a></li>
<li class="chapter" data-level="3.2.1" data-path="regresion-logistica.html"><a href="regresion-logistica.html#k-vecinos-mas-cercanos-1"><i class="fa fa-check"></i><b>3.2.1</b> k-vecinos más cercanos</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="regresion-logistica.html"><a href="regresion-logistica.html#error-para-modelos-de-clasificacion"><i class="fa fa-check"></i><b>3.3</b> Error para modelos de clasificación</a><ul>
<li class="chapter" data-level="3.3.1" data-path="regresion-logistica.html"><a href="regresion-logistica.html#ejercicio-2"><i class="fa fa-check"></i><b>3.3.1</b> Ejercicio</a></li>
<li class="chapter" data-level="3.3.2" data-path="regresion-logistica.html"><a href="regresion-logistica.html#error-de-clasificacion-y-funcion-de-perdida-0-1"><i class="fa fa-check"></i><b>3.3.2</b> Error de clasificación y función de pérdida 0-1</a></li>
<li class="chapter" data-level="3.3.3" data-path="regresion-logistica.html"><a href="regresion-logistica.html#discusion-relacion-entre-devianza-y-error-de-clasificacion"><i class="fa fa-check"></i><b>3.3.3</b> Discusión: relación entre devianza y error de clasificación</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="regresion-logistica.html"><a href="regresion-logistica.html#regresion-logistica-1"><i class="fa fa-check"></i><b>3.4</b> Regresión logística</a><ul>
<li class="chapter" data-level="3.4.1" data-path="regresion-logistica.html"><a href="regresion-logistica.html#regresion-logistica-simple"><i class="fa fa-check"></i><b>3.4.1</b> Regresión logística simple</a></li>
<li class="chapter" data-level="3.4.2" data-path="regresion-logistica.html"><a href="regresion-logistica.html#funcion-logistica"><i class="fa fa-check"></i><b>3.4.2</b> Función logística</a></li>
<li class="chapter" data-level="3.4.3" data-path="regresion-logistica.html"><a href="regresion-logistica.html#regresion-logistica-2"><i class="fa fa-check"></i><b>3.4.3</b> Regresión logística</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="regresion-logistica.html"><a href="regresion-logistica.html#aprendizaje-de-coeficientes-para-regresion-logistica-binomial."><i class="fa fa-check"></i><b>3.5</b> Aprendizaje de coeficientes para regresión logística (binomial).</a></li>
<li class="chapter" data-level="3.6" data-path="regresion-logistica.html"><a href="regresion-logistica.html#observaciones-adicionales"><i class="fa fa-check"></i><b>3.6</b> Observaciones adicionales</a></li>
<li class="chapter" data-level="" data-path="regresion-logistica.html"><a href="regresion-logistica.html#ejercicio-datos-de-diabetes"><i class="fa fa-check"></i>Ejercicio: datos de diabetes</a></li>
<li class="chapter" data-level="3.7" data-path="regresion-logistica.html"><a href="regresion-logistica.html#mas-sobre-problemas-de-clasificacion"><i class="fa fa-check"></i><b>3.7</b> Más sobre problemas de clasificación</a><ul>
<li class="chapter" data-level="3.7.1" data-path="regresion-logistica.html"><a href="regresion-logistica.html#analisis-de-error-para-clasificadores-binarios"><i class="fa fa-check"></i><b>3.7.1</b> Análisis de error para clasificadores binarios</a></li>
<li class="chapter" data-level="3.7.2" data-path="regresion-logistica.html"><a href="regresion-logistica.html#regresion-logistica-para-problemas-de-mas-de-2-clases"><i class="fa fa-check"></i><b>3.7.2</b> Regresión logística para problemas de más de 2 clases</a></li>
<li class="chapter" data-level="3.7.3" data-path="regresion-logistica.html"><a href="regresion-logistica.html#regresion-logistica-multinomial"><i class="fa fa-check"></i><b>3.7.3</b> Regresión logística multinomial</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="regresion-regularizada.html"><a href="regresion-regularizada.html"><i class="fa fa-check"></i><b>4</b> Regresión regularizada</a><ul>
<li class="chapter" data-level="4.0.1" data-path="regresion-regularizada.html"><a href="regresion-regularizada.html#sesgo-y-varianza-en-modelos-lineales"><i class="fa fa-check"></i><b>4.0.1</b> Sesgo y varianza en modelos lineales</a></li>
<li class="chapter" data-level="4.0.2" data-path="regresion-regularizada.html"><a href="regresion-regularizada.html#reduciendo-varianza-de-los-coeficientes"><i class="fa fa-check"></i><b>4.0.2</b> Reduciendo varianza de los coeficientes</a></li>
<li class="chapter" data-level="4.1" data-path="regresion-regularizada.html"><a href="regresion-regularizada.html#regularizacion-ridge"><i class="fa fa-check"></i><b>4.1</b> Regularización ridge</a><ul>
<li class="chapter" data-level="4.1.1" data-path="regresion-regularizada.html"><a href="regresion-regularizada.html#seleccion-de-coeficiente-de-regularizacion"><i class="fa fa-check"></i><b>4.1.1</b> Selección de coeficiente de regularización</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="regresion-regularizada.html"><a href="regresion-regularizada.html#entrenamiento-validacion-y-prueba"><i class="fa fa-check"></i><b>4.2</b> Entrenamiento, Validación y Prueba</a><ul>
<li class="chapter" data-level="4.2.1" data-path="regresion-regularizada.html"><a href="regresion-regularizada.html#validacion-cruzada"><i class="fa fa-check"></i><b>4.2.1</b> Validación cruzada</a></li>
<li class="chapter" data-level="" data-path="regresion-regularizada.html"><a href="regresion-regularizada.html#ejercicio-4"><i class="fa fa-check"></i>Ejercicio</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="regresion-regularizada.html"><a href="regresion-regularizada.html#regularizacion-lasso"><i class="fa fa-check"></i><b>4.3</b> Regularización lasso</a></li>
<li class="chapter" data-level="4.4" data-path="regresion-regularizada.html"><a href="regresion-regularizada.html#tarea"><i class="fa fa-check"></i><b>4.4</b> Tarea</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="descenso-estocastico.html"><a href="descenso-estocastico.html"><i class="fa fa-check"></i><b>5</b> Descenso estocástico</a><ul>
<li class="chapter" data-level="5.1" data-path="descenso-estocastico.html"><a href="descenso-estocastico.html#algoritmo-de-descenso-estocastico"><i class="fa fa-check"></i><b>5.1</b> Algoritmo de descenso estocástico</a></li>
<li class="chapter" data-level="5.2" data-path="descenso-estocastico.html"><a href="descenso-estocastico.html#por-que-usar-descenso-estocastico-por-minilotes"><i class="fa fa-check"></i><b>5.2</b> ¿Por qué usar descenso estocástico por minilotes?</a></li>
<li class="chapter" data-level="5.3" data-path="descenso-estocastico.html"><a href="descenso-estocastico.html#escogiendo-la-tasa-de-aprendizaje"><i class="fa fa-check"></i><b>5.3</b> Escogiendo la tasa de aprendizaje</a></li>
<li class="chapter" data-level="5.4" data-path="descenso-estocastico.html"><a href="descenso-estocastico.html#mejoras-al-algoritmo-de-descenso-estocastico."><i class="fa fa-check"></i><b>5.4</b> Mejoras al algoritmo de descenso estocástico.</a><ul>
<li class="chapter" data-level="5.4.1" data-path="descenso-estocastico.html"><a href="descenso-estocastico.html#decaimiento-de-tasa-de-aprendizaje"><i class="fa fa-check"></i><b>5.4.1</b> Decaimiento de tasa de aprendizaje</a></li>
<li class="chapter" data-level="5.4.2" data-path="descenso-estocastico.html"><a href="descenso-estocastico.html#momento"><i class="fa fa-check"></i><b>5.4.2</b> Momento</a></li>
<li class="chapter" data-level="5.4.3" data-path="descenso-estocastico.html"><a href="descenso-estocastico.html#otras-variaciones"><i class="fa fa-check"></i><b>5.4.3</b> Otras variaciones</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html"><i class="fa fa-check"></i><b>6</b> Diagnóstico y mejora de modelos</a><ul>
<li class="chapter" data-level="6.1" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#aspectos-generales"><i class="fa fa-check"></i><b>6.1</b> Aspectos generales</a></li>
<li class="chapter" data-level="6.2" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#que-hacer-cuando-el-desempeno-no-es-satisfactorio"><i class="fa fa-check"></i><b>6.2</b> ¿Qué hacer cuando el desempeño no es satisfactorio?</a></li>
<li class="chapter" data-level="6.3" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#pipeline-de-procesamiento"><i class="fa fa-check"></i><b>6.3</b> Pipeline de procesamiento</a></li>
<li class="chapter" data-level="6.4" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#diagnosticos-sesgo-y-varianza"><i class="fa fa-check"></i><b>6.4</b> Diagnósticos: sesgo y varianza</a></li>
<li class="chapter" data-level="6.5" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#refinando-el-pipeline"><i class="fa fa-check"></i><b>6.5</b> Refinando el pipeline</a></li>
<li class="chapter" data-level="6.6" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#consiguiendo-mas-datos"><i class="fa fa-check"></i><b>6.6</b> Consiguiendo más datos</a></li>
<li class="chapter" data-level="6.7" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#usar-datos-adicionales"><i class="fa fa-check"></i><b>6.7</b> Usar datos adicionales</a></li>
<li class="chapter" data-level="6.8" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#examen-de-modelo-y-analisis-de-errores"><i class="fa fa-check"></i><b>6.8</b> Examen de modelo y Análisis de errores</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html"><i class="fa fa-check"></i><b>7</b> Métodos basados en árboles</a><ul>
<li class="chapter" data-level="7.1" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#arboles-para-regresion-y-clasificacion."><i class="fa fa-check"></i><b>7.1</b> Árboles para regresión y clasificación.</a><ul>
<li class="chapter" data-level="7.1.1" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#arboles-para-clasificacion"><i class="fa fa-check"></i><b>7.1.1</b> Árboles para clasificación</a></li>
<li class="chapter" data-level="7.1.2" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#tipos-de-particion"><i class="fa fa-check"></i><b>7.1.2</b> Tipos de partición</a></li>
<li class="chapter" data-level="7.1.3" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#medidas-de-impureza"><i class="fa fa-check"></i><b>7.1.3</b> Medidas de impureza</a></li>
<li class="chapter" data-level="7.1.4" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#reglas-de-particion-y-tamano-del-arobl"><i class="fa fa-check"></i><b>7.1.4</b> Reglas de partición y tamaño del árobl</a></li>
<li class="chapter" data-level="7.1.5" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#costo---complejidad-breiman"><i class="fa fa-check"></i><b>7.1.5</b> Costo - Complejidad (Breiman)</a></li>
<li class="chapter" data-level="7.1.6" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#opcional-predicciones-con-cart"><i class="fa fa-check"></i><b>7.1.6</b> (Opcional) Predicciones con CART</a></li>
<li class="chapter" data-level="7.1.7" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#arboles-para-regresion"><i class="fa fa-check"></i><b>7.1.7</b> Árboles para regresión</a></li>
<li class="chapter" data-level="7.1.8" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#variabilidad-en-el-proceso-de-construccion"><i class="fa fa-check"></i><b>7.1.8</b> Variabilidad en el proceso de construcción</a></li>
<li class="chapter" data-level="7.1.9" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#relaciones-lineales"><i class="fa fa-check"></i><b>7.1.9</b> Relaciones lineales</a></li>
<li class="chapter" data-level="7.1.10" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#ventajas-y-desventajas-de-arboles"><i class="fa fa-check"></i><b>7.1.10</b> Ventajas y desventajas de árboles</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#bagging-de-arboles"><i class="fa fa-check"></i><b>7.2</b> Bagging de árboles</a><ul>
<li class="chapter" data-level="7.2.1" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#ejemplo-28"><i class="fa fa-check"></i><b>7.2.1</b> Ejemplo</a></li>
<li class="chapter" data-level="7.2.2" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#mejorando-bagging"><i class="fa fa-check"></i><b>7.2.2</b> Mejorando bagging</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#bosques-aleatorios"><i class="fa fa-check"></i><b>7.3</b> Bosques aleatorios</a><ul>
<li class="chapter" data-level="7.3.1" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#sabiduria-de-las-masas"><i class="fa fa-check"></i><b>7.3.1</b> Sabiduría de las masas</a></li>
<li class="chapter" data-level="7.3.2" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#ejemplo-29"><i class="fa fa-check"></i><b>7.3.2</b> Ejemplo</a></li>
<li class="chapter" data-level="7.3.3" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#mas-detalles-de-bosques-aleatorios."><i class="fa fa-check"></i><b>7.3.3</b> Más detalles de bosques aleatorios.</a></li>
<li class="chapter" data-level="7.3.4" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#importancia-de-variables"><i class="fa fa-check"></i><b>7.3.4</b> Importancia de variables</a></li>
<li class="chapter" data-level="7.3.5" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#ajustando-arboles-aleatorios."><i class="fa fa-check"></i><b>7.3.5</b> Ajustando árboles aleatorios.</a></li>
<li class="chapter" data-level="7.3.6" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#ventajas-y-desventajas-de-arboles-aleatorios"><i class="fa fa-check"></i><b>7.3.6</b> Ventajas y desventajas de árboles aleatorios</a></li>
<li class="chapter" data-level="7.3.7" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#tarea-para-23-de-octubre"><i class="fa fa-check"></i><b>7.3.7</b> Tarea (para 23 de octubre)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html"><i class="fa fa-check"></i><b>8</b> Validación de modelos: problemas comunes</a><ul>
<li class="chapter" data-level="8.1" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#filtracion-de-datos"><i class="fa fa-check"></i><b>8.1</b> Filtración de datos</a></li>
<li class="chapter" data-level="8.2" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#series-de-tiempo"><i class="fa fa-check"></i><b>8.2</b> Series de tiempo</a></li>
<li class="chapter" data-level="8.3" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#filtracion-en-el-preprocesamiento"><i class="fa fa-check"></i><b>8.3</b> Filtración en el preprocesamiento</a></li>
<li class="chapter" data-level="8.4" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#uso-de-variables-fuera-de-rango-temporal"><i class="fa fa-check"></i><b>8.4</b> Uso de variables fuera de rango temporal</a></li>
<li class="chapter" data-level="8.5" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#datos-en-conglomerados-y-muestreo-complejo"><i class="fa fa-check"></i><b>8.5</b> Datos en conglomerados y muestreo complejo</a><ul>
<li class="chapter" data-level="" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#ejemplo-32"><i class="fa fa-check"></i>Ejemplo</a></li>
<li class="chapter" data-level="8.5.1" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#censura-y-evaluacion-incompleta"><i class="fa fa-check"></i><b>8.5.1</b> Censura y evaluación incompleta</a></li>
<li class="chapter" data-level="8.5.2" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#ejemplo-tiendas-cerradas"><i class="fa fa-check"></i><b>8.5.2</b> Ejemplo: tiendas cerradas</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#muestras-de-validacion-chicas"><i class="fa fa-check"></i><b>8.6</b> Muestras de validación chicas</a><ul>
<li class="chapter" data-level="" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#ejercicio-5"><i class="fa fa-check"></i>Ejercicio</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#otros-ejemplos"><i class="fa fa-check"></i><b>8.7</b> Otros ejemplos</a></li>
<li class="chapter" data-level="8.8" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#resumen-1"><i class="fa fa-check"></i><b>8.8</b> Resumen</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Publicado con bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Minicurso de verano de Aprendizaje Máquina</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regresion-logistica" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Regresión logística</h1>
<div id="el-problema-de-clasificacion" class="section level2">
<h2><span class="header-section-number">3.1</span> El problema de clasificación</h2>
<p>Una variabla <span class="math inline">\(G\)</span> <strong>categórica</strong> o <strong>cualitativa</strong> toma valores que no
son numéricos. Por ejemplo, si <span class="math inline">\(G\)</span> denota el estado del contrato de celular
de un cliente dentro de un año, podríamos tener <span class="math inline">\(G\in \{ activo, cancelado\}\)</span>.</p>
<p>En un <strong>problema de clasificación</strong> buscamos predecir una variable respuesta
categórica <span class="math inline">\(G\)</span> en función de otras variables de entrada
<span class="math inline">\(X=(X_1,X_2,\ldots, X_p)\)</span>.</p>
<div id="ejemplos-1" class="section level4 unnumbered">
<h4>Ejemplos</h4>
<ul>
<li><p>Predecir si un cliente cae en impago de una tarjeta de crédito, de forma
que podemos tener <span class="math inline">\(G=corriente\)</span> o <span class="math inline">\(G=impago\)</span>. Variables de entrada podrían
ser <span class="math inline">\(X_1=\)</span> porcentaje de saldo usado, <span class="math inline">\(X_2=\)</span> atrasos en los úlltimos 3 meses,
<span class="math inline">\(X_3=\)</span> edad, etc</p></li>
<li><p>En nuestro ejemplo de
reconocimiento de dígitos tenemos <span class="math inline">\(G\in\{ 0,1,\ldots, 9\}\)</span>. Nótese
que los` dígitos no se pueden considerar como valores numéricos (son etiquetas).
Tenemos que las entradas <span class="math inline">\(X_j\)</span> para <span class="math inline">\(j=1,2,\ldots, 256\)</span> son valores de cada pixel
(imágenes blanco y negro).</p></li>
<li><p>En reconocimiento de imágenes quiza tenemos que <span class="math inline">\(G\)</span> pertenece a un conjunto
que típicamente contiene miles de valores (manzana, árbol, pluma, perro, coche, persona,
cara, etc.). Las <span class="math inline">\(X_j\)</span> son valores de pixeles de la imagen para tres canales
(rojo, verde y azul). Si las imágenes son de 100x100, tendríamos 30,000 variables
de entrada.</p></li>
</ul>
</div>
<div id="que-estimar-en-problemas-de-clasificacion" class="section level3 unnumbered">
<h3>¿Qué estimar en problemas de clasificación?</h3>
<p>En problemas de regresión, consideramos modelos de la forma <span class="math inline">\(Y= f(X) + \epsilon\)</span>,
y vimos que podíamos plantear el problema de aprendizaje supervisado como uno
donde el objetivo
es estimar lo mejor que podamos la función <span class="math inline">\(f\)</span> mediante un estimador
<span class="math inline">\(\hat{f}\)</span>. Usamos entonces <span class="math inline">\(\hat{f}\)</span> para hacer predicciónes. En el caso de regresión:</p>
<ul>
<li><span class="math inline">\(f(X)\)</span> es la relación sistemática de <span class="math inline">\(Y\)</span> en función de <span class="math inline">\(X\)</span></li>
<li>Dada <span class="math inline">\(X\)</span>, la variable observada <span class="math inline">\(Y\)</span> es una variable aleatoria
(<span class="math inline">\(\epsilon\)</span> depende de otras variables que no conocemos)</li>
</ul>
<p>No podemos usar un modelo así
en clasificación pues <span class="math inline">\(G\)</span> no es numérica. Sin embargo, podemos pensar que <span class="math inline">\(X\)</span>
nos da cierta información probabilística acerca de las clases que pueden ocurrir:</p>
<ul>
<li><span class="math inline">\(P(G|X)\)</span> es la probabilidad condicional de observar <span class="math inline">\(G\)</span> si tenemos <span class="math inline">\(X\)</span>. Esto es la información sistemática de <span class="math inline">\(G\)</span> en función de <span class="math inline">\(X\)</span></li>
<li>Dada <span class="math inline">\(X\)</span>, la clase observada <span class="math inline">\(G\)</span> es una variable aleatoria
(depende de otras variables que no conocemos).</li>
</ul>
<p>En analogía con el problema de regresión, quisiéramos estimar las probabilidades condicionales <span class="math inline">\(P(G|X)\)</span>, que es la parte sistemática de la relación de <span class="math inline">\(G\)</span> en función de <span class="math inline">\(X\)</span>.</p>
<p>Normalmente codificamos las clases <span class="math inline">\(g\)</span> con una etiqueta numérica, de modo
que <span class="math inline">\(G\in\{1,2,\ldots, K\}\)</span>:</p>
<div id="ejemplo-7" class="section level4 unnumbered">
<h4>Ejemplo</h4>
<p>(Impago de tarjetas de crédito)
Supongamos que <span class="math inline">\(X=\)</span> porcentaje del crédito máximo usado, y <span class="math inline">\(G\in\{1, 2\}\)</span>, donde
<span class="math inline">\(1\)</span> corresponde al corriente y <span class="math inline">\(2\)</span> representa impago. Podríamos tener, por ejemplo:</p>
<span class="math display">\[\begin{align*} 
p_1(10\%) &amp;= P(G=1|X=10\%) = 0.95 \\
p_2(10\%) &amp;= P(G=2|X=10\%) =  0.05
\end{align*}\]</span>
<p>y</p>
<span class="math display">\[\begin{align*} 
p_1(95\%) &amp;= P(G=1|X=95\%) = 0.70 \\
p_2(95\%) &amp;= P(G=2|X=95\%) =  0.30
\end{align*}\]</span>
<p>En resumen:</p>

<div class="comentario">
En problemas de clasificación queremos estimar la parte
sistemática de la relación de <span class="math inline">\(G\)</span> en función <span class="math inline">\(X\)</span>, que en este caso quiere
decir que buscamos estimar las probabilidades condicionales:
<span class="math display">\[\begin{align*}
p_1(x) &amp;= P(G=1|X=x), \\
p_2(x) &amp;= P(G=2|X=x), \\
\vdots &amp;  \\
p_K(x) &amp;= P(G=K|X=x)
\end{align*}\]</span>
para cada valor <span class="math inline">\(x\)</span> de las entradas.
</div>

<p>A partir de estas probabilidades de clase podemos producir un clasificador de
varias maneras (las discutiremos más adelante). La
forma más simple es usando el clasificador de Bayes:</p>

<div class="comentario">
<p>Dadas las probabilidades condicionales <span class="math inline">\(p_1(x),p_2(x),\ldots, p_K(x)\)</span>, el
<strong>clasificador de Bayes</strong> asociado está dado por
<span class="math display">\[G (x) = \arg\max_{g} p_g(x)\]</span></p>
Es decir, clasificamos en la clase que tiene máxima probabilidad de ocurrir.
</div>

</div>
<div id="ejemplo-8" class="section level4 unnumbered">
<h4>Ejemplo</h4>
<p>(Impago de tarjetas de crédito)
Supongamos que <span class="math inline">\(X=\)</span> porcentaje del crédito máximo usado, y <span class="math inline">\(G\in\{1, 2\}\)</span>, donde
<span class="math inline">\(1\)</span> corresponde al corriente y <span class="math inline">\(2\)</span> representa impago.
Las probabilidades condicionales de clase para la clase <em>al corriente</em> podrían
ser, por ejemplo:</p>
<ul>
<li><span class="math inline">\(p_1(x) = P(G=1|X = x) =0.95\)</span> si <span class="math inline">\(x &lt; 15\%\)</span></li>
<li><span class="math inline">\(p_1(x) = P(G=1|X = x) = 0.95 - 0.007(x-15)\)</span> si <span class="math inline">\(x&gt;=15\%\)</span></li>
</ul>
<p>Estas son probabilidades, pues hay otras variables que influyen en que un cliente
permanezca al corriente o no en sus pagos más allá de información contenida en el
porcentaje de crédito usado. Nótese que estas probabilidades son diferentes
a las no condicionadas, por ejempo, podríamos tener que a total <span class="math inline">\(P(G=1)=0.83\)</span>.</p>
<pre class="sourceCode r"><code class="sourceCode r">p_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="cf">function</span>(x){
  <span class="kw">ifelse</span>(x <span class="op">&lt;</span><span class="st"> </span><span class="dv">15</span>, <span class="fl">0.95</span>, <span class="fl">0.95</span> <span class="op">-</span><span class="st"> </span><span class="fl">0.007</span> <span class="op">*</span><span class="st"> </span>(x <span class="op">-</span><span class="st"> </span><span class="dv">15</span>))
}
<span class="kw">curve</span>(p_<span class="dv">1</span>, <span class="dv">0</span>,<span class="dv">100</span>, <span class="dt">xlab =</span> <span class="st">&#39;Porcentaje de crédito máximo&#39;</span>, <span class="dt">ylab =</span> <span class="st">&#39;p_1(x)&#39;</span>,
  <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>))</code></pre>
<p><img src="03-clasificacion_files/figure-html/unnamed-chunk-4-1.png" width="384" /></p>
<p>¿Por qué en este ejemplo ya no mostramos la función <span class="math inline">\(p_2(x)\)</span>?</p>
<p>Si usamos el clasificador de Bayes, tendríamos por ejemplo que
si <span class="math inline">\(X=10\%\)</span>, como <span class="math inline">\(p_1(10\%) = 0.95\)</span> y <span class="math inline">\(p_2(10\%)=0.05\)</span>, nuestra predicción
de clase sería <span class="math inline">\(G(10\%) = 1\)</span> (al corriente), pero si <span class="math inline">\(X=70\%\)</span>,
<span class="math inline">\(G(70\%) = 1\)</span> (impago), pues <span class="math inline">\(p_1(70\%) = 0.57\)</span> y <span class="math inline">\(p_2(70\%) = 0.43\)</span>.</p>
</div>
</div>
</div>
<div id="estimacion-de-probabilidades-de-clase" class="section level2">
<h2><span class="header-section-number">3.2</span> Estimación de probabilidades de clase</h2>
<p>¿Cómo estimamos ahora las probabilidades de clase a partir de una
muestra de entrenamiento? Veremos por ahora
dos métodos: k-vecinos más cercanos y regresión logística.</p>
<div id="ejemplo-9" class="section level3 unnumbered">
<h3>Ejemplo</h3>
<p>Vamos a generar unos datos con el modelo simple del ejemplo anterior:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(dplyr)
<span class="kw">library</span>(tidyr)
<span class="kw">library</span>(kknn)
<span class="kw">set.seed</span>(<span class="dv">1933</span>)
x &lt;-<span class="st"> </span><span class="kw">pmin</span>(<span class="kw">rexp</span>(<span class="dv">500</span>,<span class="dv">1</span><span class="op">/</span><span class="dv">30</span>),<span class="dv">100</span>)
probs &lt;-<span class="st"> </span><span class="kw">p_1</span>(x)
g &lt;-<span class="st"> </span><span class="kw">ifelse</span>(<span class="kw">rbinom</span>(<span class="kw">length</span>(x), <span class="dv">1</span>, probs)<span class="op">==</span><span class="dv">1</span> ,<span class="dv">1</span>, <span class="dv">2</span>)
dat_ent &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">x =</span> x, <span class="dt">p_1 =</span> probs, <span class="dt">g =</span> <span class="kw">factor</span>(g))
dat_ent <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(x, g) </code></pre>
<pre><code>## # A tibble: 500 x 2
##         x g    
##     &lt;dbl&gt; &lt;fct&gt;
##  1  0.532 1    
##  2 25.4   1    
##  3 37.5   1    
##  4 20.9   1    
##  5 70.9   2    
##  6 14.8   1    
##  7 49.4   1    
##  8 20.9   1    
##  9 35.5   1    
## 10  9.83  1    
## # ... with 490 more rows</code></pre>
<p>Como este problema es de dos clases, podemos graficar como sigue:</p>
<pre class="sourceCode r"><code class="sourceCode r">graf_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">ggplot</span>(dat_ent, <span class="kw">aes</span>(<span class="dt">x =</span> x)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_jitter</span>(<span class="kw">aes</span>(<span class="dt">colour =</span> g, <span class="dt">y =</span> <span class="kw">as.numeric</span>(g<span class="op">==</span><span class="st">&#39;1&#39;</span>)), <span class="dt">width=</span><span class="dv">0</span>, <span class="dt">height=</span><span class="fl">0.1</span>)
graf_<span class="dv">1</span></code></pre>
<p><img src="03-clasificacion_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
</div>
<div id="k-vecinos-mas-cercanos-1" class="section level3">
<h3><span class="header-section-number">3.2.1</span> k-vecinos más cercanos</h3>
<p>Podemos extender fácilmente k vecinos más cercanos para ver un ejemplo de cómo
estimar
las probabilidades de clase <span class="math inline">\(p_g(x)\)</span>. La idea general es igual que en regresión:</p>
<p>Supongamos que tenemos un conjunto de entrenamiento
<span class="math display">\[{\mathcal L}=\{ (x^{(1)},g^{(1)}),(x^{(2)},g^{(2)}), \ldots, (x^{(N)}, g^{(N)}) \}\]</span></p>
<p>La idea es que si queremos predecir en <span class="math inline">\(x_0\)</span>, busquemos varios <span class="math inline">\(k\)</span> vecinos más cercanos
a <span class="math inline">\(x_0\)</span>, y estimamos entonces <span class="math inline">\(p_g(x)\)</span> como la <strong>proporción</strong> de casos tipo <span class="math inline">\(g\)</span> que
hay entre los <span class="math inline">\(k\)</span> vecinos de <span class="math inline">\(x_0\)</span>.</p>
<p>Vemos entonces que este método es un intento de hacer una aproximación directa
de las probabilidades condicionales de clase.</p>
<p>Podemos escribir esto como:</p>
<p>Estimamos contando los elementos de cada clase entre los <span class="math inline">\(k\)</span> vecinos más cercanos:
<span class="math display">\[\hat{p}_g (x_0) = \frac{1}{k}\sum_{x^{(i)} \in N_k(x_0)} I( g^{(i)} = g),\]</span>
para <span class="math inline">\(g=1,2,\ldots, K\)</span>, donde <span class="math inline">\(N_k(x_0)\)</span> es el conjunto de <span class="math inline">\(k\)</span> vecinos más cercanos en <span class="math inline">\({\mathcal L}\)</span> de <span class="math inline">\(x_0\)</span>, y <span class="math inline">\(I(g^{(i)}=g)=1\)</span> cuando <span class="math inline">\(g^{(i)}=g\)</span>, y cero en otro caso.</p>
<div id="ejemplo-10" class="section level4 unnumbered">
<h4>Ejemplo</h4>
<p>Regresamos a nuestro problema de impago. Vamos a intentar estimar la
probabilidad condicional de estar al corriente usando k vecinos
más cercanos (curva roja):</p>
<pre class="sourceCode r"><code class="sourceCode r">graf_data &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">x =</span> <span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">100</span>, <span class="dv">1</span>))
vmc &lt;-<span class="st"> </span><span class="kw">kknn</span>(g <span class="op">~</span><span class="st"> </span>x, <span class="dt">train =</span> dat_ent,  <span class="dt">k =</span> <span class="dv">60</span>,
              <span class="dt">test =</span> graf_data, <span class="dt">kernel =</span> <span class="st">&#39;rectangular&#39;</span>)
graf_data<span class="op">$</span>p_<span class="dv">1</span> &lt;-<span class="st"> </span>vmc<span class="op">$</span>prob[ ,<span class="dv">1</span>]
graf_verdadero &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">x =</span> <span class="dv">0</span><span class="op">:</span><span class="dv">100</span>, <span class="dt">p_1 =</span> <span class="kw">p_1</span>(x))
graf_<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">data =</span> graf_data, <span class="kw">aes</span>(<span class="dt">y =</span> p_<span class="dv">1</span>), <span class="dt">colour =</span> <span class="st">&#39;red&#39;</span>, <span class="dt">size=</span><span class="fl">1.2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">data =</span> graf_verdadero, <span class="kw">aes</span>(<span class="dt">y =</span> p_<span class="dv">1</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&#39;Probabilidad al corriente&#39;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">xlab</span>(<span class="st">&#39;% crédito usado&#39;</span>)</code></pre>
<p><img src="03-clasificacion_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>Igual que en el caso de regresión, ahora tenemos qué pensar cómo validar nuestra
estimación, pues no vamos a tener la curva negra real para comparar.</p>
</div>
<div id="ejemplo-11" class="section level4 unnumbered">
<h4>Ejemplo</h4>
<p>Consideremos datos de diabetes en mujeres Pima:</p>
<p>A population of women who were at least 21 years old, of Pima Indian heritage and living near Phoenix, Arizona, was tested for diabetes according to World Health Organization criteria. The data were collected by the US National Institute of Diabetes and Digestive and Kidney Diseases. We used the 532 complete records after dropping the (mainly missing) data on serum insulin.</p>
<ul>
<li>npreg number of pregnancies.</li>
<li>glu plasma glucose concentration in an oral glucose tolerance test.</li>
<li>bp diastolic blood pressure (mm Hg).</li>
<li>skin triceps skin fold thickness (mm).</li>
<li>bmi body mass index (weight in kg/(height in m)^2).</li>
<li>ped diabetes pedigree function.</li>
<li>age age in years.</li>
<li>type Yes or No, for diabetic according to WHO criteria.</li>
</ul>
<pre class="sourceCode r"><code class="sourceCode r">diabetes_ent &lt;-<span class="st"> </span><span class="kw">as_data_frame</span>(MASS<span class="op">::</span>Pima.tr)
diabetes_pr &lt;-<span class="st"> </span><span class="kw">as_data_frame</span>(MASS<span class="op">::</span>Pima.te)
diabetes_ent</code></pre>
<pre><code>## # A tibble: 200 x 8
##    npreg   glu    bp  skin   bmi   ped   age type 
##  * &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;
##  1     5    86    68    28  30.2 0.364    24 No   
##  2     7   195    70    33  25.1 0.163    55 Yes  
##  3     5    77    82    41  35.8 0.156    35 No   
##  4     0   165    76    43  47.9 0.259    26 No   
##  5     0   107    60    25  26.4 0.133    23 No   
##  6     5    97    76    27  35.6 0.378    52 Yes  
##  7     3    83    58    31  34.3 0.336    25 No   
##  8     1   193    50    16  25.9 0.655    24 No   
##  9     3   142    80    15  32.4 0.2      63 No   
## 10     2   128    78    37  43.3 1.22     31 Yes  
## # ... with 190 more rows</code></pre>
<p>Intentaremos predecir diabetes dependiendo del BMI:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ggplot2)
<span class="kw">ggplot</span>(diabetes_ent, <span class="kw">aes</span>(<span class="dt">x =</span> bmi, <span class="dt">y=</span> <span class="kw">as.numeric</span>(type<span class="op">==</span><span class="st">&#39;Yes&#39;</span>), <span class="dt">colour =</span> type)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>()</code></pre>
<p><img src="03-clasificacion_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>Usamos <span class="math inline">\(20\)</span> vecinos más cercanos para estimar <span class="math inline">\(p_g(x)\)</span>:</p>
<pre class="sourceCode r"><code class="sourceCode r">graf_data &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">bmi =</span> <span class="kw">seq</span>(<span class="dv">20</span>,<span class="dv">45</span>, <span class="dv">1</span>))
vmc_<span class="dv">5</span> &lt;-<span class="st"> </span><span class="kw">kknn</span>(type <span class="op">~</span><span class="st"> </span>bmi, <span class="dt">train =</span> diabetes_ent,  <span class="dt">k =</span> <span class="dv">20</span>,
              <span class="dt">test =</span> graf_data, <span class="dt">kernel =</span> <span class="st">&#39;rectangular&#39;</span>)
graf_data<span class="op">$</span>Yes &lt;-<span class="st"> </span>vmc_<span class="dv">5</span><span class="op">$</span>prob[ ,<span class="st">&quot;Yes&quot;</span>]
graf_data<span class="op">$</span>No &lt;-<span class="st"> </span>vmc_<span class="dv">5</span><span class="op">$</span>prob[ ,<span class="st">&quot;No&quot;</span>]
graf_data &lt;-<span class="st"> </span>graf_data <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">gather</span>(type, prob, Yes<span class="op">:</span>No)
<span class="kw">ggplot</span>(diabetes_ent, <span class="kw">aes</span>(<span class="dt">x =</span> bmi, <span class="dt">y=</span> <span class="kw">as.numeric</span>(type<span class="op">==</span><span class="st">&#39;Yes&#39;</span>), <span class="dt">colour =</span> type)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">data =</span> <span class="kw">filter</span>(graf_data, type <span class="op">==</span><span class="st">&#39;Yes&#39;</span>) , 
            <span class="kw">aes</span>(<span class="dt">x=</span>bmi, <span class="dt">y =</span> prob, <span class="dt">colour=</span>type, <span class="dt">group =</span> type)) <span class="op">+</span>
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&#39;Probabilidad diabetes&#39;</span>)</code></pre>
<p><img src="03-clasificacion_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
</div>
</div>
</div>
<div id="error-para-modelos-de-clasificacion" class="section level2">
<h2><span class="header-section-number">3.3</span> Error para modelos de clasificación</h2>
<p>En regresión, vimos que la pérdida cuadrática era una buena opción para ajustar
modelos (descenso en gradiente, por ejemplo), y también para evaluar su desempeño.
Ahora necesitamos una pérdida apropiada para trabajar con modelos de clasificación.</p>
<p>Consideremos entonces que tenemos una estimación <span class="math inline">\(\hat{p}_g(x)\)</span> de las probabilidad
de clase <span class="math inline">\(P(G=g|X=x)\)</span>. Supongamos que observamos ahora <span class="math inline">\((x, g)\)</span>.</p>
<ul>
<li>Si
<span class="math inline">\(\hat{p}_{g}(x)\)</span> es muy cercana a uno, deberíamos penalizar poco, pues dimos
probabilidad alta a <span class="math inline">\(G=g\)</span>.</li>
<li>Si <span class="math inline">\(\hat{p}_{g}(x)\)</span> es chica, deberíamos penalizar más, pues dimos probabilidad baja
a <span class="math inline">\(G=g\)</span>.</li>
<li>Si <span class="math inline">\(\hat{p}_{g}(x)\)</span> es muy cercana a cero, y observamos <span class="math inline">\(G=g\)</span>, deberíamos hacer
una penalización muy alta (convergiendo a <span class="math inline">\(\infty\)</span>, pues no es aceptable que sucedan
eventos con probabilidad estimada extremadamente baja).</li>
</ul>
<p>Quisiéramos encontrar una función <span class="math inline">\(h\)</span> apropiada, de forma que la pérdida
al observar <span class="math inline">\((x, g)\)</span> sea
<span class="math display">\[s(\hat{p}_{g}(x)),\]</span>
y que cumpla con los puntos arriba señalados. Entonces tenemos que</p>
<ul>
<li><span class="math inline">\(s\)</span> debe ser una función continua y decreciente en <span class="math inline">\([0,1]\)</span></li>
<li>Podemos poner <span class="math inline">\(s(1)=0\)</span> (no hay pérdida si ocurre algo con probabilidad 1)</li>
<li><span class="math inline">\(s(p)\)</span> debe ser muy grande is <span class="math inline">\(p\)</span> es muy chica.</li>
</ul>
<p>Una opción analíticamente conveniente es
<span class="math display">\[s(z) = - 2log(z)\]</span></p>
<pre class="sourceCode r"><code class="sourceCode r">s &lt;-<span class="st"> </span><span class="cf">function</span>(z){ <span class="dv">-2</span><span class="op">*</span><span class="kw">log</span>(z)}
<span class="kw">curve</span>(s, <span class="dv">0</span>, <span class="dv">1</span>)</code></pre>
<p><img src="03-clasificacion_files/figure-html/unnamed-chunk-11-1.png" width="288" /></p>
<p>Y entonces la pérdida (que llamamos <strong>devianza</strong>) que construimos está dada, para
<span class="math inline">\((x,g)\)</span> observado y probabilidades estimadas <span class="math inline">\(\hat{p}_g(x)\)</span> por</p>
<p><span class="math display">\[
- 2\log(\hat{p}_g(x))
\]</span></p>
<p>Su valor esperado (según el proceso que genera los datos) es nuestra medición
del desempeño del modelo <span class="math inline">\(\hat{p}_g (x)\)</span>:</p>
<p><span class="math display">\[-2E\left [ \log(\hat{p}_G(X)) \right ]\]</span></p>
<p><strong>Observaciones</strong>:</p>
<ul>
<li><p>Ojo: el nombre de devianza se utiliza
de manera diferente en distintos lugares (pero para cosas similares).</p></li>
<li><p>Usamos el factor 2 por razones históricas (la medida de devianza
definida en estadística tiene un 2, para usar más fácilmente en
pruebas de hipótesis relacionadas con comparaciones de modelos). Para nuestros
propósitos, podemos usar o no el 2.</p></li>
<li><p>No es fácil interpretar la devianza, pero es útil para comparar modelos. Veremos
otras medidas más fáciles de intrepretar más adelante.</p></li>
</ul>
<p>Compara la siguiente definición con la que vimos para modelos de regresión:</p>

<div class="comentario">
<p>Sea <span class="math display">\[{\mathcal L}=\{ (x^{(1)},g^{(1)}),(x^{(2)},g^{(2)}), \ldots, (x^{(N)}, g^{(N)}) \}\]</span>
una muestra de entrenamiento, a partir de las cuales construimos mediante
un algoritmo funciones estimadas
<span class="math inline">\(\hat{p}_{g} (x)\)</span> para <span class="math inline">\(g=1,2,\ldots, K\)</span>. La <strong>devianza promedio de entrenamiento</strong>
está dada por
<span class="math display" id="eq:devianza">\[\begin{equation}
\overline{err} = - \frac{2}{N}\sum_{i=1}^N log(\hat{p}_{g^{(i)}} (x^{(i)}))
  \tag{3.1}
\end {equation}\]</span></p>
Sea <span class="math display">\[{\mathcal T}=\{ (x_0^{(1)},g_0^{(1)}),(x_0^{(2)},g_0^{(2)}), \ldots, (x_0^{(m)}, g_0^{(m)}) \}\]</span> una muestra de prueba. La <strong>devianza promedio de prueba</strong> es
<span class="math display">\[\begin{equation}
\hat{Err} = - \frac{2}{m}\sum_{i=1}^m log(\hat{p}_{g_0^{(i)}} (x_0^{(i)}))
\end {equation}\]</span>
que es una estimación de la devianza de predicción
<span class="math display">\[-2E\left [ \log(\hat{p}_G(X)) \right ]\]</span>
</div>

<div id="ejemplo-12" class="section level4 unnumbered">
<h4>Ejemplo</h4>
<p>Regresamos a nuestros ejemplo de impago de tarjetas de crédito. Primero
calculamos la devianza de entrenamiento</p>
<pre class="sourceCode r"><code class="sourceCode r">s &lt;-<span class="st"> </span><span class="cf">function</span>(x) <span class="dv">-2</span><span class="op">*</span><span class="kw">log</span>(x)

vmc &lt;-<span class="st"> </span><span class="kw">kknn</span>(g <span class="op">~</span><span class="st"> </span>x, <span class="dt">train =</span> dat_ent,  <span class="dt">k =</span> <span class="dv">60</span>,
              <span class="dt">test =</span> dat_ent, <span class="dt">kernel =</span> <span class="st">&#39;rectangular&#39;</span>)
dat_dev &lt;-<span class="st"> </span>dat_ent <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(x,g)
dat_dev<span class="op">$</span>hat_p_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">predict</span>(vmc, <span class="dt">type =</span><span class="st">&#39;prob&#39;</span>)[,<span class="dv">1</span>]
dat_dev<span class="op">$</span>hat_p_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">predict</span>(vmc, <span class="dt">type =</span><span class="st">&#39;prob&#39;</span>)[,<span class="dv">2</span>]
dat_dev &lt;-<span class="st"> </span>dat_dev <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">hat_p_g =</span> <span class="kw">ifelse</span>(g<span class="op">==</span><span class="dv">1</span>, hat_p_<span class="dv">1</span>, hat_p_<span class="dv">2</span>))</code></pre>
<p>Nótese que dependiendo de qué clase observamos (columna <span class="math inline">\(g\)</span>), extraemos la
probabilidad correspondiente a la columna hat_p_g:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(dat_dev, <span class="dv">50</span>)</code></pre>
<pre><code>## # A tibble: 50 x 5
##         x g     hat_p_1 hat_p_2 hat_p_g
##     &lt;dbl&gt; &lt;fct&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
##  1  0.532 1       0.967  0.0333   0.967
##  2 25.4   1       0.883  0.117    0.883
##  3 37.5   1       0.85   0.15     0.85 
##  4 20.9   1       0.9    0.1      0.9  
##  5 70.9   2       0.6    0.4      0.4  
##  6 14.8   1       0.933  0.0667   0.933
##  7 49.4   1       0.8    0.2      0.8  
##  8 20.9   1       0.9    0.1      0.9  
##  9 35.5   1       0.75   0.25     0.75 
## 10  9.83  1       0.933  0.0667   0.933
## # ... with 40 more rows</code></pre>
<p>Ahora aplicamos la función <span class="math inline">\(s\)</span> que describimos arriba, y promediamos sobre
el conjunto de entrenamiento:</p>
<pre class="sourceCode r"><code class="sourceCode r">dat_dev &lt;-<span class="st"> </span>dat_dev <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">dev =</span> <span class="kw">s</span>(hat_p_g))
dat_dev <span class="op">%&gt;%</span><span class="st"> </span>ungroup <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">summarise</span>(<span class="dt">dev_entrena =</span> <span class="kw">mean</span>(dev))</code></pre>
<pre><code>## # A tibble: 1 x 1
##   dev_entrena
##         &lt;dbl&gt;
## 1       0.700</code></pre>
<p>Recordemos que la devianza de entrenamiento no es la cantidad que evalúa el
desempeño del modelo. Hagamos el cálculo entonces para una muestra de prueba:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1213</span>)
x &lt;-<span class="st"> </span><span class="kw">pmin</span>(<span class="kw">rexp</span>(<span class="dv">1000</span>,<span class="dv">1</span><span class="op">/</span><span class="dv">30</span>),<span class="dv">100</span>)
probs &lt;-<span class="st"> </span><span class="kw">p_1</span>(x)
g &lt;-<span class="st"> </span><span class="kw">ifelse</span>(<span class="kw">rbinom</span>(<span class="kw">length</span>(x), <span class="dv">1</span>, probs)<span class="op">==</span><span class="dv">1</span> ,<span class="dv">1</span>, <span class="dv">2</span>)
dat_prueba &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">x =</span> x, <span class="dt">g =</span> <span class="kw">factor</span>(g))

vmc &lt;-<span class="st"> </span><span class="kw">kknn</span>(g <span class="op">~</span><span class="st"> </span>x, <span class="dt">train =</span> dat_ent,  <span class="dt">k =</span> <span class="dv">60</span>,
              <span class="dt">test =</span> dat_prueba, <span class="dt">kernel =</span> <span class="st">&#39;rectangular&#39;</span>)
dat_dev &lt;-<span class="st"> </span>dat_prueba <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(x,g)
dat_dev<span class="op">$</span>hat_p_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">predict</span>(vmc, <span class="dt">type =</span><span class="st">&#39;prob&#39;</span>)[,<span class="dv">1</span>]
dat_dev<span class="op">$</span>hat_p_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">predict</span>(vmc, <span class="dt">type =</span><span class="st">&#39;prob&#39;</span>)[,<span class="dv">2</span>]
dat_dev &lt;-<span class="st"> </span>dat_dev <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">hat_p_g =</span> <span class="kw">ifelse</span>(g<span class="op">==</span><span class="dv">1</span>, hat_p_<span class="dv">1</span>, hat_p_<span class="dv">2</span>))
dat_dev &lt;-<span class="st"> </span>dat_dev <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">dev =</span> <span class="kw">s</span>(hat_p_g))
dat_dev <span class="op">%&gt;%</span><span class="st"> </span>ungroup <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">summarise</span>(<span class="dt">dev_prueba =</span> <span class="kw">mean</span>(dev))</code></pre>
<pre><code>## # A tibble: 1 x 1
##   dev_prueba
##        &lt;dbl&gt;
## 1      0.711</code></pre>
</div>
<div id="ejercicio-2" class="section level3">
<h3><span class="header-section-number">3.3.1</span> Ejercicio</h3>
<p>Utiliza 5, 20, 60, 200 y 400 vecinos más cercanos para nuestro ejemplo de tarjetas
de crédito. ¿Cuál tiene menor devianza de prueba? ¿Cuál tiene menor devianza
de entrenamiento? Grafica el mejor que obtengas y otros dos modelos malos. ¿Por qué
crees que la devianza es muy grande para los modelos malos?</p>
<p>Nota: ten cuidado con probabilidades iguales a 0 o 1, pues en en estos casos
la devianza puede dar <span class="math inline">\(\infty\)</span>. Puedes por ejemplo hacer que las probabilidades
siempre estén en <span class="math inline">\([\epsilon, 1-\epsilon]\)</span> para <span class="math inline">\(\epsilon&gt;0\)</span> chica.</p>
<p>Empieza con el código en <em>clase_3_ejercicio.R</em>.</p>
</div>
<div id="error-de-clasificacion-y-funcion-de-perdida-0-1" class="section level3">
<h3><span class="header-section-number">3.3.2</span> Error de clasificación y función de pérdida 0-1</h3>
<p>Otra medida común para medir el error de un clasificador es
el <em>error de clasificación</em>, que también llamamos <em>probabilidad de clasificación
incorrecta</em>, o error bajo pérdida 0-1.</p>

<div class="comentario">
<p>Si <span class="math inline">\(\hat{G}\)</span> es un clasificador (que puede
ser construido a partir de probabilidades de clase),
decimos que su <strong>error de clasificación</strong> es</p>
<p><span class="math display">\[P(\hat{G}\neq G)\]</span></p>
</div>

<p>Aunque esta definición aplica para cualquier clasificador, podemos usarlo
para clasificadores construidos con probabilidades de clase de la siguiente
forma:</p>

<div class="comentario">
Sean <span class="math inline">\(\hat{p}_g(x)\)</span> probabilidades de clase estimadas. El clasificador asociado
está dado por
<span class="math display">\[\hat{G} (x) = \arg\max_g \hat{p}_g(x)\]</span>
Podemos estimar su error de clasificación <span class="math inline">\(P(\hat{G} \neq G)\)</span> con una muestra
de prueba
<span class="math display">\[{\mathcal T}=\{ (x_0^{(1)},g_0^{(1)}),(x_0^{(2)},g_0^{(2)}), \ldots, (x_0^{(m)}, g_0^{(m)})\]</span>
mediante
<span class="math display">\[\hat{Err} = \frac{1}{m} \sum_{j=i}^m I(\hat{G}(x_0^{(i)}) \neq g_0^{(i)}),\]</span>
es decir, la proporción de casos de prueba que son clasificados incorrectamente.
</div>

<div id="ejemplo-13" class="section level4 unnumbered">
<h4>Ejemplo</h4>
<p>Veamos cómo se comporta en términos de error de clasificación nuestro último modelo:</p>
<pre class="sourceCode r"><code class="sourceCode r">dat_dev<span class="op">$</span>hat_G &lt;-<span class="st"> </span><span class="kw">predict</span>(vmc)
dat_dev <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">correcto =</span> hat_G <span class="op">==</span><span class="st"> </span>g) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>ungroup <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">summarise</span>(<span class="dt">p_correctos =</span> <span class="kw">mean</span>(correcto)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">error_clasif =</span> <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p_correctos)</code></pre>
<pre><code>## # A tibble: 1 x 2
##   p_correctos error_clasif
##         &lt;dbl&gt;        &lt;dbl&gt;
## 1       0.851        0.149</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">vmc_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">kknn</span>(g <span class="op">~</span><span class="st"> </span>x, <span class="dt">train =</span> dat_ent,  <span class="dt">k =</span> <span class="dv">3</span>,
              <span class="dt">test =</span> dat_prueba, <span class="dt">kernel =</span> <span class="st">&#39;rectangular&#39;</span>)
dat_dev<span class="op">$</span>hat_G &lt;-<span class="st"> </span><span class="kw">predict</span>(vmc_<span class="dv">2</span>)
dat_dev <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">correcto =</span> hat_G <span class="op">==</span><span class="st"> </span>g) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>ungroup <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">summarise</span>(<span class="dt">p_correctos =</span> <span class="kw">mean</span>(correcto)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">error_clasif =</span> <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p_correctos)</code></pre>
<pre><code>## # A tibble: 1 x 2
##   p_correctos error_clasif
##         &lt;dbl&gt;        &lt;dbl&gt;
## 1        0.82         0.18</code></pre>
</div>
</div>
<div id="discusion-relacion-entre-devianza-y-error-de-clasificacion" class="section level3">
<h3><span class="header-section-number">3.3.3</span> Discusión: relación entre devianza y error de clasificación</h3>
<p>Cuando utilizamos devianza,
el mejor desempeño se alcanza cuando las probabilidades <span class="math inline">\(\hat{p}_g (x)\)</span>
están bien calibradas, es decir, están cercanas a las probabilidades
verdaderas <span class="math inline">\(p_g (x)\)</span>. Esto se puede ver demostrando que las probabilidades
<span class="math inline">\(\hat{p}_g (x)\)</span> que minimizan la devianza
<span class="math display">\[-2E(\log (\hat{p}_G (X))) = -2E_X \left[  \sum_{k=1}^K p_g(X)\log\hat{p}_g(X)    \right]\]</span></p>
<p>son precisamente <span class="math inline">\(\hat{p}_g (x)=p_g (x)\)</span>.</p>
<p>Por otro lado, si consideramos el error de clasificación <span class="math inline">\(P(\hat{G}\neq G)\)</span>,
es posible demostrar que se minimiza cuando
<span class="math inline">\(\hat{G} = G_{bayes}\)</span>, donde</p>
<p><span class="math display">\[{G}_{bayes} (x) = \arg\max_g {p}_g(x).\]</span></p>
<p>En consecuencia, cuando las <span class="math inline">\(\hat{p}_g(x)\)</span> estimadas están cercanas
a las verdaderas <span class="math inline">\(p_g (x)\)</span> (que es lo que intentamos hacer cuando usamos devianza),
el clasificador <span class="math inline">\(\hat{G}(x)\)</span> producido a partir de las <span class="math inline">\(\hat{p}_g(x)\)</span> deberá
estar cercano a <span class="math inline">\(G_{bayes}(x)\)</span>, que es el clasificador que minimiza el error
de clasificación.</p>
<p>Este argumento explica que buscar modelos con devianza baja no está alineado
con buscar modelos con error de clasificación bajo.</p>
<p>Cuando sea posible, es mejor trabajar con probabilidades de clase y devianza que solamente
con clasificadores y error de clasificación. Hay varias razones para esto:</p>
<ul>
<li>Tenemos una medida de qué tan seguros estamos en la clasificación (por ejemplo,
<span class="math inline">\(p_1 = 0.55\)</span> en vez de <span class="math inline">\(p_1 = 0.995\)</span>).</li>
<li>La salida de probabilides es un insumo más útil para tareas posteriores (por ejemplo,
si quisiéramos ofrecer las 3 clases más probables en clasificación de imágenes).</li>
<li>Permite hacer selección de modelos de manera más atinada: por ejemplo, dada una
misma tasa de correctos, preferimos aquellos modelos que lo hacen con probabilidades
que discriminan más (más altas cuando está en lo correcto y más bajas cuando
se equivoca).</li>
</ul>
</div>
</div>
<div id="regresion-logistica-1" class="section level2">
<h2><span class="header-section-number">3.4</span> Regresión logística</h2>
<p>En <span class="math inline">\(k\)</span> vecinos más cercanos, intentamos estimar directamente con promedios
las probabilidades de clase. Regresión logística (y otros métodos, como redes neuronales),
son ajustados intentando minimizar la devianza de entrenamiento. Esto es necesario
si queremos aprovechar la estructura adicional que estos modelos aportan (recordemos
el caso de regresión lineal: intentamos minimizar el error de entrenamiento para
estimar nuestro predictor, y así podíamos explotar apropiadamente
la estructura lineal del problema).</p>
<p>Regresión logística es un método lineal de clasificación, en el sentido de
que produce fronteras lineales de decisión para el clasificador asociado.</p>
<div id="regresion-logistica-simple" class="section level3">
<h3><span class="header-section-number">3.4.1</span> Regresión logística simple</h3>
<p>Vamos a construir el modelo de regresión logística (binaria)
para una sola entrada.
Suponemos que
tenemos una sola entrada <span class="math inline">\(X_1\)</span>, y<br />
que <span class="math inline">\(G\in\{1,2\}\)</span>. Nos convendrá crear una nueva variable <span class="math inline">\(Y\)</span> dada por
<span class="math inline">\(Y=1\)</span> si <span class="math inline">\(G=2\)</span>, <span class="math inline">\(Y=0\)</span> si <span class="math inline">\(G=1\)</span>.</p>
<p>Nótese que intentar estimar las probabilidades de clase <span class="math inline">\(p_1(x)\)</span> de forma lineal con</p>
<p><span class="math display">\[p_1(x)=\beta_0+\beta_1 x_1\]</span>
tiene el defecto de que el lado derecho puede producir valores fuera
de <span class="math inline">\([0,1]\)</span>. La idea es entonces aplicar una función <span class="math inline">\(h\)</span> simple
que transforme la recta real al intervalo <span class="math inline">\([0,1]:\)</span>
<span class="math display">\[p_1(x) = h(\beta_0+\beta_1 x_1),\]</span>
donde <span class="math inline">\(h\)</span> es una función que toma valores en <span class="math inline">\([0,1]\)</span>. ¿Cúal es la función
más simple que hace esto?</p>
</div>
<div id="funcion-logistica" class="section level3">
<h3><span class="header-section-number">3.4.2</span> Función logística</h3>
<p>Comenzamos con el caso más simple, poniendo
<span class="math inline">\(\beta_0=0\)</span> y <span class="math inline">\(\beta_1=1\)</span>, de modo que
<span class="math display">\[p_1(x)=h(x).\]</span>
¿Cómo debe ser <span class="math inline">\(h\)</span> para garantizar que <span class="math inline">\(h(x)\)</span> está entre 0 y 1 para toda <span class="math inline">\(x\)</span>?
No van a funcionar polinomios, por ejemplo, porque para un polinomio cuando
<span class="math inline">\(x\)</span> tiende a infinito, el polinomio tiende a <span class="math inline">\(\infty\)</span> o a <span class="math inline">\(-\infty\)</span>.
Hay varias posibilidades, pero una de las más simples es tomar (ver gráfica
al margen):</p>

<div class="comentario">
La función logística está dada por
<span class="math display">\[h(x)=\frac{e^x}{1+e^x}\]</span>
</div>

<pre class="sourceCode r"><code class="sourceCode r">h &lt;-<span class="st"> </span><span class="cf">function</span>(x){<span class="kw">exp</span>(x)<span class="op">/</span>(<span class="dv">1</span><span class="op">+</span><span class="kw">exp</span>(x)) }
<span class="kw">curve</span>(h, <span class="dt">from=</span><span class="op">-</span><span class="dv">6</span>, <span class="dt">to =</span><span class="dv">6</span>)</code></pre>
<p><img src="03-clasificacion_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>Esta función comprime adecuadamente (para nuestros propósitos)
el rango de todos los reales dentro del intervalo <span class="math inline">\([0,1]\)</span>.</p>

<div class="comentario">
El modelo de regresión logística simple está dado por
<span class="math display">\[p_1(x)=p_1(x;\beta)= h(\beta_0+\beta_1x_1)= \frac{e^{\beta_0+\beta_1x_1}}{1+ e^{\beta_0+\beta_1x_1}},\]</span>
y <span class="math display">\[p_0(x)=p_0(x;\beta)=1-p_1(x;\beta),\]</span>
donde <span class="math inline">\(\beta=(\beta_0,\beta_1)\)</span>.
</div>

<p>Este es un modelo paramétrico con 2 parámetros.</p>
<div id="ejercicio-3" class="section level4 unnumbered">
<h4>Ejercicio</h4>
<ul>
<li><p>Demostrar que, si <span class="math inline">\(p_1(x)\)</span> está dado como en la ecuación anterior, entonces
también podemos escribir:
<span class="math display">\[p_o(x)=\frac{1}{1+e^{\beta_0+\beta_1x_1}}.\]</span></p></li>
<li><p>Graficar las funciones <span class="math inline">\(p_1(x;\beta)\)</span> para distintos
valores de <span class="math inline">\(\beta_0\)</span> y <span class="math inline">\(\beta_1\)</span>.</p></li>
</ul>
</div>
<div id="ejemplo-14" class="section level4">
<h4><span class="header-section-number">3.4.2.1</span> Ejemplo</h4>
<p>En nuestro ejemplo:</p>
<pre class="sourceCode r"><code class="sourceCode r">graf_data &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">x =</span> <span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">100</span>, <span class="dv">1</span>))
vmc_graf &lt;-<span class="st"> </span><span class="kw">kknn</span>(g <span class="op">~</span><span class="st"> </span>x, <span class="dt">train =</span> dat_ent,  <span class="dt">k =</span> <span class="dv">60</span>,
              <span class="dt">test =</span> graf_data, <span class="dt">kernel =</span> <span class="st">&#39;rectangular&#39;</span>)
graf_data<span class="op">$</span>p_<span class="dv">1</span> &lt;-<span class="st"> </span>vmc_graf<span class="op">$</span>prob[ ,<span class="dv">1</span>]
graf_verdadero &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">x =</span> <span class="dv">0</span><span class="op">:</span><span class="dv">100</span>, <span class="dt">p_1 =</span> <span class="kw">p_1</span>(x))
graf_<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">data =</span> graf_data, <span class="kw">aes</span>(<span class="dt">y =</span> p_<span class="dv">1</span>), <span class="dt">colour =</span> <span class="st">&#39;red&#39;</span>, <span class="dt">size=</span><span class="fl">1.2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">data =</span> graf_verdadero, <span class="kw">aes</span>(<span class="dt">y =</span> p_<span class="dv">1</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&#39;Probabilidad al corriente&#39;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">xlab</span>(<span class="st">&#39;% crédito usado&#39;</span>)</code></pre>
<p><img src="03-clasificacion_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p>Ahora intentaremos ajustar a mano (intenta cambiar
las betas para p_mod_1 y p_mod_2 en el ejemplo de abajo)
algunos modelos logísticos para las probabilidades
de clase:</p>
<pre class="sourceCode r"><code class="sourceCode r">h &lt;-<span class="st"> </span><span class="cf">function</span>(z) <span class="kw">exp</span>(z)<span class="op">/</span>(<span class="dv">1</span><span class="op">+</span><span class="kw">exp</span>(z))
p_logistico &lt;-<span class="st"> </span><span class="cf">function</span>(beta_<span class="dv">0</span>, beta_<span class="dv">1</span>){
  p &lt;-<span class="st"> </span><span class="cf">function</span>(x){
    z &lt;-<span class="st"> </span>beta_<span class="dv">0</span> <span class="op">+</span><span class="st"> </span>beta_<span class="dv">1</span><span class="op">*</span>x
    <span class="kw">h</span>(z)
  }
}
p_mod_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">p_logistico</span>(<span class="op">-</span><span class="dv">20</span>, <span class="dv">1</span>)
p_mod_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">p_logistico</span>(<span class="dv">3</span>, <span class="fl">-0.04</span>)
graf_data &lt;-<span class="st"> </span>graf_data <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">p_mod_1 =</span> <span class="kw">p_mod_1</span>(x), <span class="dt">p_mod_2 =</span> <span class="kw">p_mod_2</span>(x))
graf_<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">data =</span> graf_data, <span class="kw">aes</span>(<span class="dt">y =</span> p_mod_<span class="dv">2</span>), <span class="dt">colour =</span> <span class="st">&#39;red&#39;</span>, <span class="dt">size=</span><span class="fl">1.2</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_line</span>(<span class="dt">data =</span> graf_data, <span class="kw">aes</span>(<span class="dt">y =</span> p_mod_<span class="dv">1</span>), <span class="dt">colour =</span> <span class="st">&#39;orange&#39;</span>, <span class="dt">size=</span><span class="fl">1.2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">data =</span> graf_verdadero, <span class="kw">aes</span>(<span class="dt">y =</span> p_<span class="dv">1</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&#39;Probabilidad al corriente&#39;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">xlab</span>(<span class="st">&#39;% crédito usado&#39;</span>)</code></pre>
<p><img src="03-clasificacion_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<p>Podemos usar también la función glm de R para ajustar los coeficientes:</p>
<pre class="sourceCode r"><code class="sourceCode r">mod_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">glm</span>(g<span class="op">==</span><span class="dv">1</span> <span class="op">~</span><span class="st"> </span>x, <span class="dt">data =</span> dat_ent, <span class="dt">family =</span> <span class="st">&#39;binomial&#39;</span>)
<span class="kw">coef</span>(mod_<span class="dv">1</span>)</code></pre>
<pre><code>## (Intercept)           x 
##  3.24467326 -0.04353428</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">p_mod_final &lt;-<span class="st"> </span><span class="kw">p_logistico</span>(<span class="kw">coef</span>(mod_<span class="dv">1</span>)[<span class="dv">1</span>], <span class="kw">coef</span>(mod_<span class="dv">1</span>)[<span class="dv">2</span>])
graf_data &lt;-<span class="st"> </span>graf_data <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">p_mod_f =</span> <span class="kw">p_mod_final</span>(x))

graf_<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">data =</span> graf_data, <span class="kw">aes</span>(<span class="dt">y =</span> p_mod_f), <span class="dt">colour =</span> <span class="st">&#39;red&#39;</span>, <span class="dt">size=</span><span class="fl">1.2</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_line</span>(<span class="dt">data =</span> graf_data, <span class="kw">aes</span>(<span class="dt">y =</span> p_mod_<span class="dv">1</span>), <span class="dt">colour =</span> <span class="st">&#39;orange&#39;</span>, <span class="dt">size=</span><span class="fl">1.2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">data =</span> graf_verdadero, <span class="kw">aes</span>(<span class="dt">y =</span> p_<span class="dv">1</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&#39;Probabilidad al corriente&#39;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">xlab</span>(<span class="st">&#39;% crédito usado&#39;</span>)</code></pre>
<p><img src="03-clasificacion_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
</div>
</div>
<div id="regresion-logistica-2" class="section level3">
<h3><span class="header-section-number">3.4.3</span> Regresión logística</h3>
<p>Ahora escribimos el modelo cuando tenemos más de una entrada. La idea es la misma:
primero combinamos las variables linealmente usando pesos <span class="math inline">\(\beta\)</span>, y despúes
comprimimos a <span class="math inline">\([0,1]\)</span> usando la función logística:</p>

<div class="comentario">
El modelo de regresión logística está dado por
<span class="math display">\[p_1(x)=p_1(x;\beta)= h(\beta_0+\beta_1x_1 + \beta_2x_2 +\cdots + \beta_p x_p),\]</span>
y <span class="math display">\[p_0(x)=p_0(x;\beta)=1-p_1(x;\beta),\]</span>
donde <span class="math inline">\(\beta=(\beta_0,\beta_1, \ldots, \beta_p)\)</span>.
</div>

</div>
</div>
<div id="aprendizaje-de-coeficientes-para-regresion-logistica-binomial." class="section level2">
<h2><span class="header-section-number">3.5</span> Aprendizaje de coeficientes para regresión logística (binomial).</h2>
<p>Ahora veremos cómo aprender los coeficientes con una muestra de entrenamiento. La idea
general es :</p>
<ul>
<li>Usamos la devianza de entrenamiento como medida de ajuste</li>
<li>Usamos descenso en gradiente para minimizar esta devianza y aprender los coeficientes.</li>
</ul>
<p>Sea entonces <span class="math inline">\({\mathcal L}\)</span> una muestra de entrenamiento:</p>
<p><span class="math display">\[{\mathcal L}=\{ (x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}), \ldots, (x^{(N)}, y^{(N)}) \}\]</span></p>
<p>Donde <span class="math inline">\(y=1\)</span> o <span class="math inline">\(y=0\)</span> son las dos clases. Escribimos también</p>
<p><span class="math display">\[p_1(x)=p_1(x;\beta)= h(\beta_0+\beta_1x_1 + \beta_2x_2 +\cdots + \beta_p x_p),\]</span></p>
<p>y definimos la devianza sobre el conjunto de entrenamiento</p>
<p><span class="math display">\[D(\beta) = -2\sum_{i=1}^N \log(p_{y^{(i)}} (x^{(i)})).\]</span></p>
<p>Los <strong>coeficientes estimados por regresión logística</strong> están dados por
<span class="math display">\[\hat{\beta} = \arg\min_\beta D(\beta)\]</span></p>
<p>Para minimizar utilizaremos descenso en gradiente (aunque hay más opciones).</p>
<p>La última expresión para <span class="math inline">\(D(\beta)\)</span> puede ser difícil de operar, pero podemos reescribir como:
<span class="math display">\[D(\beta) = -2\sum_{i=1}^N y^{(i)} \log(p_{1} (x^{(i)})) + (1-y^{(i)}) \log(p_{0} (x^{(i)})).\]</span></p>
<p>Para hacer descenso en gradiente, necesitamos encontrar <span class="math inline">\(\frac{\partial D}{\beta_j}\)</span>
para <span class="math inline">\(j=1,2,\ldots,p\)</span>.</p>
<p>Igual que en regresión lineal, comenzamos por calcular la derivada de un término:</p>
<p><span class="math display">\[D^{(i)} (\beta) = y^{(i)} \log(p_{1} (x^{(i)})) + (1-y^{(i)}) \log(1-p_{1} (x^{(i)}))\]</span></p>
<p>Calculamos primero las derivadas de <span class="math inline">\(p_1 (x^{(i)};\beta)\)</span> (demostrar la siguiente ecuación):
<span class="math display">\[\frac{\partial  p_1}{\partial \beta_0} = {p_1(x^{(i)})(1-p_1(x^{(i)}))},\]</span>
y
<span class="math display">\[\frac{\partial  p_1}{\partial \beta_j} = p_1(x^{(i)})(1-p_1(x^{(i)}))x_j^{(i)},\]</span></p>
<p>Así que
<span class="math display">\[\begin{align*}
\frac{\partial D^{(i)}}{\partial \beta_j} &amp;= \frac{y^{(i)}}{(p_1(x^{(i)}))}\frac{\partial  p_1}{\partial \beta_j} -
\frac{1- y^{(i)}}{(1-p_1(x^{(i)}))}\frac{\partial  p_1}{\partial \beta_j} \\
 &amp;= \left( \frac{y^{(i)} - p_1(x^{(i)})}{(p_1(x^{(i)}))(1-p_1(x^{(i)}))}  \right )\frac{\partial  p_1}{\partial \beta_j} \\
 &amp; = \left ( y^{(i)} - p_1(x^{(i)}) \right ) x_j^{(i)} \\ 
\end{align*}\]</span></p>
<p>para <span class="math inline">\(j=0,1,\ldots,p\)</span>, usando la convención de <span class="math inline">\(x_0^{(i)}=1\)</span>. Podemos sumar
ahora sobre la muestra de entrenamiento para obtener</p>
<p><span class="math display">\[ \frac{\partial D}{\partial\beta_j} = - 2\sum_{i=1}^N  (y^{(i)}-p(x^{(i)}))x_j^{(i)}\]</span></p>
<p>De modo que,</p>

<div class="comentario">
Para un paso <span class="math inline">\(\eta&gt;0\)</span> fijo, la iteración de descenso para regresión logística para
el coeficiente <span class="math inline">\(\beta_j\)</span> es:
<span class="math display">\[\beta_{j}^{(k+1)} = \beta_j^{(k)} + {2\eta} \sum_{i=1}^N (y^{(i)}-p(x^{(i)}))x_j^{(i)}\]</span>
para
<span class="math inline">\(j=0,1,\ldots, p\)</span>, donde fijamos <span class="math inline">\(x_0^{(i)}=1\)</span>.
</div>

<p>Podríamos usar las siguientes implementaciones, que representan cambios
menores de lo que hicimos en regresión lineal:</p>
<pre class="sourceCode r"><code class="sourceCode r">devianza_calc &lt;-<span class="st"> </span><span class="cf">function</span>(x, y){
  dev_fun &lt;-<span class="st"> </span><span class="cf">function</span>(beta){
    <span class="co"># usando p</span>
    <span class="co">#p_beta &lt;- h(as.matrix(cbind(1, x)) %*% beta)</span>
    <span class="co">#-2*sum(y*log(p_beta) + (1-y)*log(1-p_beta))</span>
    <span class="co"># usando x*beta</span>
    x_beta &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(<span class="kw">cbind</span>(<span class="dv">1</span>, x)) <span class="op">%*%</span><span class="st"> </span>beta
    <span class="dv">-2</span><span class="op">*</span><span class="kw">sum</span>(y<span class="op">*</span>x_beta <span class="op">-</span><span class="st"> </span><span class="kw">log</span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(x_beta)))
  }
  dev_fun
}

grad_calc &lt;-<span class="st"> </span><span class="cf">function</span>(x_ent, y_ent){
  salida_grad &lt;-<span class="st"> </span><span class="cf">function</span>(beta){
    p_beta &lt;-<span class="st"> </span><span class="kw">h</span>(<span class="kw">as.matrix</span>(<span class="kw">cbind</span>(<span class="dv">1</span>, x_ent)) <span class="op">%*%</span><span class="st"> </span>beta) 
    e &lt;-<span class="st"> </span>y_ent <span class="op">-</span><span class="st"> </span>p_beta
    grad_out &lt;-<span class="st"> </span><span class="dv">-2</span><span class="op">*</span><span class="kw">as.numeric</span>(<span class="kw">t</span>(<span class="kw">cbind</span>(<span class="dv">1</span>,x_ent)) <span class="op">%*%</span><span class="st"> </span>e)
    <span class="kw">names</span>(grad_out) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;Intercept&#39;</span>, <span class="kw">colnames</span>(x_ent))
    grad_out
  }
  salida_grad
}
descenso &lt;-<span class="st"> </span><span class="cf">function</span>(n, z_<span class="dv">0</span>, eta, h_deriv){
  z &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>,n, <span class="kw">length</span>(z_<span class="dv">0</span>))
  z[<span class="dv">1</span>, ] &lt;-<span class="st"> </span>z_<span class="dv">0</span>
  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>(n<span class="dv">-1</span>)){
    z[i<span class="op">+</span><span class="dv">1</span>, ] &lt;-<span class="st"> </span>z[i, ] <span class="op">-</span><span class="st"> </span>eta <span class="op">*</span><span class="st"> </span><span class="kw">h_deriv</span>(z[i, ])
  }
  z
}</code></pre>
<div id="ejemplo-15" class="section level4 unnumbered">
<h4>Ejemplo</h4>
<p>Probemos nuestros cálculos con el ejemplo de 1 entrada de tarjetas de crédito.</p>
<pre class="sourceCode r"><code class="sourceCode r">dat_ent<span class="op">$</span>y &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(dat_ent<span class="op">$</span>g<span class="op">==</span><span class="dv">1</span>)
dat_ent &lt;-<span class="st"> </span>dat_ent <span class="op">%&gt;%</span><span class="st"> </span>ungroup <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">x_s =</span> (x <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(x))<span class="op">/</span><span class="kw">sd</span>(x))
devianza &lt;-<span class="st"> </span><span class="kw">devianza_calc</span>(dat_ent[, <span class="st">&#39;x_s&#39;</span>, <span class="dt">drop =</span> <span class="ot">FALSE</span>], dat_ent<span class="op">$</span>y)
grad &lt;-<span class="st"> </span><span class="kw">grad_calc</span>(dat_ent[, <span class="st">&#39;x_s&#39;</span>, <span class="dt">drop =</span> <span class="ot">FALSE</span>], dat_ent<span class="op">$</span>y)
<span class="kw">grad</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>))</code></pre>
<pre><code>## Intercept       x_s 
## -354.2728  363.2408</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">grad</span>(<span class="kw">c</span>(<span class="fl">0.5</span>,<span class="op">-</span><span class="fl">0.1</span>))</code></pre>
<pre><code>## Intercept       x_s 
## -217.8069  140.9315</code></pre>
<p>Verificamos cálculo de gradiente:</p>
<pre class="sourceCode r"><code class="sourceCode r">(<span class="kw">devianza</span>(<span class="kw">c</span>(<span class="fl">0.5+0.0001</span>,<span class="op">-</span><span class="fl">0.1</span>)) <span class="op">-</span><span class="st"> </span><span class="kw">devianza</span>(<span class="kw">c</span>(<span class="fl">0.5</span>,<span class="op">-</span><span class="fl">0.1</span>)))<span class="op">/</span><span class="fl">0.0001</span></code></pre>
<pre><code>## [1] -217.7951</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">(<span class="kw">devianza</span>(<span class="kw">c</span>(<span class="fl">0.5</span>,<span class="op">-</span><span class="fl">0.1+0.0001</span>)) <span class="op">-</span><span class="st"> </span><span class="kw">devianza</span>(<span class="kw">c</span>(<span class="fl">0.5</span>,<span class="op">-</span><span class="fl">0.1</span>)))<span class="op">/</span><span class="fl">0.0001</span></code></pre>
<pre><code>## [1] 140.9435</code></pre>
<p>Y hacemos descenso:</p>
<pre class="sourceCode r"><code class="sourceCode r">iteraciones &lt;-<span class="st"> </span><span class="kw">descenso</span>(<span class="dv">100</span>, <span class="dt">z_0=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>), <span class="dt">eta =</span> <span class="fl">0.001</span>, <span class="dt">h_deriv =</span> grad)
<span class="kw">tail</span>(iteraciones, <span class="dv">20</span>)</code></pre>
<pre><code>##            [,1]      [,2]
##  [81,] 2.013788 -1.082934
##  [82,] 2.014013 -1.083081
##  [83,] 2.014223 -1.083218
##  [84,] 2.014419 -1.083345
##  [85,] 2.014602 -1.083465
##  [86,] 2.014773 -1.083576
##  [87,] 2.014932 -1.083680
##  [88,] 2.015081 -1.083777
##  [89,] 2.015220 -1.083868
##  [90,] 2.015350 -1.083952
##  [91,] 2.015471 -1.084031
##  [92,] 2.015585 -1.084105
##  [93,] 2.015690 -1.084174
##  [94,] 2.015789 -1.084238
##  [95,] 2.015881 -1.084298
##  [96,] 2.015967 -1.084354
##  [97,] 2.016048 -1.084407
##  [98,] 2.016123 -1.084456
##  [99,] 2.016193 -1.084501
## [100,] 2.016258 -1.084544</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(<span class="kw">apply</span>(iteraciones, <span class="dv">1</span>, devianza))</code></pre>
<p><img src="03-clasificacion_files/figure-html/unnamed-chunk-32-1.png" width="480" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">matplot</span>(iteraciones)</code></pre>
<p><img src="03-clasificacion_files/figure-html/unnamed-chunk-32-2.png" width="480" /></p>
<p>Comparamos con glm:</p>
<pre class="sourceCode r"><code class="sourceCode r">mod_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">glm</span>(y<span class="op">~</span>x_s, <span class="dt">data=</span>dat_ent, <span class="dt">family =</span> <span class="st">&#39;binomial&#39;</span>) 
<span class="kw">coef</span>(mod_<span class="dv">1</span>)</code></pre>
<pre><code>## (Intercept)         x_s 
##    2.017181   -1.085146</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">mod_<span class="dv">1</span><span class="op">$</span>deviance</code></pre>
<pre><code>## [1] 351.676</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">devianza</span>(iteraciones[<span class="dv">100</span>,])</code></pre>
<pre><code>## [1] 351.676</code></pre>
<p>Nótese que esta devianza está calculada sin dividir intre entre el número de casos. Podemos calcular la devianza promedio de entrenamiento haciendo:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">devianza</span>(iteraciones[<span class="dv">100</span>,])<span class="op">/</span><span class="kw">nrow</span>(dat_ent)</code></pre>
<pre><code>## [1] 0.703352</code></pre>
</div>
</div>
<div id="observaciones-adicionales" class="section level2">
<h2><span class="header-section-number">3.6</span> Observaciones adicionales</h2>
<div id="maxima-verosimilitud" class="section level4 unnumbered">
<h4>Máxima verosimilitud</h4>
<p>Es fácil ver que este método de estimación de los coeficientes (minimizando la
devianza de entrenamiento) es el método de máxima verosimilitud. La verosimilitud
de la muestra de entrenamiento está dada por:</p>
<p><span class="math display">\[L(\beta) =\prod_{i=1}^N p_{y^{(i)}} (x^{(i)})\]</span>
Y la log verosimilitud es</p>
<p><span class="math display">\[l(\beta) =\sum_{i=1}^N \log(p_{y^{(i)}} (x^{(i)})).\]</span></p>
<p>Así que ajustar el modelo minimizando la expresión
<a href="regresion-logistica.html#eq:devianza">(3.1)</a>
es los mismo que hacer máxima verosimilitud (condicional a los valores de <span class="math inline">\(x\)</span>).</p>
</div>
<div id="normalizacion" class="section level4 unnumbered">
<h4>Normalización</h4>
<p>Igual que en regresión lineal, en regresión logística conviene normalizar
las entradas antes de ajustar el modelo</p>
</div>
<div id="desempeno-de-regresion-logistica-como-metodo-de-aprendizaje" class="section level4 unnumbered">
<h4>Desempeño de regresión logística como método de aprendizaje</h4>
<p>Igual que en regresión lineal, regresión logística supera a métodos
más sofisticados o nuevos en numerosos ejemplos. Las razones son similares:
la rigidez de regresión logística es una fortaleza cuando la estructura
lineal es una buena aproximación.</p>
</div>
</div>
<div id="ejercicio-datos-de-diabetes" class="section level2 unnumbered">
<h2>Ejercicio: datos de diabetes</h2>
<p>Ya están divididos los datos en entrenamiento y prueba</p>
<pre class="sourceCode r"><code class="sourceCode r">diabetes_ent &lt;-<span class="st"> </span><span class="kw">as_data_frame</span>(MASS<span class="op">::</span>Pima.tr)
diabetes_pr &lt;-<span class="st"> </span><span class="kw">as_data_frame</span>(MASS<span class="op">::</span>Pima.te)
diabetes_ent</code></pre>
<pre><code>## # A tibble: 200 x 8
##    npreg   glu    bp  skin   bmi   ped   age type 
##  * &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;
##  1     5    86    68    28  30.2 0.364    24 No   
##  2     7   195    70    33  25.1 0.163    55 Yes  
##  3     5    77    82    41  35.8 0.156    35 No   
##  4     0   165    76    43  47.9 0.259    26 No   
##  5     0   107    60    25  26.4 0.133    23 No   
##  6     5    97    76    27  35.6 0.378    52 Yes  
##  7     3    83    58    31  34.3 0.336    25 No   
##  8     1   193    50    16  25.9 0.655    24 No   
##  9     3   142    80    15  32.4 0.2      63 No   
## 10     2   128    78    37  43.3 1.22     31 Yes  
## # ... with 190 more rows</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">diabetes_ent<span class="op">$</span>id &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(diabetes_ent)
diabetes_pr<span class="op">$</span>id &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(diabetes_pr)</code></pre>
<p>Normalizamos</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(dplyr)
<span class="kw">library</span>(tidyr)
datos_norm &lt;-<span class="st"> </span>diabetes_ent <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">gather</span>(variable, valor, npreg<span class="op">:</span>age) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(variable) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="dt">media =</span> <span class="kw">mean</span>(valor), <span class="dt">de =</span> <span class="kw">sd</span>(valor))

normalizar &lt;-<span class="st"> </span><span class="cf">function</span>(datos, datos_norm){
  datos <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">gather</span>(variable, valor, npreg<span class="op">:</span>age) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">left_join</span>(datos_norm) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">valor_s =</span> (valor  <span class="op">-</span><span class="st"> </span>media)<span class="op">/</span>de) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">select</span>(id, type, variable, valor_s) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">spread</span>(variable, valor_s)
}

diabetes_ent_s &lt;-<span class="st"> </span><span class="kw">normalizar</span>(diabetes_ent, datos_norm)
diabetes_pr_s &lt;-<span class="st"> </span><span class="kw">normalizar</span>(diabetes_pr, datos_norm)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">x_ent &lt;-<span class="st"> </span>diabetes_ent_s <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(age<span class="op">:</span>skin) <span class="op">%&gt;%</span><span class="st"> </span>as.matrix
p &lt;-<span class="st"> </span><span class="kw">ncol</span>(x_ent)
y_ent &lt;-<span class="st"> </span>diabetes_ent_s<span class="op">$</span>type <span class="op">==</span><span class="st"> &#39;Yes&#39;</span>
grad &lt;-<span class="st"> </span><span class="kw">grad_calc</span>(x_ent, y_ent)
iteraciones &lt;-<span class="st"> </span><span class="kw">descenso</span>(<span class="dv">1000</span>, <span class="kw">rep</span>(<span class="dv">0</span>, p <span class="op">+</span><span class="st"> </span><span class="dv">1</span>), <span class="fl">0.001</span>, <span class="dt">h_deriv =</span> grad)
<span class="kw">matplot</span>(iteraciones)</code></pre>
<p><img src="03-clasificacion_files/figure-html/unnamed-chunk-37-1.png" width="672" /></p>
<pre class="sourceCode r"><code class="sourceCode r">diabetes_coef &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">variable =</span> <span class="kw">c</span>(<span class="st">&#39;Intercept&#39;</span>,<span class="kw">colnames</span>(x_ent)), <span class="dt">coef =</span> iteraciones[<span class="dv">1000</span>,])
diabetes_coef</code></pre>
<pre><code>## # A tibble: 8 x 2
##   variable     coef
##   &lt;chr&gt;       &lt;dbl&gt;
## 1 Intercept -0.956 
## 2 age        0.452 
## 3 bmi        0.513 
## 4 bp        -0.0547
## 5 glu        1.02  
## 6 npreg      0.347 
## 7 ped        0.559 
## 8 skin      -0.0225</code></pre>
<p>Ahora calculamos devianza de prueba y error de clasificación:</p>
<pre class="sourceCode r"><code class="sourceCode r">x_prueba &lt;-<span class="st"> </span>diabetes_pr_s <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(age<span class="op">:</span>skin) <span class="op">%&gt;%</span><span class="st"> </span>as.matrix
y_prueba &lt;-<span class="st"> </span>diabetes_pr_s<span class="op">$</span>type <span class="op">==</span><span class="st"> &#39;Yes&#39;</span>
dev_prueba &lt;-<span class="st"> </span><span class="kw">devianza_calc</span>(x_prueba, y_prueba)
<span class="kw">dev_prueba</span>(iteraciones[<span class="dv">1000</span>,])<span class="op">/</span><span class="kw">nrow</span>(x_prueba)</code></pre>
<pre><code>## [1] 0.8813972</code></pre>
<p>Y para el error clasificación de prueba, necesitamos las probabilidades de clase ajustadas:</p>
<pre class="sourceCode r"><code class="sourceCode r">beta &lt;-<span class="st"> </span>iteraciones[<span class="dv">1000</span>, ]
p_beta &lt;-<span class="st"> </span><span class="kw">h</span>(<span class="kw">as.matrix</span>(<span class="kw">cbind</span>(<span class="dv">1</span>, x_prueba)) <span class="op">%*%</span><span class="st"> </span>beta) 
y_pred &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(p_beta <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>)
<span class="kw">mean</span>(y_prueba <span class="op">!=</span><span class="st"> </span>y_pred)</code></pre>
<pre><code>## [1] 0.1987952</code></pre>
</div>
<div id="mas-sobre-problemas-de-clasificacion" class="section level2">
<h2><span class="header-section-number">3.7</span> Más sobre problemas de clasificación</h2>
<p>En esta parte presentamos técnicas adicionales para evaluar el
desempeño de un modelo. En la parte anterior vimos que</p>
<ul>
<li><p>La <strong>devianza</strong> es una buena medida para ajustar y evaluar el desempeño de un modelo y
comparar modelos, y utiliza las probabilidades de clase. Sin embargo, es una medida de dificil de interpretar en cuanto
a los errores que podemos esperar del modelo.</p></li>
<li><p>Por otro lado, la <strong>tasa de clasificación incorrecta</strong> puede
usarse para evaluar el desempeño de un clasificador
(incluyendo uno derivado de probabilidades de clase), puede interpretarse
con facilidad,
pero se queda corta en muchas aplicaciones. Una deficiencia grande
de esta medida es que, contrario al problema de regresión, hay errores
de clasificación que son cualitativamente diferentes.</p></li>
</ul>
<div id="ejemplo-16" class="section level4 unnumbered">
<h4>Ejemplo</h4>
<ul>
<li><p>Por ejemplo, diagnosticar a alguien con una enfermedad cuando no la tiene
tiene consecuencias distintas a diagnosticar como libre de enfermedad a alguien
que la tiene. Estas consecuencias dependen de cómo son son los tratamientos consecuentes, de y qué tan peligrosa es la enfermedad.</p></li>
<li><p>Cuando usamos un buscador como Google, es cualitativamente diferente que el
buscador omita resultados relevantes a que nos presente resultados irrelevantes.</p></li>
<li><p>¿Otros ejemplos?</p></li>
</ul>
<p>En general, los costos de los distintos errores son distintos, y en muchos
problemas quiséramos entenderlos y controlarlos individualmente. Aunque en teoría
podríamos asignar costos a los errores y definir una función de pérdida apropiada,
en la práctica esto muchas veces no es tan fácil o deseable. Podemos, sin embargo,
reportar el tipo de errores que ocurren</p>

<div class="comentario">
<p><strong>Matriz de confusión</strong>.</p>
<p>Sea <span class="math inline">\(\hat{G}\)</span> un clasificador binario. La matriz de confusión <span class="math inline">\(C\)</span> de <span class="math inline">\(\hat{G}\)</span> está
dada por</p>
<span class="math inline">\(C_{i,j} = \text{Número de casos de la clase verdadera j que son clasificados como clase i
 por el clasificador}\)</span>
</div>

</div>
<div id="ejemplo-17" class="section level4 unnumbered">
<h4>Ejemplo</h4>
<p>En un ejemplo de tres clases, podríamos obtener la matriz de confusión:</p>
<table>
<thead>
<tr class="header">
<th align="right">A</th>
<th></th>
<th>B C</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">A.pred</td>
<td>50</td>
<td>2</td>
<td>0</td>
</tr>
<tr class="even">
<td align="right">B.pred</td>
<td>20</td>
<td>105</td>
<td>10</td>
</tr>
<tr class="odd">
<td align="right">C.pred</td>
<td>20</td>
<td>10</td>
<td>30</td>
</tr>
</tbody>
</table>
<p>Esto quiere decir que de 90 casos de clase <span class="math inline">\(A\)</span>, sólo clasificamos
a 50 en la clase correcta, de 117 casos de clase <span class="math inline">\(B\)</span>, acertamos en 105, etcétera.
Podemos ver esta tabla de distintas formas, por ejemplo, usando porcentajes
por columna, nos dice cómo se distribuyen los casos de cada clase:</p>
<pre class="sourceCode r"><code class="sourceCode r">knitr<span class="op">::</span><span class="kw">kable</span>(<span class="kw">round</span>(<span class="kw">prop.table</span>(tabla_<span class="dv">1</span>, <span class="dv">2</span>),<span class="dv">2</span>))</code></pre>
<table>
<thead>
<tr class="header">
<th></th>
<th align="center">A</th>
<th align="center">B</th>
<th align="center">C</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A.pred</td>
<td align="center">0.56</td>
<td align="center">0.02</td>
<td align="center">0.00</td>
</tr>
<tr class="even">
<td>B.pred</td>
<td align="center">0.22</td>
<td align="center">0.90</td>
<td align="center">0.25</td>
</tr>
<tr class="odd">
<td>C.pred</td>
<td align="center">0.22</td>
<td align="center">0.09</td>
<td align="center">0.75</td>
</tr>
</tbody>
</table>
<p>Mientras que una tabla de porcentajes por renglón nos muestra
qué pasa cada vez que hacemos una predicción dada:</p>
<pre class="sourceCode r"><code class="sourceCode r">knitr<span class="op">::</span><span class="kw">kable</span>(<span class="kw">round</span>(<span class="kw">prop.table</span>(tabla_<span class="dv">1</span>, <span class="dv">1</span>),<span class="dv">2</span>))</code></pre>
<table>
<thead>
<tr class="header">
<th></th>
<th align="center">A</th>
<th align="center">B</th>
<th align="center">C</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A.pred</td>
<td align="center">0.96</td>
<td align="center">0.04</td>
<td align="center">0.00</td>
</tr>
<tr class="even">
<td>B.pred</td>
<td align="center">0.15</td>
<td align="center">0.78</td>
<td align="center">0.07</td>
</tr>
<tr class="odd">
<td>C.pred</td>
<td align="center">0.33</td>
<td align="center">0.17</td>
<td align="center">0.50</td>
</tr>
</tbody>
</table>
<p>Ahora pensemos cómo podría sernos de utilidad esta tabla. Discute</p>
<ul>
<li><p>El clasificador fuera uno de severidad de emergencias en un hospital,
donde A=requiere atención inmediata B=urgente C=puede posponerse.</p></li>
<li><p>El clasificador fuera de tipos de cliente de un negocio. Por ejemplo,
A = cliente de gasto potencial alto, B=cliente medio, C=abandonador. Imagínate
que tiene un costo intentar conservar a un abandonador, y hay una inversión
alta para tratar a los clientes A.</p></li>
</ul>
<p>La tasa de incorrectas es la misma en los dos ejemplos, pero la adecuación
del clasificador es muy diferente.</p>
</div>
<div id="analisis-de-error-para-clasificadores-binarios" class="section level3">
<h3><span class="header-section-number">3.7.1</span> Análisis de error para clasificadores binarios</h3>
<p>Cuando la variable a predecir es binaria (dos clases), podemos
etiquetar una clase como <em>positivo</em> y otra como <em>negativo</em>. En el fondo
no importa cómo catalogemos cada clase, pero para problemas particulares
una asignación puede ser más natural. Por ejemplo, en diagnóstico de
enfermedades, positivo=tiene la enfermedad, en análisis de crédito,
positivo=cae en impago, en sistemas de recomendacion, positivo = le gusta
el producto X, en recuperación de textos, positivo=el documento es relevante a la
búsqueda, etc.</p>

<div class="comentario">
<p>Hay dos tipos de errores en un clasificador binario (positivo - negativo):</p>
<ul>
<li>Falsos positivos (fp): clasificar como positivo a un caso negativo.</li>
<li>Falsos negativos (fn): clasificar como negativo a un caso positivo.</li>
</ul>
A los casos clasificados correctamente les llamamos positivos verdaderos (pv)
y negativos verdaderos (nv).
</div>

<p>La matriz de confusion es entonces</p>
<pre class="sourceCode r"><code class="sourceCode r">tabla &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="st">&#39;-&#39;</span> =<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;positivo.pred&#39;</span>,<span class="st">&#39;negativo.pred&#39;</span>,<span class="st">&#39;total&#39;</span>),
                    <span class="st">&#39;positivo&#39;</span>=<span class="kw">c</span>(<span class="st">&#39;pv&#39;</span>,<span class="st">&#39;fn&#39;</span>,<span class="st">&#39;pos&#39;</span>),
                    <span class="st">&#39;negativo&#39;</span>=<span class="kw">c</span>(<span class="st">&#39;fp&#39;</span>,<span class="st">&#39;nv&#39;</span>,<span class="st">&#39;neg&#39;</span>),
                    <span class="st">&#39;total&#39;</span> =<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;pred.pos&#39;</span>,<span class="st">&#39;pred.neg&#39;</span>,<span class="st">&#39;&#39;</span>))
knitr<span class="op">::</span><span class="kw">kable</span>(tabla)</code></pre>
<ul>
<li><table>
<thead>
<tr class="header">
<th align="right">po</th>
<th>sitivo ne</th>
<th>gativo to</th>
<th align="left">tal</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">positivo.pred</td>
<td>pv</td>
<td>fp</td>
<td align="left">pred.pos</td>
</tr>
<tr class="even">
<td align="right">negativo.pred</td>
<td>fn</td>
<td>nv</td>
<td align="left">pred.neg</td>
</tr>
<tr class="odd">
<td align="right">total</td>
<td>pos</td>
<td>neg</td>
<td align="left"></td>
</tr>
</tbody>
</table></li>
</ul>
<p>Nótese que un clasificador bueno, en general, es uno
que tiene la mayor parte de los casos en la diagonal de la matriz
de confusión.</p>
<p>Podemos estudiar a nuestro clasificador en términos de las proporciones de casos que caen en cada celda, que dependen del desempeño del clasificador en cuanto a casos positivos y negativos. La nomenclatura es
confusa, pues en distintas áreas se usan distintos nombres para estas proporciones:</p>
<ul>
<li><p>Tasa de falsos positivos
<span class="math display">\[\frac{fp}{fp+nv}=\frac{fp}{neg}\]</span></p></li>
<li><p>Tasa de falsos negativos
<span class="math display">\[\frac{fn}{pv+fn}=\frac{fn}{pos}\]</span></p></li>
<li><p>Especificidad
<span class="math display">\[\frac{nv}{fp+nv}=\frac{nv}{neg}\]</span></p></li>
<li><p>Sensibilidad o Recall
<span class="math display">\[\frac{pv}{pv+fn}=\frac{pv}{pos}\]</span></p></li>
</ul>
<p>Y también otras que tienen como base las predicciones:</p>
<ul>
<li><p>Valor predictivo positivo o Precisión
<span class="math display">\[\frac{vp}{vp+fp}=\frac{vp}{pred.pos}\]</span></p></li>
<li><p>Valor predictivo negativo
<span class="math display">\[\frac{vn}{fn+vn}=\frac{vn}{pred.neg}\]</span></p></li>
</ul>
<p>Y hay varias medidas resumen que ponderan de distinta forma</p>
<ul>
<li><p>Tasa de clasificación incorrecta
<span class="math display">\[\frac{fn+fv}{neg+pos}\]</span></p></li>
<li><p>Medida F (media armónica de precisión y recall)
<span class="math display">\[2\frac{precision \cdot recall}{precision +  recall}\]</span></p></li>
<li><p>AUC (area bajo la curva ROC) ver más adelante</p></li>
<li><p>Kappa
<span class="math display">\[\kappa = \frac{p_o - p_e}{1-p_e},\]</span>
donde <span class="math inline">\(p_o =\)</span> tasa de correctos, y
<span class="math inline">\(p_e\)</span> es la probabilidad de clasificar correctamente al azar, dado por
<span class="math display">\[p_e = \frac{pos}{total}\frac{pred.pos}{total} + \frac{neg}{total}\frac{pred.neg}{total}\]</span></p></li>
</ul>
<p>Dependiendo de el tema y el objetivo hay medidas más naturales que otras:</p>
<ul>
<li>En pruebas clínicas, se usa típicamente sensibilidad y especificidad (proporción de positivos que detectamos y proporción de negativos que descartamos).</li>
<li>En búsqueda y recuperación de documentos (positivo=el documento es relevante, negativo=el documento no es relevante), se usa precisión y recall (precisión=de los documentos que entregamos (predicción positiva), cuáles son realmente positivos/relevantes, y recall=de todos los documentos relevantes, cuáles devolvemos). Aquí la tasa de falsos positivos (de todos los negativos, cuáles se predicen positivos), por ejemplo, no es de ayuda pues generalmente son bajas y no discriminan el desempeño de los clasificadores. La razón es que típicamente hay una gran cantidad de negativos, y se devuelven relativamente pocos documentos, de forma que la tasa de falsos positivos generalmente es muy pequeña.</li>
<li><span class="math inline">\(\kappa\)</span> señala un problema importante cuando interpretamos tasas de correctos.
Por ejemplo, supongamos que hay un 85% de positivos y un 15% de negativos. Si nuestro
clasificador clasifica todo a positivo, nuestra tasa de correctos sería 85% - pero
nuestro clasificador no está aprovechando los datos. En este caso,
<span class="math display">\[p_e = 0.85(1) + 0.15(0)= 0.85\]</span>,
y tenemos que <span class="math inline">\(\kappa = 0\)</span> (similar al azar). Supongamos por otra parte
que escogemos 50% del tiempo positivo al azar. Esto quiere decir que
tendríamos <span class="math inline">\(p_o=0.5\)</span>. Pero
<span class="math display">\[p_e = 0.85(0.50) + 0.15(0.50) = 0.50,\]</span>
de modo que otra vez <span class="math inline">\(\kappa = 0\)</span>. <span class="math inline">\(\kappa\)</span> es un valor entre 0 y 1 que mide
qué tan superior es nuestro clasificador a uno dado al azar (uno que la predicción
no tiene qué ver con la clase verdadera).</li>
</ul>
</div>
<div id="regresion-logistica-para-problemas-de-mas-de-2-clases" class="section level3">
<h3><span class="header-section-number">3.7.2</span> Regresión logística para problemas de más de 2 clases</h3>
<p>Consideramos ahora un problema con más de dos clases, de manera que <span class="math inline">\(G ∈ {1,2,...,K}\)</span>
(<span class="math inline">\(K\)</span> clases), y tenemos <span class="math inline">\(X = (X1 ...,Xp)\)</span> entradas.
¿Cómo generalizar el modelo de regresión logística a este problema?
Una estrategia es la de uno contra todos:</p>
<p>En clasificación uno contra todos, hacemos</p>
<ol style="list-style-type: decimal">
<li><p>Para cada clase <span class="math inline">\(g\in\{1,\ldots,K\}\)</span> entrenamos un modelo de regresión
logística (binaria) <span class="math inline">\(\hat{p}^{(g)}(x)\)</span>, tomando como positivos a los casos de 1
clase <span class="math inline">\(g\)</span>, y como negativos a todo el resto. Esto lo hacemos como en las secciones anteriores, y de manera independiente para cada clase.</p></li>
<li><p>Para clasificar un nuevo caso <span class="math inline">\(x\)</span>,
calculamos
<span class="math display">\[\hat{p}^{(1)}, \hat{p}^{(2)},\ldots, \hat{p}^{(K)}\]</span></p></li>
</ol>
<p>y clasificamos a la clase de máxima probabilidad
<span class="math display">\[\hat{G}(x) = \arg\max_g \hat{p}^{(g)}(x)\]</span>
Nótese que no hay ninguna garantía de que las probabilidades de clase
sumen 1, pues se trata de estimaciones independientes de cada clase. En este sentido, produce estimaciones que en realidad no satisfacen las propiedades del modelo de probabilidad establecido. Sin embargo, esta estrategia es simple y en
muchos casos funciona bien.</p>
</div>
<div id="regresion-logistica-multinomial" class="section level3">
<h3><span class="header-section-number">3.7.3</span> Regresión logística multinomial</h3>
<p>Si queremos obtener estimaciones de las probabilidades de clase que sumen uno, entonces tenemos que contruir las estimaciones de cada clase de clase de manera conjunta.
Como vimos antes, tenemos que estimar, para cada <span class="math inline">\(x\)</span> y <span class="math inline">\(g\in\{1,\ldots, K\}\)</span>,
las probabilidades condicionales de clase:
<span class="math display">\[p_g(x) = P(G = g|X = x).\]</span></p>
<p>Podemos generalizar para más de 2 clases usando una idea similar:</p>
<p><span class="math display">\[p_1(x) =  \exp(\beta_{0,1} + \beta_{1,1}x_1 + \ldots + \beta_{p,1} x_p)/Z\]</span></p>
<p><span class="math display">\[p_2(x) =  \exp(\beta_{0,2} + \beta_{1,2}x_2 + \ldots + \beta_{p.2} x_p)/Z\]</span>
hasta
<span class="math display">\[p_{K-1}(x) =  \exp(\beta_{0,{K-1}} + \beta_{1,{K-1}}x_2 + \ldots + \beta_{p,{K-1}} x_p)/Z\]</span>
y
<span class="math display">\[p_K(x) = \exp(\beta_{0,{K}} + \beta_{1,{K}}x_2 + \ldots + \beta_{p,{K}} x_p)/Z\]</span></p>
<p>En este caso, para que las probabilidades sumen 1, necesitamos que
<span class="math display">\[Z =  \sum_{j=1}^{K}\exp(\beta_0^j + \beta_1^jx_2 + \ldots + \beta_p^j x_p)\]</span></p>
<p>Para ajustar coeficientes, usamos el mismo criterio de devianza de entrenamiento.
Buscamos minimizar:
<span class="math display">\[D(\beta)=−2 \sum_{i=1}^N p_{g^{(i)}}(x^{(i)}),\]</span>
Donde <span class="math inline">\(\beta\)</span> contiene todos los coeficientes organizados en un vector
de tamaño <span class="math inline">\((p+1)(K+1)\)</span>:
<span class="math display">\[\beta = ( \beta_0^1, \beta_1^1, \ldots , \beta_p^1,  \beta_0^2, \beta_1^2, \ldots , \beta_p^2, \ldots \beta_0^{K}, \beta_1^{K}, \ldots , \beta_p^{K} )\]</span></p>
<p>Y ahora podemos usar algún método númerico para minimizar la devianza (por ejemplo,
descenso en gradiente). Cuando
es muy importante tener probabilidades bien calibradas, el enfoque multinomial
es más apropiado, pero muchas veces, especialmente si sólo nos interesa clasificar, los
dos métodos dan resultados similares.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="regresion-lineal.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="regresion-regularizada.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/felipegonzalez/aprendizaje-maquina-verano-2018/edit/master/03-clasificacion.Rmd",
"text": "Edit"
},
"download": ["am-curso-verano.pdf", "am-curso-verano.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
