<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Minicurso de verano de Aprendizaje Máquina</title>
  <meta name="description" content="Minicurso de Aprendizaje Máquina, ITAM 2018.">
  <meta name="generator" content="bookdown 0.7.12 and GitBook 2.6.7">

  <meta property="og:title" content="Minicurso de verano de Aprendizaje Máquina" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Minicurso de Aprendizaje Máquina, ITAM 2018." />
  <meta name="github-repo" content="felipegonzalez/aprendizaje-maquina-verano-2018" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Minicurso de verano de Aprendizaje Máquina" />
  
  <meta name="twitter:description" content="Minicurso de Aprendizaje Máquina, ITAM 2018." />
  

<meta name="author" content="Felipe González">


<meta name="date" content="2018-06-12">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="diagnostico-y-mejora-de-modelos.html">
<link rel="next" href="validacion-de-modelos-problemas-comunes.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.2/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.7.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotlyjs-1.29.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotlyjs-1.29.2/plotly-latest.min.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="toc.css" type="text/css" />
<link rel="stylesheet" href="font-awesome.min.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Minicurso aprendizaje máquina</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Temario</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#referencias"><i class="fa fa-check"></i>Referencias</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#software"><i class="fa fa-check"></i>Software</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduccion.html"><a href="introduccion.html"><i class="fa fa-check"></i><b>1</b> Introducción</a><ul>
<li class="chapter" data-level="1.1" data-path="introduccion.html"><a href="introduccion.html#que-es-aprendizaje-de-maquina-machine-learning"><i class="fa fa-check"></i><b>1.1</b> ¿Qué es aprendizaje de máquina (machine learning)?</a></li>
<li class="chapter" data-level="1.2" data-path="introduccion.html"><a href="introduccion.html#aprendizaje-supervisado"><i class="fa fa-check"></i><b>1.2</b> Aprendizaje Supervisado</a><ul>
<li class="chapter" data-level="1.2.1" data-path="introduccion.html"><a href="introduccion.html#proceso-generador-de-datos-modelo-teorico"><i class="fa fa-check"></i><b>1.2.1</b> Proceso generador de datos (modelo teórico)</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="introduccion.html"><a href="introduccion.html#predicciones"><i class="fa fa-check"></i><b>1.3</b> Predicciones</a></li>
<li class="chapter" data-level="1.4" data-path="introduccion.html"><a href="introduccion.html#cuantificacion-de-error-o-precision"><i class="fa fa-check"></i><b>1.4</b> Cuantificación de error o precisión</a></li>
<li class="chapter" data-level="1.5" data-path="introduccion.html"><a href="introduccion.html#aprendizaje"><i class="fa fa-check"></i><b>1.5</b> Tarea de aprendizaje supervisado</a><ul>
<li class="chapter" data-level="" data-path="introduccion.html"><a href="introduccion.html#observaciones"><i class="fa fa-check"></i>Observaciones</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="introduccion.html"><a href="introduccion.html#por-que-tenemos-errores"><i class="fa fa-check"></i><b>1.6</b> ¿Por qué tenemos errores?</a></li>
<li class="chapter" data-level="1.7" data-path="introduccion.html"><a href="introduccion.html#resumen"><i class="fa fa-check"></i><b>1.7</b> Resumen</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="regresion-lineal.html"><a href="regresion-lineal.html"><i class="fa fa-check"></i><b>2</b> Regresión lineal</a><ul>
<li class="chapter" data-level="2.1" data-path="regresion-lineal.html"><a href="regresion-lineal.html#introduccion-1"><i class="fa fa-check"></i><b>2.1</b> Introducción</a></li>
<li class="chapter" data-level="2.2" data-path="regresion-lineal.html"><a href="regresion-lineal.html#aprendizaje-de-coeficientes-ajuste"><i class="fa fa-check"></i><b>2.2</b> Aprendizaje de coeficientes (ajuste)</a></li>
<li class="chapter" data-level="2.3" data-path="regresion-lineal.html"><a href="regresion-lineal.html#descenso-en-gradiente"><i class="fa fa-check"></i><b>2.3</b> Descenso en gradiente</a><ul>
<li class="chapter" data-level="2.3.1" data-path="regresion-lineal.html"><a href="regresion-lineal.html#seleccion-de-tamano-de-paso-eta"><i class="fa fa-check"></i><b>2.3.1</b> Selección de tamaño de paso <span class="math inline">\(\eta\)</span></a></li>
<li class="chapter" data-level="2.3.2" data-path="regresion-lineal.html"><a href="regresion-lineal.html#funciones-de-varias-variables"><i class="fa fa-check"></i><b>2.3.2</b> Funciones de varias variables</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="regresion-lineal.html"><a href="regresion-lineal.html#descenso-en-gradiente-para-regresion-lineal"><i class="fa fa-check"></i><b>2.4</b> Descenso en gradiente para regresión lineal</a></li>
<li class="chapter" data-level="2.5" data-path="regresion-lineal.html"><a href="regresion-lineal.html#normalizacion-de-entradas"><i class="fa fa-check"></i><b>2.5</b> Normalización de entradas</a></li>
<li class="chapter" data-level="2.6" data-path="regresion-lineal.html"><a href="regresion-lineal.html#interpretacion-de-modelos-lineales"><i class="fa fa-check"></i><b>2.6</b> Interpretación de modelos lineales</a></li>
<li class="chapter" data-level="2.7" data-path="regresion-lineal.html"><a href="regresion-lineal.html#por-que-el-modelo-lineal-funciona-bien-muchas-veces"><i class="fa fa-check"></i><b>2.7</b> ¿Por qué el modelo lineal funciona bien (muchas veces)?</a><ul>
<li class="chapter" data-level="2.7.1" data-path="regresion-lineal.html"><a href="regresion-lineal.html#k-vecinos-mas-cercanos"><i class="fa fa-check"></i><b>2.7.1</b> k vecinos más cercanos</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="regresion-lineal.html"><a href="regresion-lineal.html#ejercicio-1"><i class="fa fa-check"></i>Ejercicio</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="regresion-logistica.html"><a href="regresion-logistica.html"><i class="fa fa-check"></i><b>3</b> Regresión logística</a><ul>
<li class="chapter" data-level="3.1" data-path="regresion-logistica.html"><a href="regresion-logistica.html#el-problema-de-clasificacion"><i class="fa fa-check"></i><b>3.1</b> El problema de clasificación</a><ul>
<li class="chapter" data-level="" data-path="regresion-logistica.html"><a href="regresion-logistica.html#que-estimar-en-problemas-de-clasificacion"><i class="fa fa-check"></i>¿Qué estimar en problemas de clasificación?</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="regresion-logistica.html"><a href="regresion-logistica.html#estimacion-de-probabilidades-de-clase"><i class="fa fa-check"></i><b>3.2</b> Estimación de probabilidades de clase</a><ul>
<li class="chapter" data-level="" data-path="regresion-logistica.html"><a href="regresion-logistica.html#ejemplo-9"><i class="fa fa-check"></i>Ejemplo</a></li>
<li class="chapter" data-level="3.2.1" data-path="regresion-logistica.html"><a href="regresion-logistica.html#k-vecinos-mas-cercanos-1"><i class="fa fa-check"></i><b>3.2.1</b> k-vecinos más cercanos</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="regresion-logistica.html"><a href="regresion-logistica.html#error-para-modelos-de-clasificacion"><i class="fa fa-check"></i><b>3.3</b> Error para modelos de clasificación</a><ul>
<li class="chapter" data-level="3.3.1" data-path="regresion-logistica.html"><a href="regresion-logistica.html#ejercicio-2"><i class="fa fa-check"></i><b>3.3.1</b> Ejercicio</a></li>
<li class="chapter" data-level="3.3.2" data-path="regresion-logistica.html"><a href="regresion-logistica.html#error-de-clasificacion-y-funcion-de-perdida-0-1"><i class="fa fa-check"></i><b>3.3.2</b> Error de clasificación y función de pérdida 0-1</a></li>
<li class="chapter" data-level="3.3.3" data-path="regresion-logistica.html"><a href="regresion-logistica.html#discusion-relacion-entre-devianza-y-error-de-clasificacion"><i class="fa fa-check"></i><b>3.3.3</b> Discusión: relación entre devianza y error de clasificación</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="regresion-logistica.html"><a href="regresion-logistica.html#regresion-logistica-1"><i class="fa fa-check"></i><b>3.4</b> Regresión logística</a><ul>
<li class="chapter" data-level="3.4.1" data-path="regresion-logistica.html"><a href="regresion-logistica.html#regresion-logistica-simple"><i class="fa fa-check"></i><b>3.4.1</b> Regresión logística simple</a></li>
<li class="chapter" data-level="3.4.2" data-path="regresion-logistica.html"><a href="regresion-logistica.html#funcion-logistica"><i class="fa fa-check"></i><b>3.4.2</b> Función logística</a></li>
<li class="chapter" data-level="3.4.3" data-path="regresion-logistica.html"><a href="regresion-logistica.html#regresion-logistica-2"><i class="fa fa-check"></i><b>3.4.3</b> Regresión logística</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="regresion-logistica.html"><a href="regresion-logistica.html#aprendizaje-de-coeficientes-para-regresion-logistica-binomial."><i class="fa fa-check"></i><b>3.5</b> Aprendizaje de coeficientes para regresión logística (binomial).</a></li>
<li class="chapter" data-level="3.6" data-path="regresion-logistica.html"><a href="regresion-logistica.html#observaciones-adicionales"><i class="fa fa-check"></i><b>3.6</b> Observaciones adicionales</a></li>
<li class="chapter" data-level="" data-path="regresion-logistica.html"><a href="regresion-logistica.html#ejercicio-datos-de-diabetes"><i class="fa fa-check"></i>Ejercicio: datos de diabetes</a></li>
<li class="chapter" data-level="3.7" data-path="regresion-logistica.html"><a href="regresion-logistica.html#mas-sobre-problemas-de-clasificacion"><i class="fa fa-check"></i><b>3.7</b> Más sobre problemas de clasificación</a><ul>
<li class="chapter" data-level="3.7.1" data-path="regresion-logistica.html"><a href="regresion-logistica.html#analisis-de-error-para-clasificadores-binarios"><i class="fa fa-check"></i><b>3.7.1</b> Análisis de error para clasificadores binarios</a></li>
<li class="chapter" data-level="3.7.2" data-path="regresion-logistica.html"><a href="regresion-logistica.html#regresion-logistica-para-problemas-de-mas-de-2-clases"><i class="fa fa-check"></i><b>3.7.2</b> Regresión logística para problemas de más de 2 clases</a></li>
<li class="chapter" data-level="3.7.3" data-path="regresion-logistica.html"><a href="regresion-logistica.html#regresion-logistica-multinomial"><i class="fa fa-check"></i><b>3.7.3</b> Regresión logística multinomial</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="regresion-regularizada.html"><a href="regresion-regularizada.html"><i class="fa fa-check"></i><b>4</b> Regresión regularizada</a><ul>
<li class="chapter" data-level="4.0.1" data-path="regresion-regularizada.html"><a href="regresion-regularizada.html#sesgo-y-varianza-en-modelos-lineales"><i class="fa fa-check"></i><b>4.0.1</b> Sesgo y varianza en modelos lineales</a></li>
<li class="chapter" data-level="4.0.2" data-path="regresion-regularizada.html"><a href="regresion-regularizada.html#reduciendo-varianza-de-los-coeficientes"><i class="fa fa-check"></i><b>4.0.2</b> Reduciendo varianza de los coeficientes</a></li>
<li class="chapter" data-level="4.1" data-path="regresion-regularizada.html"><a href="regresion-regularizada.html#regularizacion-ridge"><i class="fa fa-check"></i><b>4.1</b> Regularización ridge</a><ul>
<li class="chapter" data-level="4.1.1" data-path="regresion-regularizada.html"><a href="regresion-regularizada.html#seleccion-de-coeficiente-de-regularizacion"><i class="fa fa-check"></i><b>4.1.1</b> Selección de coeficiente de regularización</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="regresion-regularizada.html"><a href="regresion-regularizada.html#entrenamiento-validacion-y-prueba"><i class="fa fa-check"></i><b>4.2</b> Entrenamiento, Validación y Prueba</a><ul>
<li class="chapter" data-level="4.2.1" data-path="regresion-regularizada.html"><a href="regresion-regularizada.html#validacion-cruzada"><i class="fa fa-check"></i><b>4.2.1</b> Validación cruzada</a></li>
<li class="chapter" data-level="" data-path="regresion-regularizada.html"><a href="regresion-regularizada.html#ejercicio-4"><i class="fa fa-check"></i>Ejercicio</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="regresion-regularizada.html"><a href="regresion-regularizada.html#regularizacion-lasso"><i class="fa fa-check"></i><b>4.3</b> Regularización lasso</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="descenso-estocastico.html"><a href="descenso-estocastico.html"><i class="fa fa-check"></i><b>5</b> Descenso estocástico</a><ul>
<li class="chapter" data-level="5.1" data-path="descenso-estocastico.html"><a href="descenso-estocastico.html#algoritmo-de-descenso-estocastico"><i class="fa fa-check"></i><b>5.1</b> Algoritmo de descenso estocástico</a></li>
<li class="chapter" data-level="5.2" data-path="descenso-estocastico.html"><a href="descenso-estocastico.html#por-que-usar-descenso-estocastico-por-minilotes"><i class="fa fa-check"></i><b>5.2</b> ¿Por qué usar descenso estocástico por minilotes?</a></li>
<li class="chapter" data-level="5.3" data-path="descenso-estocastico.html"><a href="descenso-estocastico.html#escogiendo-la-tasa-de-aprendizaje"><i class="fa fa-check"></i><b>5.3</b> Escogiendo la tasa de aprendizaje</a></li>
<li class="chapter" data-level="5.4" data-path="descenso-estocastico.html"><a href="descenso-estocastico.html#mejoras-al-algoritmo-de-descenso-estocastico."><i class="fa fa-check"></i><b>5.4</b> Mejoras al algoritmo de descenso estocástico.</a><ul>
<li class="chapter" data-level="5.4.1" data-path="descenso-estocastico.html"><a href="descenso-estocastico.html#decaimiento-de-tasa-de-aprendizaje"><i class="fa fa-check"></i><b>5.4.1</b> Decaimiento de tasa de aprendizaje</a></li>
<li class="chapter" data-level="5.4.2" data-path="descenso-estocastico.html"><a href="descenso-estocastico.html#momento"><i class="fa fa-check"></i><b>5.4.2</b> Momento</a></li>
<li class="chapter" data-level="5.4.3" data-path="descenso-estocastico.html"><a href="descenso-estocastico.html#otras-variaciones"><i class="fa fa-check"></i><b>5.4.3</b> Otras variaciones</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html"><i class="fa fa-check"></i><b>6</b> Diagnóstico y mejora de modelos</a><ul>
<li class="chapter" data-level="6.1" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#aspectos-generales"><i class="fa fa-check"></i><b>6.1</b> Aspectos generales</a></li>
<li class="chapter" data-level="6.2" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#que-hacer-cuando-el-desempeno-no-es-satisfactorio"><i class="fa fa-check"></i><b>6.2</b> ¿Qué hacer cuando el desempeño no es satisfactorio?</a></li>
<li class="chapter" data-level="6.3" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#pipeline-de-procesamiento"><i class="fa fa-check"></i><b>6.3</b> Pipeline de procesamiento</a></li>
<li class="chapter" data-level="6.4" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#diagnosticos-sesgo-y-varianza"><i class="fa fa-check"></i><b>6.4</b> Diagnósticos: sesgo y varianza</a></li>
<li class="chapter" data-level="6.5" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#refinando-el-pipeline"><i class="fa fa-check"></i><b>6.5</b> Refinando el pipeline</a></li>
<li class="chapter" data-level="6.6" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#consiguiendo-mas-datos"><i class="fa fa-check"></i><b>6.6</b> Consiguiendo más datos</a></li>
<li class="chapter" data-level="6.7" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#usar-datos-adicionales"><i class="fa fa-check"></i><b>6.7</b> Usar datos adicionales</a></li>
<li class="chapter" data-level="6.8" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#examen-de-modelo-y-analisis-de-errores"><i class="fa fa-check"></i><b>6.8</b> Examen de modelo y Análisis de errores</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html"><i class="fa fa-check"></i><b>7</b> Métodos basados en árboles</a><ul>
<li class="chapter" data-level="7.1" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#arboles-para-regresion-y-clasificacion."><i class="fa fa-check"></i><b>7.1</b> Árboles para regresión y clasificación.</a><ul>
<li class="chapter" data-level="7.1.1" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#arboles-para-clasificacion"><i class="fa fa-check"></i><b>7.1.1</b> Árboles para clasificación</a></li>
<li class="chapter" data-level="7.1.2" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#tipos-de-particion"><i class="fa fa-check"></i><b>7.1.2</b> Tipos de partición</a></li>
<li class="chapter" data-level="7.1.3" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#medidas-de-impureza"><i class="fa fa-check"></i><b>7.1.3</b> Medidas de impureza</a></li>
<li class="chapter" data-level="7.1.4" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#reglas-de-particion-y-tamano-del-arobl"><i class="fa fa-check"></i><b>7.1.4</b> Reglas de partición y tamaño del árobl</a></li>
<li class="chapter" data-level="7.1.5" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#costo---complejidad-breiman"><i class="fa fa-check"></i><b>7.1.5</b> Costo - Complejidad (Breiman)</a></li>
<li class="chapter" data-level="7.1.6" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#opcional-predicciones-con-cart"><i class="fa fa-check"></i><b>7.1.6</b> (Opcional) Predicciones con CART</a></li>
<li class="chapter" data-level="7.1.7" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#arboles-para-regresion"><i class="fa fa-check"></i><b>7.1.7</b> Árboles para regresión</a></li>
<li class="chapter" data-level="7.1.8" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#variabilidad-en-el-proceso-de-construccion"><i class="fa fa-check"></i><b>7.1.8</b> Variabilidad en el proceso de construcción</a></li>
<li class="chapter" data-level="7.1.9" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#relaciones-lineales"><i class="fa fa-check"></i><b>7.1.9</b> Relaciones lineales</a></li>
<li class="chapter" data-level="7.1.10" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#ventajas-y-desventajas-de-arboles"><i class="fa fa-check"></i><b>7.1.10</b> Ventajas y desventajas de árboles</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#bagging-de-arboles"><i class="fa fa-check"></i><b>7.2</b> Bagging de árboles</a><ul>
<li class="chapter" data-level="7.2.1" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#ejemplo-28"><i class="fa fa-check"></i><b>7.2.1</b> Ejemplo</a></li>
<li class="chapter" data-level="7.2.2" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#mejorando-bagging"><i class="fa fa-check"></i><b>7.2.2</b> Mejorando bagging</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#bosques-aleatorios"><i class="fa fa-check"></i><b>7.3</b> Bosques aleatorios</a><ul>
<li class="chapter" data-level="7.3.1" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#sabiduria-de-las-masas"><i class="fa fa-check"></i><b>7.3.1</b> Sabiduría de las masas</a></li>
<li class="chapter" data-level="7.3.2" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#ejemplo-29"><i class="fa fa-check"></i><b>7.3.2</b> Ejemplo</a></li>
<li class="chapter" data-level="7.3.3" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#mas-detalles-de-bosques-aleatorios."><i class="fa fa-check"></i><b>7.3.3</b> Más detalles de bosques aleatorios.</a></li>
<li class="chapter" data-level="7.3.4" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#importancia-de-variables"><i class="fa fa-check"></i><b>7.3.4</b> Importancia de variables</a></li>
<li class="chapter" data-level="7.3.5" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#ajustando-arboles-aleatorios."><i class="fa fa-check"></i><b>7.3.5</b> Ajustando árboles aleatorios.</a></li>
<li class="chapter" data-level="7.3.6" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#ventajas-y-desventajas-de-arboles-aleatorios"><i class="fa fa-check"></i><b>7.3.6</b> Ventajas y desventajas de árboles aleatorios</a></li>
<li class="chapter" data-level="7.3.7" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#tarea-para-23-de-octubre"><i class="fa fa-check"></i><b>7.3.7</b> Tarea (para 23 de octubre)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html"><i class="fa fa-check"></i><b>8</b> Validación de modelos: problemas comunes</a><ul>
<li class="chapter" data-level="8.1" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#filtracion-de-datos"><i class="fa fa-check"></i><b>8.1</b> Filtración de datos</a></li>
<li class="chapter" data-level="8.2" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#series-de-tiempo"><i class="fa fa-check"></i><b>8.2</b> Series de tiempo</a></li>
<li class="chapter" data-level="8.3" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#filtracion-en-el-preprocesamiento"><i class="fa fa-check"></i><b>8.3</b> Filtración en el preprocesamiento</a></li>
<li class="chapter" data-level="8.4" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#uso-de-variables-fuera-de-rango-temporal"><i class="fa fa-check"></i><b>8.4</b> Uso de variables fuera de rango temporal</a></li>
<li class="chapter" data-level="8.5" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#datos-en-conglomerados-y-muestreo-complejo"><i class="fa fa-check"></i><b>8.5</b> Datos en conglomerados y muestreo complejo</a><ul>
<li class="chapter" data-level="" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#ejemplo-32"><i class="fa fa-check"></i>Ejemplo</a></li>
<li class="chapter" data-level="8.5.1" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#censura-y-evaluacion-incompleta"><i class="fa fa-check"></i><b>8.5.1</b> Censura y evaluación incompleta</a></li>
<li class="chapter" data-level="8.5.2" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#ejemplo-tiendas-cerradas"><i class="fa fa-check"></i><b>8.5.2</b> Ejemplo: tiendas cerradas</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#muestras-de-validacion-chicas"><i class="fa fa-check"></i><b>8.6</b> Muestras de validación chicas</a><ul>
<li class="chapter" data-level="" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#ejercicio-5"><i class="fa fa-check"></i>Ejercicio</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#otros-ejemplos"><i class="fa fa-check"></i><b>8.7</b> Otros ejemplos</a></li>
<li class="chapter" data-level="8.8" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#resumen-1"><i class="fa fa-check"></i><b>8.8</b> Resumen</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Publicado con bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Minicurso de verano de Aprendizaje Máquina</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="metodos-basados-en-arboles" class="section level1">
<h1><span class="header-section-number">Sección 7</span> Métodos basados en árboles</h1>
<div id="arboles-para-regresion-y-clasificacion." class="section level2">
<h2><span class="header-section-number">7.1</span> Árboles para regresión y clasificación.</h2>
<p>La idea básica de los árboles es buscar puntos
de cortes en las variables de entrada para
hacer predicciones, ir dividiendo la muestra,
y encontrar cortes sucesivos para refinar las predicciones.</p>
<div id="ejemplo-23" class="section level4 unnumbered">
<h4>Ejemplo</h4>
<p>Buscamos clasificar hogares según su ingreso, usando
como entradas características de los hogares. Podríamos tener,
por ejemplo:</p>
<pre class="sourceCode r"><code class="sourceCode r">knitr<span class="op">::</span><span class="kw">include_graphics</span>(<span class="st">&#39;./imagenes/arboles_1.png&#39;</span>)</code></pre>
<p><img src="imagenes/arboles_1.png" width="496" /></p>
<ul>
<li>Con este árbol podemos clasificar nuevos hogares.</li>
<li>Nótese que los árboles pueden capturar interacciones entre las
variables de entradas. En nuestro ejemplo ficticio, “automóvil” nos
da información acerca del ingreso, pero solo caundo el nivel de educación
del jefe de familia es bajo. (Ejercicio: si el ingreso fuera una cantidad numérica, ¿cómo escribirías este modelo con una suma
de términos que involucren las variables mostradas en el diagrama?)</li>
<li>Los árboles también pueden aproximar relaciones no lineales entre entradas
y variable de salida (es similar a los ejemplos donde haciamos categorización
de variables de entrada).</li>
<li>Igual que en redes neuronales, en lugar de buscar puntos de corte o interacciones
a mano, con los árboles intentamos encontrarlos de manera automática.</li>
</ul>
</div>
<div id="arboles-para-clasificacion" class="section level3">
<h3><span class="header-section-number">7.1.1</span> Árboles para clasificación</h3>
<p>Un árbol particiona el espacio de entradas en rectángulos paralelos a los
ejes, y hace predicciones basadas en un modelo simple dentro de
cada una de esas particiones.</p>
<p>Por ejemplo:</p>
<pre class="sourceCode r"><code class="sourceCode r">knitr<span class="op">::</span><span class="kw">include_graphics</span>(<span class="st">&#39;./imagenes/arboles_2.png&#39;</span>)</code></pre>
<p><img src="imagenes/arboles_2.png" width="540" /></p>
<ul>
<li>El proceso de partición binaria recursiva (con una entrada a la vez)
puede representarse mediante árboles binarios.</li>
<li>Los nodos terminales representan a la partición obtenida.</li>
</ul>
<p>Para definir el proceso de construcción de los árboles, debemos definir:</p>
<ol style="list-style-type: decimal">
<li>¿Cómo escoger las particiones? Idea: buscar hacer los nodos sucesivamente
más puros (que una sola clase domine).</li>
<li>¿Cuándo declarar a un nodo como terminal? ¿Cuándo particionar más profundamente? Idea: dependiendo de la aplicación, buscamos hacer árboles
chicos, o en otras árboles grandes que después podamos para no sobreajustar.</li>
<li>¿Cómo hacer predicciones en nodos terminales? Idea: escoger la clase más
común en cada nodo terminal (la de máxima probabilidad).</li>
</ol>
</div>
<div id="tipos-de-particion" class="section level3">
<h3><span class="header-section-number">7.1.2</span> Tipos de partición</h3>
<p>Supongamos que tenemos variables de entrada <span class="math inline">\((X_1,\ldots, X_p)\)</span>. Recursivamente
particionamos cada nodo escogiendo entre particiones tales que:</p>
<ul>
<li>Dependen de una sola variable de entrada <span class="math inline">\(X_i\)</span></li>
<li>Si <span class="math inline">\(X_i\)</span> es continua, la partición es de la forma <span class="math inline">\(\{X_i\leq c\},\{X_i&gt; c\}\)</span>,
para alguna <span class="math inline">\(c\)</span> (punto de corte)</li>
<li>Si <span class="math inline">\(X_i\)</span> es categórica, la partición es de la forma
<span class="math inline">\(\{X_i\in S\},\{X_i\notin S\}\)</span>, para algún subconjunto <span class="math inline">\(S\)</span> de categorías de <span class="math inline">\(X_i\)</span>.</li>
<li>En cada nodo candidato, escogemos uno de estos cortes para particionar.</li>
</ul>
<p>¿Cómo escogemos la partición en cada nodo? En cada nodo, la partición
se escoge de una manera miope o local, intentando separar las
clases lo mejor que se pueda (sin considerar qué pasa en cortes hechos
más adelante). En un nodo dado, escogemos la partición que
<strong>reduce lo más posible su impureza</strong>.</p>
</div>
<div id="medidas-de-impureza" class="section level3">
<h3><span class="header-section-number">7.1.3</span> Medidas de impureza</h3>
<p>Consideramos un nodo <span class="math inline">\(t\)</span> de un árbol <span class="math inline">\(T\)</span>, y sean <span class="math inline">\(p_1(t),\ldots, p_K(t)\)</span> las
proporciones de casos de <span class="math inline">\(t\)</span> que caen en cada categoría.</p>

<div class="comentario">
La <strong>impureza</strong> de un nodo <span class="math inline">\(t\)</span> está dada por
<span class="math display">\[i(t) = -\sum_{j=1}^K p_j(t)\log p_j(t)\]</span>
Este medida se llama entropía. Hay otras posibilidades como medida
de impureza (por ejemplo, coeficiente de Gini).
</div>

<div id="ejemplo-24" class="section level4">
<h4><span class="header-section-number">7.1.3.1</span> Ejemplo</h4>
<p>Graficamos la medida de impureza para dos clases:</p>
<pre class="sourceCode r"><code class="sourceCode r">impureza &lt;-<span class="st"> </span><span class="cf">function</span>(p){
  <span class="op">-</span>(p<span class="op">*</span><span class="kw">log</span>(p) <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span><span class="op">-</span>p)<span class="op">*</span><span class="kw">log</span>(<span class="dv">1</span><span class="op">-</span>p))
}
<span class="kw">curve</span>(impureza, <span class="dv">0</span>,<span class="dv">1</span>)</code></pre>
<p><img src="07-arboles_files/figure-html/unnamed-chunk-4-1.png" width="480" /></p>
<p>Donde vemos que la máxima impureza se alcanza cuando las proporciones de
clase en un nodo so 50-50, y la mínima impureza (máxima pureza) se alcanza
cuando en el nodo solo hay casos de una clase. Nótese que esta cantidad es proporcional a la devianza del nodo, donde tenemos porbabilidad constante de clase 1 igual a <span class="math inline">\(p\)</span>.</p>
</div>
</div>
<div id="reglas-de-particion-y-tamano-del-arobl" class="section level3">
<h3><span class="header-section-number">7.1.4</span> Reglas de partición y tamaño del árobl</h3>
<p>Podemos escribir la regla de partición, que se aplica a cada nodo de un árbol</p>

<div class="comentario">
<strong>Regla de partición</strong>
En cada nodo, buscamos entre <strong>todas</strong> las variables <span class="math inline">\(X_i\)</span> y <strong>todos</strong>
los puntos de corte <span class="math inline">\(c\)</span> la que da la mayor reducción
de impureza posible (donde la impureza de un corte es el promedio
ponderado por casos de las impurezas de los nodos resultantes).
</div>

<div id="ejemplo-25" class="section level4 unnumbered">
<h4>Ejemplo</h4>
<p>Consideremos un nodo <span class="math inline">\(t\)</span>, cuyos casos de entrenamiento son:</p>
<pre class="sourceCode r"><code class="sourceCode r">n_t &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">200</span>,<span class="dv">100</span>, <span class="dv">150</span>)
impureza &lt;-<span class="st"> </span><span class="cf">function</span>(p){
  <span class="op">-</span><span class="kw">sum</span>(p<span class="op">*</span><span class="kw">log</span>(p))
}
<span class="kw">impureza</span>(n_t<span class="op">/</span><span class="kw">sum</span>(n_t))</code></pre>
<pre><code>## [1] 1.060857</code></pre>
<p>Y comparamos con</p>
<pre class="sourceCode r"><code class="sourceCode r">n_t &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">300</span>,<span class="dv">10</span>, <span class="dv">140</span>)
impureza &lt;-<span class="st"> </span><span class="cf">function</span>(p){
  p &lt;-<span class="st"> </span>p[p<span class="op">&gt;</span><span class="dv">0</span>]
  <span class="op">-</span><span class="kw">sum</span>(p<span class="op">*</span><span class="kw">log</span>(p))
}
<span class="kw">impureza</span>(n_t<span class="op">/</span><span class="kw">sum</span>(n_t))</code></pre>
<pre><code>## [1] 0.7181575</code></pre>
<p>Ahora supongamos que tenemos un posible corte, el primero
resulta en</p>
<pre class="sourceCode r"><code class="sourceCode r">n_t &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">300</span>,<span class="dv">10</span>, <span class="dv">140</span>)
n_<span class="dv">1</span> =<span class="st"> </span><span class="kw">c</span>(<span class="dv">300</span>,<span class="dv">0</span>,<span class="dv">0</span>)
n_<span class="dv">2</span> =<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">10</span>,<span class="dv">140</span>)
(<span class="kw">sum</span>(n_<span class="dv">1</span>)<span class="op">/</span><span class="kw">sum</span>(n_t))<span class="op">*</span><span class="kw">impureza</span>(n_<span class="dv">1</span><span class="op">/</span><span class="kw">sum</span>(n_<span class="dv">1</span>)) <span class="op">+</span><span class="st"> </span>(<span class="kw">sum</span>(n_<span class="dv">2</span>)<span class="op">/</span><span class="kw">sum</span>(n_t))<span class="op">*</span><span class="kw">impureza</span>(n_<span class="dv">2</span><span class="op">/</span><span class="kw">sum</span>(n_<span class="dv">2</span>))</code></pre>
<pre><code>## [1] 0.08164334</code></pre>
<p>Un peor corte es:</p>
<pre class="sourceCode r"><code class="sourceCode r">n_t &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">300</span>,<span class="dv">10</span>, <span class="dv">140</span>)
n_<span class="dv">1</span> =<span class="st"> </span><span class="kw">c</span>(<span class="dv">200</span>,<span class="dv">0</span>,<span class="dv">40</span>)
n_<span class="dv">2</span> =<span class="st"> </span><span class="kw">c</span>(<span class="dv">100</span>,<span class="dv">10</span>,<span class="dv">100</span>)
(<span class="kw">sum</span>(n_<span class="dv">1</span>)<span class="op">/</span><span class="kw">sum</span>(n_t))<span class="op">*</span><span class="kw">impureza</span>(n_<span class="dv">1</span><span class="op">/</span><span class="kw">sum</span>(n_<span class="dv">1</span>)) <span class="op">+</span><span class="st"> </span>(<span class="kw">sum</span>(n_<span class="dv">2</span>)<span class="op">/</span><span class="kw">sum</span>(n_t))<span class="op">*</span><span class="kw">impureza</span>(n_<span class="dv">2</span><span class="op">/</span><span class="kw">sum</span>(n_<span class="dv">2</span>))</code></pre>
<pre><code>## [1] 0.6377053</code></pre>
<p>Lo que resta explicar es qué criterio de paro utilizamos para dejar de particionar.</p>

<div class="comentario">
<p><strong>Regla de paro</strong>
Cuando usemos árboles en ótros métodos, generalmente hay dos opciones:</p>
<ul>
<li>Particionar hasta cierta profundidad fija (por ejemplo, máximo 8 nodos terminales). Este enfoque generalmente usa árboles relativamente chicos (se usa en boosting de árboles).</li>
<li>Dejar de particionar cuando encontramos un número mínimo de casos en un nodo (por ejemplo, 5 o 10 casos). Este enfoque resulta en árboles grandes, probablemente sobreajustados (se usa en bosques aleatorios).</li>
</ul>
<p>Y cuando utilizamos los árboles por sí solos para hacer predicciones:</p>
<ul>
<li>Podemos probar distintos valores de tamaño de árbol, y escogemos por validación (muestra o cruzada) el tamaño final.</li>
<li>Podemos usar el método CART de Breiman, que consiste en construir un árbol
grande y luego podar al tamaño correcto.
</div></li>
</ul>
</div>
<div id="ejemplo-26" class="section level4 unnumbered">
<h4>Ejemplo</h4>
<p>Construímos algunos árboles con los datos de spam:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(rpart)                 
<span class="kw">library</span>(rpart.plot)
<span class="kw">library</span>(ggplot2)
<span class="kw">library</span>(dplyr)
<span class="kw">library</span>(tidyr)
spam_entrena &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&#39;./datos/spam-entrena.csv&#39;</span>)
spam_prueba &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&#39;./datos/spam-prueba.csv&#39;</span>)
<span class="kw">head</span>(spam_entrena)</code></pre>
<pre><code>##   X wfmake wfaddress wfall wf3d wfour wfover wfremove wfinternet wforder
## 1 1   0.00      0.57  0.00    0  0.00      0        0          0    0.00
## 2 2   1.24      0.41  1.24    0  0.00      0        0          0    0.00
## 3 3   0.00      0.00  0.00    0  0.00      0        0          0    0.00
## 4 4   0.00      0.00  0.48    0  0.96      0        0          0    0.48
## 5 5   0.54      0.00  0.54    0  1.63      0        0          0    0.00
## 6 6   0.00      0.00  0.00    0  0.00      0        0          0    0.00
##   wfmail wfreceive wfwill wfpeople wfreport wfaddresses wffree wfbusiness
## 1      0      0.57   0.57     1.15        0           0   0.00       0.00
## 2      0      0.00   0.41     0.00        0           0   0.41       0.00
## 3      0      0.00   0.00     0.00        0           0   0.00       0.00
## 4      0      0.00   0.00     0.00        0           0   0.96       0.96
## 5      0      0.00   0.54     0.00        0           0   0.54       0.54
## 6      0      0.00   0.00     0.00        0           0   0.00       0.00
##   wfemail wfyou wfcredit wfyour wffont wf000 wfmoney wfhp wfhpl wfgeorge
## 1    1.73  3.46        0   1.15      0  0.00    0.00    0     0      0.0
## 2    0.82  3.73        0   1.24      0  0.00    0.41    0     0      0.0
## 3    0.00 12.19        0   4.87      0  0.00    9.75    0     0      0.0
## 4    0.00  1.44        0   0.48      0  0.96    0.00    0     0      0.0
## 5    0.00  2.17        0   5.97      0  0.54    0.00    0     0      0.0
## 6    0.00  5.00        0   0.00      0  0.00    0.00    0     0      2.5
##   wf650 wflab wflabs wftelnet wf857 wfdata wf415 wf85 wftechnology wf1999
## 1     0     0      0        0     0      0     0    0            0      0
## 2     0     0      0        0     0      0     0    0            0      0
## 3     0     0      0        0     0      0     0    0            0      0
## 4     0     0      0        0     0      0     0    0            0      0
## 5     0     0      0        0     0      0     0    0            0      0
## 6     0     0      0        0     0      0     0    0            0      0
##   wfparts wfpm wfdirect wfcs wfmeeting wforiginal wfproject wfre wfedu
## 1       0    0        0    0         0          0         0 0.00     0
## 2       0    0        0    0         0          0         0 0.41     0
## 3       0    0        0    0         0          0         0 0.00     0
## 4       0    0        0    0         0          0         0 0.48     0
## 5       0    0        0    0         0          0         0 0.00     0
## 6       0    0        0    0         0          0         0 0.00     0
##   wftable wfconference cfsc cfpar cfbrack cfexc cfdollar cfpound
## 1       0            0    0 0.000   0.000 0.107    0.000   0.000
## 2       0            0    0 0.065   0.000 0.461    0.527   0.000
## 3       0            0    0 0.000   0.000 0.000    0.000   0.000
## 4       0            0    0 0.133   0.066 0.468    0.267   0.000
## 5       0            0    0 0.000   0.000 0.715    0.318   0.000
## 6       0            0    0 0.000   0.000 0.833    0.000   0.416
##   crlaverage crllongest crltotal spam
## 1      1.421          7       54    1
## 2      3.166         19      114    1
## 3      1.000          1        7    0
## 4      3.315         61      242    1
## 5      2.345         22      129    1
## 6      1.937          8       31    0</code></pre>
<p>Podemos construir un árbol grande. En este caso,
buscamos que los nodos resultantes tengan al menos un caso
y para particionar pedimos que el nodo tenga al menos 10 casos:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">22</span>)
control_completo &lt;-<span class="st"> </span><span class="kw">rpart.control</span>(<span class="dt">cp=</span><span class="dv">0</span>, 
                                  <span class="dt">minsplit=</span><span class="dv">10</span>, 
                                  <span class="dt">minbucket=</span><span class="dv">1</span>, 
                                  <span class="dt">xval=</span><span class="dv">10</span>, 
                                  <span class="dt">maxdepth=</span><span class="dv">30</span>)
spam_tree_completo&lt;-<span class="kw">rpart</span>(spam <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> spam_entrena, <span class="dt">method =</span> <span class="st">&quot;class&quot;</span>,
                          <span class="dt">control =</span> control_completo)
<span class="kw">prp</span>(spam_tree_completo, <span class="dt">type=</span><span class="dv">4</span>, <span class="dt">extra=</span><span class="dv">4</span>)</code></pre>
<pre><code>## Warning: labs do not fit even at cex 0.15, there may be some overplotting</code></pre>
<p><img src="07-arboles_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>Podemos examinar la parte de arriba del árbol:</p>
<pre class="sourceCode r"><code class="sourceCode r">arbol.chico<span class="fl">.1</span> &lt;-<span class="st"> </span><span class="kw">prune</span>(spam_tree_completo, <span class="dt">cp=</span><span class="fl">0.07</span>)
<span class="kw">prp</span>(arbol.chico<span class="fl">.1</span>, <span class="dt">type =</span> <span class="dv">4</span>, <span class="dt">extra =</span> <span class="dv">4</span>)</code></pre>
<p><img src="07-arboles_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>Podemos hacer predicciones con este árbol grande. Por ejemplo, en entrenamiento tenemos:</p>
<pre class="sourceCode r"><code class="sourceCode r">prop &lt;-<span class="st"> </span><span class="kw">predict</span>(spam_tree_completo, <span class="dt">newdata =</span> spam_entrena)
<span class="kw">table</span>(prop[,<span class="dv">2</span>]<span class="op">&gt;</span><span class="fl">0.5</span>, spam_entrena<span class="op">$</span>spam )</code></pre>
<pre><code>##        
##            0    1
##   FALSE 1835   34
##   TRUE    26 1172</code></pre>
<p>y en prueba:</p>
<pre class="sourceCode r"><code class="sourceCode r">prop_arbol_grande &lt;-<span class="st"> </span><span class="kw">predict</span>(spam_tree_completo, <span class="dt">newdata =</span> spam_prueba)
tab_confusion &lt;-<span class="st"> </span><span class="kw">table</span>(prop_arbol_grande[,<span class="dv">2</span>]<span class="op">&gt;</span><span class="fl">0.5</span>, spam_prueba<span class="op">$</span>spam )
<span class="kw">prop.table</span>(tab_confusion, <span class="dv">2</span>)</code></pre>
<pre><code>##        
##                  0          1
##   FALSE 0.90507012 0.11202636
##   TRUE  0.09492988 0.88797364</code></pre>
<p>Y notamos la brecha grande entre prueba y entrenamiento, lo que sugiere sobreajuste. Este árbol es demasiado grande.</p>
</div>
</div>
<div id="costo---complejidad-breiman" class="section level3">
<h3><span class="header-section-number">7.1.5</span> Costo - Complejidad (Breiman)</h3>
<p>Una manera de escoger árboles del tamaño correcto es utilizando una medida inventada
por Breiman para medir la calidad de un árbol. La complejidad
de un árbol <span class="math inline">\(T\)</span> está dada por (para <span class="math inline">\(\alpha\)</span> fija):</p>
<p><span class="math display">\[C_\alpha (T) = \overline{err}(T) + \alpha \vert T\vert\]</span>
donde</p>
<ul>
<li><span class="math inline">\(\overline{err}(T)\)</span> es el error de clasificación de <span class="math inline">\(T\)</span></li>
<li><span class="math inline">\(\vert T\vert\)</span> es el número de nodos terminales del árbol</li>
<li><span class="math inline">\(\alpha&gt;0\)</span> es un parámetro de penalización del tamaño del árbol.</li>
</ul>
<p>Esta medida de complejidad incluye qué tan bien clasifica el árbol
en la muestra de entrenamiento, pero penaliza por el tamaño del árbol.</p>
<p>Para escoger el tamaño del árbol correcto, definimos
<span class="math inline">\(T_\alpha \subset T\)</span> como el subárbol de <span class="math inline">\(T\)</span> que
minimiza la medida <span class="math inline">\(C_\alpha (T_\alpha)\)</span>.</p>
<p>Para entender esta decisión, obsérvese que:</p>
<ul>
<li>Un subárbol grande de <span class="math inline">\(T\)</span> tiene menor valor de <span class="math inline">\(\overline{err}(T)\)</span> (pues usa más cortes)</li>
<li>Pero un subárbol grande de <span class="math inline">\(T\)</span> tiene más penalización por complejidad <span class="math inline">\(\alpha\vert T\vert\)</span>.</li>
</ul>
<p>De modo que para <span class="math inline">\(\alpha\)</span> fija, el árbol <span class="math inline">\(T_\alpha\)</span> hace un
balance entre error de entrenamiento y penalización por complejidad.</p>
<div id="ejemplo-27" class="section level4">
<h4><span class="header-section-number">7.1.5.1</span> Ejemplo</h4>
<p>Podemos ver subárboles más chicos creados durante el procedimiento de división
de nodos (prp está el paquete rpart.plot). En este caso
pondemos <span class="math inline">\(\alpha = 0.2\)</span> (cp = <span class="math inline">\(\alpha\)</span> = complexity parameter):</p>
<pre class="sourceCode r"><code class="sourceCode r">arbol.chico<span class="fl">.1</span> &lt;-<span class="st"> </span><span class="kw">prune</span>(spam_tree_completo, <span class="dt">cp=</span><span class="fl">0.2</span>)
<span class="kw">prp</span>(arbol.chico<span class="fl">.1</span>, <span class="dt">type =</span> <span class="dv">4</span>, <span class="dt">extra =</span> <span class="dv">4</span>)</code></pre>
<p><img src="07-arboles_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>Si disminuimos el coeficiente <span class="math inline">\(\alpha\)</span>.</p>
<pre class="sourceCode r"><code class="sourceCode r">arbol.chico<span class="fl">.1</span> &lt;-<span class="st"> </span><span class="kw">prune</span>(spam_tree_completo, <span class="dt">cp=</span><span class="fl">0.07</span>)
<span class="kw">prp</span>(arbol.chico<span class="fl">.1</span>, <span class="dt">type =</span> <span class="dv">4</span>, <span class="dt">extra =</span> <span class="dv">4</span>)</code></pre>
<p><img src="07-arboles_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>y vemos que en efecto el árbol <span class="math inline">\(T_{0.07}\)</span> contiene al árbol
<span class="math inline">\(T_{0.2}\)</span>, y ambos son subárboles del árbol gigante que construimos al principio.</p>

<div class="comentario">
Para podar un árbol con costo-complejidad, encontramos para
cada <span class="math inline">\(\alpha&gt;0\)</span> (coeficiente de complejidad) un árbol
<span class="math inline">\(T_\alpha\subset T\)</span> que minimiza el costo-complejidad. Esto resulta
en una sucesión de árboles
<span class="math inline">\(T_0\subset T_1\subset T_2\subset \cdots T_m\subset T\)</span>,
de donde podemos escoger con validación el árbol óptimo.
</div>

<p><em>Nota</em>: Esto es un teorema que hace falta demostrar: el resultado
principal es que conforme aumentamos <span class="math inline">\(\alpha\)</span>, vamos eliminiando
ramas del árbol, de manera que los</p>
<pre class="sourceCode r"><code class="sourceCode r">arbol.chico<span class="fl">.1</span> &lt;-<span class="st"> </span><span class="kw">prune</span>(spam_tree_completo, <span class="dt">cp=</span><span class="fl">0.05</span>)
<span class="kw">prp</span>(arbol.chico<span class="fl">.1</span>, <span class="dt">type =</span> <span class="dv">4</span>, <span class="dt">extra =</span> <span class="dv">4</span>)</code></pre>
<p><img src="07-arboles_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<pre class="sourceCode r"><code class="sourceCode r">arbol.chico<span class="fl">.1</span> &lt;-<span class="st"> </span><span class="kw">prune</span>(spam_tree_completo, <span class="dt">cp=</span><span class="fl">0.02</span>)
<span class="kw">prp</span>(arbol.chico<span class="fl">.1</span>, <span class="dt">type =</span> <span class="dv">4</span>, <span class="dt">extra =</span> <span class="dv">4</span>)</code></pre>
<p><img src="07-arboles_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">source</span>(<span class="st">&#39;./scripts/fancyRpartPlot.R&#39;</span>)
<span class="kw">fancyRpartPlot</span>(arbol.chico<span class="fl">.1</span>, <span class="dt">sub=</span><span class="st">&#39;&#39;</span>)</code></pre>
<pre><code>## Loading required package: RColorBrewer</code></pre>
<p><img src="07-arboles_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p><strong>Nota</strong>: Enfoques de predicción basados en un solo árbol para
clasificación y regresión son típicamente superados en
predicción por otros métodos. ¿Cuál crees que sea la razón? ¿Es un
problema de varianza o sesgo?</p>
</div>
</div>
<div id="opcional-predicciones-con-cart" class="section level3">
<h3><span class="header-section-number">7.1.6</span> (Opcional) Predicciones con CART</h3>
<p>Podemos hacer predicciones con un sólo árbol. En el caso de spam, haríamos</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">9293</span>) <span class="co"># para hacer reproducible la validación cruzada</span>
spam_tree &lt;-<span class="kw">rpart</span>(spam <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> spam_entrena, 
                  <span class="dt">method =</span> <span class="st">&quot;class&quot;</span>, <span class="dt">control=</span><span class="kw">list</span>(<span class="dt">cp=</span><span class="dv">0</span>, 
                                              <span class="dt">minsplit=</span><span class="dv">5</span>,<span class="dt">minbucket=</span><span class="dv">1</span>))</code></pre>
<p>Ahora mostramos los resultados de cada árbol para cada
valor de <span class="math inline">\(\alpha\)</span>. La siguiente función nos da una estimación
de validación cruzada del error:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">printcp</span>(spam_tree)</code></pre>
<pre><code>## 
## Classification tree:
## rpart(formula = spam ~ ., data = spam_entrena, method = &quot;class&quot;, 
##     control = list(cp = 0, minsplit = 5, minbucket = 1))
## 
## Variables actually used in tree construction:
##  [1] cfbrack      cfdollar     cfexc        cfpar        cfsc        
##  [6] crlaverage   crllongest   crltotal     wf1999       wf3d        
## [11] wf650        wfaddress    wfall        wfbusiness   wfconference
## [16] wfcredit     wfdata       wfdirect     wfedu        wfemail     
## [21] wffont       wffree       wfgeorge     wfhp         wfhpl       
## [26] wfinternet   wflabs       wfmail       wfmake       wfmeeting   
## [31] wfmoney      wforder      wforiginal   wfour        wfover      
## [36] wfpeople     wfpm         wfproject    wfre         wfreceive   
## [41] wfremove     wfreport     wftechnology wfwill       wfyou       
## [46] wfyour       X           
## 
## Root node error: 1206/3067 = 0.39322
## 
## n= 3067 
## 
##            CP nsplit rel error  xerror     xstd
## 1  0.49087894      0  1.000000 1.00000 0.022431
## 2  0.13681592      1  0.509121 0.54975 0.018903
## 3  0.05223881      2  0.372305 0.44942 0.017516
## 4  0.03980100      3  0.320066 0.34163 0.015659
## 5  0.03150912      4  0.280265 0.30514 0.014922
## 6  0.01160862      5  0.248756 0.28275 0.014436
## 7  0.01077944      6  0.237148 0.27612 0.014286
## 8  0.00663350      7  0.226368 0.25954 0.013901
## 9  0.00497512      9  0.213101 0.24046 0.013436
## 10 0.00414594     18  0.166667 0.21227 0.012701
## 11 0.00331675     20  0.158375 0.21144 0.012679
## 12 0.00276396     24  0.145108 0.20481 0.012496
## 13 0.00248756     27  0.136816 0.19320 0.012167
## 14 0.00165837     31  0.126036 0.18740 0.011997
## 15 0.00130301     44  0.104478 0.18408 0.011899
## 16 0.00124378     52  0.092869 0.18657 0.011973
## 17 0.00118455     54  0.090381 0.18740 0.011997
## 18 0.00110558     61  0.082090 0.18740 0.011997
## 19 0.00082919     67  0.075456 0.18823 0.012022
## 20 0.00066335    100  0.048093 0.19569 0.012238
## 21 0.00041459    107  0.043118 0.19652 0.012262
## 22 0.00033167    121  0.037313 0.20896 0.012611
## 23 0.00031095    126  0.035655 0.21144 0.012679
## 24 0.00027640    140  0.029851 0.21393 0.012746
## 25 0.00020730    146  0.028192 0.21393 0.012746
## 26 0.00010365    150  0.027363 0.21725 0.012836
## 27 0.00000000    158  0.026534 0.21725 0.012836</code></pre>
<p>Y usamos la regla de mínimo error o a una desviación estándar
del error mínimo:</p>
<pre class="sourceCode r"><code class="sourceCode r">arbol_podado &lt;-<span class="st"> </span><span class="kw">prune</span>(spam_tree, <span class="dt">cp =</span>  <span class="fl">0.00130301</span>)
<span class="kw">prp</span>(arbol_podado)</code></pre>
<p><img src="07-arboles_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p>Cuyo error de predicción es:</p>
<pre class="sourceCode r"><code class="sourceCode r">prop_arbol_podado &lt;-<span class="st"> </span><span class="kw">predict</span>(arbol_podado, <span class="dt">newdata=</span>spam_prueba)
<span class="kw">head</span>(prop_arbol_podado)</code></pre>
<pre><code>##            0        1
## 1 0.02578797 0.974212
## 2 0.02578797 0.974212
## 3 0.03703704 0.962963
## 4 0.12500000 0.875000
## 5 0.02578797 0.974212
## 6 0.02578797 0.974212</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">prop.table</span>(<span class="kw">table</span>((prop_arbol_podado[,<span class="dv">2</span>]<span class="op">&gt;</span><span class="fl">0.5</span>),spam_prueba<span class="op">$</span>spam),<span class="dv">2</span>)</code></pre>
<pre><code>##        
##                  0          1
##   FALSE 0.94282632 0.12191104
##   TRUE  0.05717368 0.87808896</code></pre>
</div>
<div id="arboles-para-regresion" class="section level3">
<h3><span class="header-section-number">7.1.7</span> Árboles para regresión</h3>
<p>Para problemas de regresión, el criterio de pureza y la predicción
en cada nodo terminal es diferente:</p>
<ul>
<li>En los nodos terminales usamos el promedio los casos de entrenamiento que caen en tal nodo (en lugar de la clase más común)</li>
<li>La impureza de define como varianza: si <span class="math inline">\(t\)</span> es un nodo, su impureza está dada por <span class="math inline">\(\frac{1}{n(t)}\sum (y - m)^2\)</span>, donde la suma es sobre los casos que están en el nodo y <span class="math inline">\(m\)</span> es la media de las <span class="math inline">\(y\)</span>’s del nodo.</li>
</ul>
</div>
<div id="variabilidad-en-el-proceso-de-construccion" class="section level3">
<h3><span class="header-section-number">7.1.8</span> Variabilidad en el proceso de construcción</h3>
<p>Existe variabilidad considerable en el proceso de división, lo cual
es una debilidad de los árboles. Por ejemplo:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">9923</span>)
muestra<span class="fl">.1</span> &lt;-<span class="st"> </span>spam_entrena[<span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(spam_entrena), <span class="kw">nrow</span>(spam_entrena), <span class="dt">replace=</span>T), ]
spam.tree.completo<span class="fl">.1</span> &lt;-<span class="kw">rpart</span>(spam <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span>  muestra<span class="fl">.1</span>, <span class="dt">method =</span> <span class="st">&quot;class&quot;</span>,
                          <span class="dt">control =</span> control_completo)
arbol.chico<span class="fl">.1</span> &lt;-<span class="st"> </span><span class="kw">prune</span>(spam.tree.completo<span class="fl">.1</span>, <span class="dt">cp=</span><span class="fl">0.03</span>)
<span class="kw">prp</span>(arbol.chico<span class="fl">.1</span>, <span class="dt">type =</span> <span class="dv">4</span>, <span class="dt">extra =</span> <span class="dv">4</span>)</code></pre>
<p><img src="07-arboles_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<pre class="sourceCode r"><code class="sourceCode r">muestra<span class="fl">.1</span> &lt;-<span class="st"> </span>spam_entrena[<span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(spam_entrena), <span class="kw">nrow</span>(spam_entrena), <span class="dt">replace=</span>T), ]
spam.tree.completo<span class="fl">.1</span> &lt;-<span class="kw">rpart</span>(spam <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span>  muestra<span class="fl">.1</span>, <span class="dt">method =</span> <span class="st">&quot;class&quot;</span>,
                          <span class="dt">control =</span> control_completo)
arbol.chico<span class="fl">.1</span> &lt;-<span class="st"> </span><span class="kw">prune</span>(spam.tree.completo<span class="fl">.1</span>, <span class="dt">cp=</span><span class="fl">0.03</span>)
<span class="kw">prp</span>(arbol.chico<span class="fl">.1</span>, <span class="dt">type =</span> <span class="dv">4</span>, <span class="dt">extra =</span> <span class="dv">4</span>)</code></pre>
<p><img src="07-arboles_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<p>Pequeñas diferencias en la muestra de entrenamiento produce
distintas selecciones de variables y puntos de corte, y estructuras
de árboles muchas veces distintas. Esto introduce varianza considerable
en las predicciones.</p>
</div>
<div id="relaciones-lineales" class="section level3">
<h3><span class="header-section-number">7.1.9</span> Relaciones lineales</h3>
<p>Los árboles pueden requerir ser muy grandes para estimar apropiadamente
relaciones lineales.</p>
<pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dv">200</span>,<span class="dv">0</span>,<span class="dv">1</span>)
y &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">*</span>x <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">200</span>,<span class="dv">0</span>,<span class="fl">0.1</span>)
arbol &lt;-<span class="st"> </span><span class="kw">rpart</span>(y<span class="op">~</span>x, <span class="dt">data=</span><span class="kw">data_frame</span>(<span class="dt">x=</span>x, <span class="dt">y=</span>y), <span class="dt">method =</span> <span class="st">&#39;anova&#39;</span>)
x_pred &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.05</span>)
y_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(arbol, <span class="dt">newdata =</span> <span class="kw">data_frame</span>(<span class="dt">x=</span>x_pred))
y_verdadera &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">*</span>x_pred
dat &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">x_pred=</span>x_pred, <span class="dt">y_pred=</span>y_pred, <span class="dt">y_verdadera=</span>y_verdadera) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">gather</span>(y, valor, y_pred<span class="op">:</span>y_verdadera)
<span class="kw">ggplot</span>(dat, <span class="kw">aes</span>(<span class="dt">x=</span>x_pred, <span class="dt">y=</span>valor, <span class="dt">colour=</span>y)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>()</code></pre>
<p><img src="07-arboles_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
</div>
<div id="ventajas-y-desventajas-de-arboles" class="section level3">
<h3><span class="header-section-number">7.1.10</span> Ventajas y desventajas de árboles</h3>
<p>Ventajas:</p>
<ol style="list-style-type: decimal">
<li>Árboles chicos son fáciles de explicar e interpretar</li>
<li>Capturan interacciones entre las variables de entrada</li>
<li>Son robustos en el sentido de que</li>
</ol>
<ul>
<li>valores numéricos atípicos no hacen fallar al método</li>
<li>no es necesario transformar variables</li>
<li>hay formas fáciles de lidiar con datos faltantes (cortes sucedáneos)</li>
</ul>
<ol start="4" style="list-style-type: decimal">
<li>Se ajustan rápidamente yson relativamente fáciles de interpretar (por ejemplo, son útiles para clasificar en campo)</li>
<li>Árboles grandes generalmente no sufren de sesgo.</li>
</ol>
<p>Desventajas:</p>
<ol style="list-style-type: decimal">
<li>Tienen dificultades en capturar estructuras lineales</li>
<li>En la interpretación, tienen la dificultad de que muchas veces
algunas variables de entrada “enmascaran” a otras. Que una variable de entrada
no esté en el árbol no quiere decir que no sea “importante” para predecir
(regresión ridge lidia mejor con esto).</li>
<li>Son inestables (varianza alta) por construcción: es local/miope, basada
en cortes duros si/no. Esto produce desempeño predictivo relativamente malo
(p ej: una pequeña diferencia en cortes iniciales puede resultar en estructuras
de árbol totalmente distintas).</li>
<li>Adicoinalmente, no son apropiados cuando hay variables categóricas con
muchas niveles: en estos casos, el árbol sobreajusta desde los primeros
cortes, y las predicciones son malas.</li>
</ol>
</div>
</div>
<div id="bagging-de-arboles" class="section level2">
<h2><span class="header-section-number">7.2</span> Bagging de árboles</h2>
<p>Bosques aleatorios es un método de predicción que utiliza familias de
árboles para hacer predicciones.</p>
<p>Los árboles grandes tienen la ventaja de tener sesgo bajo, pero sufren de varianza alta. Podemos explotar el sesgo bajo si logramos controlar la varianza. Una idea primera para lograr esto es es hacer
<strong>bagging</strong> de árboles:</p>
<ul>
<li>Perturbar la muestra de entrenamiento de distintas maneras y producir árboles distintos (grandes). La perturbación más usada es tomar muestras bootstrap de los datos y ajustar un árbol a cada muestra bootstrap</li>
<li>Promediar el resultado de todos estos árboles para hacer predicciones. El proceso de promediar reduce la varianza, sin tener pérdidas en sesgo.</li>
</ul>
<p>La idea básica de bagging (<em>bootstrap aggregation</em>) es la siguiente:</p>
<p>Consideramos el proceso <span class="math inline">\({\mathcal L} \to T_{\mathcal L}\)</span>, que representa
el proceso de ajuste de un árbol <span class="math inline">\(T_{\mathcal L}\)</span> a partir de la muestra de entrenamiento <span class="math inline">\({\mathcal L}\)</span>. Si pudiéramos obtener
distintas muestras de entrenamiento
<span class="math display">\[{\mathcal L}_1, {\mathcal L}_2, \ldots, {\mathcal L}_B,\]</span>
y supongamos que construimos los árboles (que suponemos de regresión)
<span class="math display">\[T_1, T_2, \ldots, T_B,\]</span>
Podríamos mejorar nuestras predicciones construyendo el
árbol promedio
<span class="math display">\[T(x) = \frac{1}{B}\sum_{i=b}^B  T_b (x)\]</span>
¿Por qué es mejor este árbol promedio que cualquiera de sus componentes? Veamos primero el sesgo. El valor esperado del árbol
promedio es
<span class="math display">\[E[T(x)] = \frac{1}{B}\sum_{i=b}^B  E[T_b (x)]\]</span>
y como cada <span class="math inline">\(T_b(x)\)</span> se construye de la misma manera a partir
de <span class="math inline">\({\mathcal L}_b\)</span>, y todas las muestras <span class="math inline">\({\mathcal L}_b\)</span> se
extraen de la misma forma, todos los términos de la suma de la derecha son iguales:
<span class="math display">\[E[T(x)] =  E[T_1 (x)],\]</span>
lo que implica que el sesgo del promedio es igual al sesgo de
un solo árbol (que es bajo, pues suponemos que los árboles son grandes).</p>
<p>Ahora veamos la varianza. Como las muestras <span class="math inline">\({\mathcal L}_b\)</span> se extraen <em>de manera independiente</em>, entonces</p>
<p><span class="math display">\[Var[T(x)] = Var\left( \frac{1}{B}\sum_{i=b}^B  T_b (x)\right) = \frac{1}{B^2}\sum_{i=b}^B  Var[T_b (x)],\]</span>
pues los distintos <span class="math inline">\(T_b(x)\)</span> no están correlacionados (en ese caso, varianza
de la suma es la suma de las varianzas), y las constantes salen de la varianza
al cuadrado. Por las mismas razones que arriba, todos los términos de la derecha son iguales, y
<span class="math display">\[Var[T(x)] = \frac{1}{B}\ Var[T_1 (x)]\]</span>
de modo que la varianza del árbol promedio es mucho más chica
que la varianza de un árbol dado (si <span class="math inline">\(B\)</span> es grande).</p>
<p>Sin embargo, no podemos tomar muestras de entrenamiento repetidamente
para ajustar estos árboles. ¿Cómo podemos simular extraer distintas
muestras de entrenamiento?</p>

<div class="comentario">
<p>Sabemos que si tenemos una muestra de entrenamiento fija <span class="math inline">\({\mathcal L}\)</span>, podemos evaluar la variación
de esta muestra tomando <strong>muestras bootstrap</strong> de <span class="math inline">\({\mathcal L}\)</span>,
que denotamos por</p>
<p><span class="math display">\[{\mathcal L}_1^*, {\mathcal L}_2^*, \ldots, {\mathcal L}_B^*,\]</span></p>
Recordatorio: una muestra bootstrap de <span class="math inline">\(\mathcal L\)</span> es una muestra con con reemplazo
de <span class="math inline">\({\mathcal L}\)</span> del mismo tamaño que <span class="math inline">\({\mathcal L}\)</span>.
</div>

<p>Entonces la idea es que construimos los árboles (que suponemos de regresión)
<span class="math display">\[T_1^*, T_2^*, \ldots, T_B^*,\]</span>
podríamos mejorar nuestras predicciones construyendo el
árbol promedio
<span class="math display">\[T^*(x) = \frac{1}{B}\sum_{i=b}^B  T_b^* (x)\]</span>
para suavizar la variación de cada árbol individual.</p>
<p>El argumento del sesgo aplica en este caso, pero el de la varianza no
exactamente, pues las muestras bootstrap no son independientes (están correlacionadas a través de la muestra de entrenamiento de donde se obtuvieron),a pesar de que las muestras bootstrap se extraen de manera independiente de <span class="math inline">\({\mathcal L}\)</span>. De esta forma, no esperamos una
reducción de varianza tan grande como en el caso de muestras independientes.</p>

<div class="comentario">
<p><strong>Bagging</strong>
Sea <span class="math inline">\({\mathcal L} =\{(x^{(i)}, y^{(i)})\}_{i=1}^n\)</span> una muestra de entrenamiento, y sean
<span class="math display">\[{\mathcal L}_1^*, {\mathcal L}_2^*, \ldots, {\mathcal L}_B^*,\]</span>
muestras bootstrap de <span class="math inline">\({\mathcal L}\)</span> (muestreamos con reemplazo
los <strong>pares</strong> <span class="math inline">\((x^{(i)}, y^{(i)})\)</span>, para obtener una muestra de tamaño <span class="math inline">\(n\)</span>).</p>
<ol style="list-style-type: decimal">
<li>Para cada muestra bootstrap construimos un árbol
<span class="math display">\[{\mathcal L}_b^* \to T_b^*\]</span>.</li>
<li>(Regresión) Promediamos árboles para reducir varianza
<span class="math display">\[T^*(x) = \frac{1}{B}\sum_{i=b}^B  T_b^*(x)\]</span></li>
<li>(Clasificación) Tomamos votos sobre todos los árboles:
<span class="math display">\[T^*(x) = argmax_g \{ \# \{i|T_b^*(x)=g\}\}.\]</span> Podemos
también calcular probabilidades promedio sobre todos
los árboles.</li>
</ol>
Bagging muchas veces reduce el error de predicción gracias
a una reducción modesta de varianza.
</div>

<p><strong>Nota</strong>: No hay garantía de bagging reduzca el error de entrenamiento, especialmente si los árboles base son muy
malos clasificadores ¿Puedes pensar en un ejemplo donde empeora?</p>
<div id="ejemplo-28" class="section level3">
<h3><span class="header-section-number">7.2.1</span> Ejemplo</h3>
<p>Probemos con el ejemplo de spam. Construimos árboles con muestras bootstrap
de los datos originales de entrenamiento:</p>
<pre class="sourceCode r"><code class="sourceCode r">muestra_bootstrap &lt;-<span class="st"> </span><span class="cf">function</span>(df){
  df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">sample_n</span>(<span class="kw">nrow</span>(df), <span class="dt">replace =</span> <span class="ot">TRUE</span>)
}
arboles_bagged &lt;-<span class="st"> </span><span class="kw">lapply</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">30</span>, <span class="cf">function</span>(i){
  muestra &lt;-<span class="st"> </span><span class="kw">muestra_bootstrap</span>(spam_entrena)
  arbol &lt;-<span class="st"> </span><span class="kw">rpart</span>(spam <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> muestra, 
                  <span class="dt">method =</span> <span class="st">&quot;class&quot;</span>, <span class="dt">control=</span><span class="kw">list</span>(<span class="dt">cp=</span><span class="dv">0</span>, 
                                              <span class="dt">minsplit=</span><span class="dv">5</span>,<span class="dt">minbucket=</span><span class="dv">1</span>))
  arbol
})</code></pre>
<p>Examinemos la parte de arriba de algunos de estos árboles:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">prp</span>(<span class="kw">prune</span>(arboles_bagged[[<span class="dv">1</span>]], <span class="dt">cp =</span><span class="fl">0.01</span>))</code></pre>
<p><img src="07-arboles_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">prp</span>(<span class="kw">prune</span>(arboles_bagged[[<span class="dv">2</span>]], <span class="dt">cp =</span><span class="fl">0.01</span>))</code></pre>
<p><img src="07-arboles_files/figure-html/unnamed-chunk-32-2.png" width="672" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">prp</span>(<span class="kw">prune</span>(arboles_bagged[[<span class="dv">3</span>]], <span class="dt">cp =</span><span class="fl">0.01</span>))</code></pre>
<p><img src="07-arboles_files/figure-html/unnamed-chunk-32-3.png" width="672" />
Ahora probemos hacer predicciones con los 30 árboles:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(purrr)
preds_clase &lt;-<span class="st"> </span><span class="kw">lapply</span>(arboles_bagged, <span class="cf">function</span>(arbol){
  preds &lt;-<span class="st"> </span><span class="kw">predict</span>(arbol, <span class="dt">newdata =</span> spam_prueba)[,<span class="dv">2</span>]
})
preds &lt;-<span class="st"> </span>preds_clase <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">reduce</span>(cbind)
<span class="kw">dim</span>(preds)</code></pre>
<pre><code>## [1] 1534   30</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">prop_bagging &lt;-<span class="st"> </span><span class="kw">apply</span>(preds, <span class="dv">1</span>, mean)
<span class="kw">prop.table</span>(<span class="kw">table</span>(prop_bagging <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, spam_prueba<span class="op">$</span>spam),<span class="dv">2</span>)</code></pre>
<pre><code>##        
##                  0          1
##   FALSE 0.96224380 0.09555189
##   TRUE  0.03775620 0.90444811</code></pre>
<p>Y vemos que tenemos una mejora inmediata con respecto un sólo árbol
grande (tanto un árbol grande como uno podado con costo-complejidad).
El único costo es el cómputo adicional para procesar las muestras bootstrap</p>

<div class="comentario">
<ul>
<li>¿Cuántas muestras bootstrap? Bagging generalmente funciona mejor
cuando tomamos tantas muestras como sea razonable - aunque también es un
parámetro que se puede afinar.</li>
<li>Bagging por sí solo se usa rara vez. El método más potente es bosques aleatorios, donde el proceso
básico es bagging de árboles, pero añadimos ruido adicional en la
construcción de árboles.
</div></li>
</ul>
</div>
<div id="mejorando-bagging" class="section level3">
<h3><span class="header-section-number">7.2.2</span> Mejorando bagging</h3>
<p>El factor que limita la mejora de desempeño de bagging es que
los árboles están correlacionados a través de la muestra de entrenamiento. Como
vimos, si los árboles fueran independientes, entonces mejoramos por un factor
de <span class="math inline">\(B\)</span> (número de muestras independientes). Veamos un argumento para entender
cómo esa correlación limita las mejoras:</p>
<p>Quiséramos calcular (para una <span class="math inline">\(x\)</span> fija)</p>
<p><span class="math display">\[Var(T(x)) = Var\left(\frac{1}{B}\sum_{i=1}^B T^*_i\right)\]</span></p>
<p>donde cada <span class="math inline">\(T^*_i\)</span> se construye a partir de una muestra bootstrap de <span class="math inline">\({\mathcal L}\)</span>.
Nótese que esta varianza es sobre la muestra de entrenamiento <span class="math inline">\({\mathcal L}\)</span>. Usando
la fórmula de la varianza para sumas generales:
<span class="math display" id="eq:varianza-ensamble">\[\begin{equation}
Var(T(x)) = Var\left(\frac{1}{B}\sum_{i=1}^B T^*_i\right) =
\sum_{i=1}^B \frac{1}{B^2} Var(T^*_i(x)) + \frac{2}{B^2}\sum_{i &lt; j} Cov(T_i^*, T_j^*)
  \tag{7.1}
\end{equation}\]</span></p>
<p>Ponemos ahora</p>
<p><span class="math display">\[\sigma^2(x) = Var(T_i^*)\]</span>
que son todas iguales porque los árboles bootstrap se extraen de la misma manera (<span class="math inline">\({\mathcal L}\to {\mathcal L}^*\to T^*\)</span>).</p>
<p>Escribimos ahora
<span class="math display">\[\rho(x) = corr(T_i^*, T_j^*)\]</span>
que es una correlación sobre <span class="math inline">\({\mathcal L}\)</span> (asegúrate que entiendes este término). Todas
estas correlaciones son iguales pues cada par de árboles se construye de la misma forma.</p>
<p>Así que la fórmula <a href="metodos-basados-en-arboles.html#eq:varianza-ensamble">(7.1)</a> queda</p>
<span class="math display" id="eq:varianza-ensamble-2">\[\begin{equation}
Var(T(x)) = 
 \frac{1}{B} \sigma^2(x) + \frac{B-1}{B} \rho(x)\sigma^2(x) =
 \sigma^2(x)\left(\frac{1}{B}  + \left(1-\frac{1}{B}\right )\rho(x)     \right)
  \tag{7.2}
\end{equation}\]</span>
<p>En el límite (cuando B es muy grande, es decir, promediamos muchos árboles):</p>
<span class="math display" id="eq:varianza-ensamble-3">\[\begin{equation}
Var(T(x)) = Var\left(\frac{1}{B}\sum_{i=1}^B T^*_i\right) \approx
 \sigma^2(x)\rho(x)     
  \tag{7.3}
\end{equation}\]</span>
<p>Si <span class="math inline">\(\rho(x)=0\)</span> (árboles no correlacionados), la varianza del ensemble
es la fracción <span class="math inline">\(1/B\)</span> de la varianza de un solo árbol, y obtenemos una
mejora considerable en varianza. En el otro extremo,
si la correlación es alta <span class="math inline">\(\rho(x)\approx 1\)</span>, entonces no obtenemos ganancias
por promediar árboles y la varianza del ensamble es similar a la de un solo árbol.</p>

<div class="comentario">
<ul>
<li>Cuando hacemos bagging de árboles, la limitación de mejora cuando promediamos
muchos árboles está dada por la correlación entre ellos: cuanto más grande
es la correlación, menor beneficio en reducción de varianza obtenemos.</li>
<li>Si alteramos el proceso para producir árboles menos correlacionados (menor <span class="math inline">\(\rho(x)\)</span>), podemos
mejorar el desempeño de bagging. Sin embargo, estas alteraciones generalmente
están acompañadas de incrementos en la varianza (<span class="math inline">\(\sigma^x(x)\)</span>).
</div></li>
</ul>
</div>
</div>
<div id="bosques-aleatorios" class="section level2">
<h2><span class="header-section-number">7.3</span> Bosques aleatorios</h2>
<p>Los bosques aleatorios son una versión de árboles de bagging decorrelacionados. Esto
se logra <em>introduciendo variabilidad en la construcción de los árboles</em> (esto es
paradójico - pero la explicación está arriba: aunque la varianza empeora
(de cada árbol), la decorrelación de árboles puede valer la pena).</p>
<div id="sabiduria-de-las-masas" class="section level3">
<h3><span class="header-section-number">7.3.1</span> Sabiduría de las masas</h3>
<p>Una explicación simple de este proceso que se cita frecuentemente es el fenómeno de
la sabiduría de las masas: cuando promediamos estimaciones pobres de un gran número
de personas (digamos ignorantes), obtenemos mejores estimaciones que cualquiera de
las componentes individuales, o incluso mejores que estimaciones de expertos. Supongamos
por ejemplo que <span class="math inline">\(G_1,G_2,\ldots, G_M\)</span> son clasificadores débiles, por ejemplo
<span class="math display">\[P(correcto) = P(G_i=G)=0.6\]</span>
para un problema con probabilidad base <span class="math inline">\(P(G=1)=0.5\)</span>. Supongamos que los predictores
son independientes, y sea <span class="math inline">\(G^*\)</span> el clasificador que se construye por mayoría
de votos a partir de <span class="math inline">\(G_1,G_2,\ldots, G_M\)</span>, es decir
<span class="math inline">\(G^*=1\)</span> si y sólo si <span class="math inline">\(\#\{ G_i = 1\} &gt; M/2\)</span>.</p>
<p>Podemos ver que el número de aciertos (X) de <span class="math inline">\(G_1,G_2,\ldots, G_M\)</span>, por independencia,
es binomial <span class="math inline">\(Bin(M, 0.6)\)</span>. Si <span class="math inline">\(M\)</span> es grande, podemos aproximar esta distribución
con una normal con media <span class="math inline">\(M*0.6\)</span> y varianza <span class="math inline">\(0.6*0.4*M\)</span>. Esto implica que</p>
<p><span class="math display">\[P(G^* correcto)=P(X &gt; 0.5M) \approx 
P\left( Z &gt; \frac{0.5M-0.6M}{\sqrt(0.24M)}\right) = P\left(Z &gt; -2.041 \sqrt{M}\right)\]</span></p>
<p>Y ahora observamos que cuando <span class="math inline">\(M\)</span> es grande, la cantidad de la derecha tiende a 1:
la masa, en promedio, tiene la razón!</p>
<p>Nótese, sin embargo, que baja dependencia entre las “opiniones” es parte crucial del argumento,
es decir, las opiniones deben estar decorrelacionadas.</p>
<hr />
<p>El proceso de decorrelación de bosques aleatorios consiste en que cada vez que tengamos que hacer
un corte en un árbol de bagging, escoger al azar un número de variables y usar estas
para buscar la mejor variable y el mejor punto de corte, como hicimos en la construcción
de árboles.</p>
<p><strong>Bosques aleatorios</strong>
Sea <span class="math inline">\(m\)</span> fija.
Sea <span class="math inline">\({\mathcal L} =\{(x^{(i)}, y^{(i)})\}_{i=1}^n\)</span> una muestra de entrenamiento, y sean
<span class="math display">\[{\mathcal L}_1^*, {\mathcal L}_2^*, \ldots, {\mathcal L}_B^*,\]</span>
muestras bootstrap de <span class="math inline">\({\mathcal L}\)</span> (muestreamos con reemplazo
los <strong>pares</strong> <span class="math inline">\((x^{(i)}, y^{(i)})\)</span>, para obtener una muestra de tamaño <span class="math inline">\(n\)</span>).</p>
<ol style="list-style-type: decimal">
<li>Para cada muestra bootstrap construimos un árbol
<span class="math display">\[{\mathcal L}_b^* \to T_b^*\]</span> de la siguiente forma:</li>
</ol>
<ul>
<li>En cada nodo candidato a particionar, escogemos al azar <span class="math inline">\(m\)</span> variables de las disponibles</li>
<li>Buscamos la mejor variable y punto de corte (como en un árbol normal) pero <em>solo entre
las variables que seleccionamos al azar</em>.</li>
<li>Seguimos hasta construir un árbol grande.</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>(Regresión) Promediamos árboles para reducir varianza
<span class="math display">\[T^*(x) = \frac{1}{B}\sum_{i=b}^B  T_b^*(x)\]</span></li>
<li>(Clasificación) Tomamos votos sobre todos los árboles:
<span class="math display">\[T^*(x) = argmax_g \{ \# \{i|T_b^*(x)=g\}\}.\]</span> Podemos
también calcular probabilidades promedio sobre todos
los árboles.</li>
</ol>
<p>Bosques aleatorios muchas veces reduce el error de predicción gracias
a una reducción a veces considerable de varianza. El objetivo final es reducir
la varianza alta que producen árboles normales debido a la forma tan agresiva
de construir sus cortes.</p>
<p><strong>Observaciones</strong>
1. El número de variables <span class="math inline">\(m\)</span> que se seleccionan en cada nodo es un parámetro
que hay que escoger (usando validación, validación cruzada).
2. Ojo: no se selecciona un conjunto de <span class="math inline">\(m\)</span> variables para cada árbol. En la construcción
de cada árbol, en cada nodo se seleccionan <span class="math inline">\(m\)</span> variables como candidatas para cortes.
3. Como inducimos aleatoriedad en la construcción de árboles,
este proceso reduce la correlación entre árboles del bosque, aunque también incrementa
su varianza. Los bosques aleatorios funcionan bien cuando la mejora en correlación
es más grande que la pérdida en varianza.
4. Reducir <span class="math inline">\(m\)</span>, a grandes rasgos:
- Aumenta el sesgo del bosque (pues es más restringido el proceso de construcción)
- Disminuye la correlación entre árboles y aumenta la varianza de cada árbol
5. Intrementar <span class="math inline">\(m\)</span>
- Disminuye el sesgo del bosque (menos restricción)
- Aumenta la correlacción entre árobles y disminuye la varianza de cada árbol</p>
</div>
<div id="ejemplo-29" class="section level3">
<h3><span class="header-section-number">7.3.2</span> Ejemplo</h3>
<p>Regresamos a nuestro ejemplo de spam. Intentemos con 500 árboles, y
6 variables (de 58 variables) para escoger como candidatos en cada corte:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(randomForest)
bosque_spam &lt;-<span class="kw">randomForest</span>(<span class="kw">factor</span>(spam) <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> spam_entrena, 
                           <span class="dt">ntree =</span> <span class="dv">1500</span>, <span class="dt">mtry =</span> <span class="dv">6</span>, <span class="dt">importance=</span><span class="ot">TRUE</span>)</code></pre>
<p>Evaluamos desempeño, donde vemos que obtenemos una mejora inmediata con respecto
a bagging:</p>
<pre class="sourceCode r"><code class="sourceCode r">probas &lt;-<span class="st"> </span><span class="kw">predict</span>(bosque_spam, <span class="dt">newdata =</span> spam_prueba, <span class="dt">type=</span><span class="st">&#39;prob&#39;</span>)
<span class="kw">head</span>(probas)</code></pre>
<pre><code>##             0         1
## 1 0.009333333 0.9906667
## 2 0.016666667 0.9833333
## 3 0.070000000 0.9300000
## 4 0.398666667 0.6013333
## 5 0.045333333 0.9546667
## 6 0.028666667 0.9713333</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">prop_bosque &lt;-<span class="st"> </span>probas[,<span class="dv">2</span>]
<span class="kw">table</span>(prop_bosque<span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, spam_prueba<span class="op">$</span>spam) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">prop.table</span>(<span class="dv">2</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">round</span>(<span class="dv">3</span>)</code></pre>
<pre><code>##        
##             0     1
##   FALSE 0.971 0.092
##   TRUE  0.029 0.908</code></pre>
<p>Comparemos las curvas ROC para:</p>
<ul>
<li>árbol grande sin podar</li>
<li>árbol podado con costo-complejidad</li>
<li>bagging de árboles</li>
<li>bosque aleatorio</li>
</ul>
<p>Las curvas de precision-recall</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ROCR)
pred_arbol &lt;-<span class="st"> </span><span class="kw">prediction</span>(prop_arbol_grande[,<span class="dv">2</span>], spam_prueba<span class="op">$</span>spam)
pred_podado &lt;-<span class="st"> </span><span class="kw">prediction</span>(prop_arbol_podado[,<span class="dv">2</span>], spam_prueba<span class="op">$</span>spam)
pred_bagging &lt;-<span class="st"> </span><span class="kw">prediction</span>(prop_bagging, spam_prueba<span class="op">$</span>spam)
pred_bosque &lt;-<span class="st"> </span><span class="kw">prediction</span>(prop_bosque, spam_prueba<span class="op">$</span>spam)
preds_roc &lt;-<span class="st"> </span><span class="kw">list</span>(pred_arbol, pred_podado, pred_bagging, pred_bosque)
perfs &lt;-<span class="st"> </span><span class="kw">lapply</span>(preds_roc, <span class="cf">function</span>(pred){
  <span class="kw">performance</span>(pred, <span class="dt">x.measure =</span> <span class="st">&#39;prec&#39;</span>, <span class="dt">measure =</span> <span class="st">&#39;rec&#39;</span>)
})
<span class="kw">plot</span>(perfs[[<span class="dv">1</span>]],  <span class="dt">lwd=</span><span class="dv">2</span>)
<span class="kw">plot</span>(perfs[[<span class="dv">2</span>]], <span class="dt">add=</span><span class="ot">TRUE</span>, <span class="dt">col=</span><span class="st">&#39;orange&#39;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)
<span class="kw">plot</span>(perfs[[<span class="dv">3</span>]], <span class="dt">add=</span><span class="ot">TRUE</span>, <span class="dt">col=</span><span class="st">&#39;gray&#39;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)
<span class="kw">plot</span>(perfs[[<span class="dv">4</span>]], <span class="dt">add=</span><span class="ot">TRUE</span>, <span class="dt">col=</span><span class="st">&#39;purple&#39;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</code></pre>
<p><img src="07-arboles_files/figure-html/unnamed-chunk-39-1.png" width="672" /></p>
<p>O las curvas ROC</p>
<pre class="sourceCode r"><code class="sourceCode r">perfs &lt;-<span class="st"> </span><span class="kw">lapply</span>(preds_roc, <span class="cf">function</span>(pred){
  <span class="kw">performance</span>(pred, <span class="dt">x.measure =</span> <span class="st">&#39;fpr&#39;</span>, <span class="dt">measure =</span> <span class="st">&#39;sens&#39;</span>)
})
<span class="kw">plot</span>(perfs[[<span class="dv">1</span>]],  <span class="dt">lwd=</span><span class="dv">2</span>)
<span class="kw">plot</span>(perfs[[<span class="dv">2</span>]], <span class="dt">add=</span><span class="ot">TRUE</span>, <span class="dt">col=</span><span class="st">&#39;orange&#39;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)
<span class="kw">plot</span>(perfs[[<span class="dv">3</span>]], <span class="dt">add=</span><span class="ot">TRUE</span>, <span class="dt">col=</span><span class="st">&#39;gray&#39;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)
<span class="kw">plot</span>(perfs[[<span class="dv">4</span>]], <span class="dt">add=</span><span class="ot">TRUE</span>, <span class="dt">col=</span><span class="st">&#39;purple&#39;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</code></pre>
<p><img src="07-arboles_files/figure-html/unnamed-chunk-40-1.png" width="672" /></p>
<hr />
</div>
<div id="mas-detalles-de-bosques-aleatorios." class="section level3">
<h3><span class="header-section-number">7.3.3</span> Más detalles de bosques aleatorios.</h3>
<p>Los bosques aleatorios, por su proceso de construcción, tienen aspectos interesantes.</p>
<p>En primer lugar, tenemos la estimación de error de prueba <strong>Out-of-Bag</strong> (OOB), que
es una estimación honesta del error de predicción basada en el proceso de bagging.</p>
<p>Obsérvese en primer lugar, que cuando tomamos muestras con reemplazo para construir
cada árbol, algunos casos de entrenamiento aparecen más de una vez, y otros
casos no se usan en la construcción del árbol. La idea es entonces es usar esos
casos excluidos para hacer una estimación honesta del error.</p>
<div id="ejemplo-30" class="section level4 unnumbered">
<h4>Ejemplo</h4>
<p>Si tenemos una muestra de entrenamiento</p>
<pre class="sourceCode r"><code class="sourceCode r">entrena &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">x=</span><span class="dv">1</span><span class="op">:</span><span class="dv">10</span>, <span class="dt">y=</span><span class="kw">rnorm</span>(<span class="dv">10</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>, <span class="dv">5</span>))
entrena</code></pre>
<pre><code>## # A tibble: 10 x 2
##        x       y
##    &lt;int&gt;   &lt;dbl&gt;
##  1     1   0.936
##  2     2  -2.84 
##  3     3  -5.63 
##  4     4  -2.21 
##  5     5   7.04 
##  6     6   7.09 
##  7     7  13.0  
##  8     8  -0.423
##  9     9  10.4  
## 10    10   5.03</code></pre>
<p>Tomamos una muestra bootstrap:</p>
<pre class="sourceCode r"><code class="sourceCode r">entrena_boot &lt;-<span class="st"> </span><span class="kw">sample_n</span>(entrena, <span class="dv">10</span>, <span class="dt">replace =</span> <span class="ot">TRUE</span>)
entrena_boot</code></pre>
<pre><code>## # A tibble: 10 x 2
##        x       y
##    &lt;int&gt;   &lt;dbl&gt;
##  1     7  13.0  
##  2    10   5.03 
##  3     2  -2.84 
##  4     8  -0.423
##  5     2  -2.84 
##  6     1   0.936
##  7     4  -2.21 
##  8    10   5.03 
##  9     8  -0.423
## 10     8  -0.423</code></pre>
<p>Construimos un predictor</p>
<pre class="sourceCode r"><code class="sourceCode r">mod_boot &lt;-<span class="st"> </span><span class="kw">lm</span>(y<span class="op">~</span>x, <span class="dt">data =</span> entrena_boot)</code></pre>
<p>y ahora obtenemos los datos que no se usaron:</p>
<pre class="sourceCode r"><code class="sourceCode r">prueba_boot &lt;-<span class="st"> </span><span class="kw">anti_join</span>(entrena, entrena_boot)</code></pre>
<pre><code>## Joining, by = c(&quot;x&quot;, &quot;y&quot;)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">prueba_boot</code></pre>
<pre><code>## # A tibble: 4 x 2
##       x      y
##   &lt;int&gt;  &lt;dbl&gt;
## 1     3  -5.63
## 2     5   7.04
## 3     6   7.09
## 4     9  10.4</code></pre>
<p>y usamos estos tres casos para estimar el error de predicción:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(<span class="kw">abs</span>(<span class="kw">predict</span>(mod_boot, prueba_boot)<span class="op">-</span>prueba_boot<span class="op">$</span>y))</code></pre>
<pre><code>## [1] 5.936181</code></pre>
<p>Esta es la estimación OOB (out-of-bag) para este modelo particular.</p>
<hr />
<p>En un principio podemos pensar que quizá por mala suerte obtenemos
pocos elementos OOB para evaluar el error, pero en realidad para muestras
no tan chicas obtenemos una fracción considerable.</p>

<div class="comentario">
Cuando el tamaño de muestra <span class="math inline">\(n\)</span> es grande, el porcentaje esperado de casos que
no están en la muestra bootstrap es alrededor del 37%
</div>

<p>Demuestra usando probabilidad y teoría de muestras con reemplazo.</p>

<div class="comentario">
<p><strong>Estimación OOB del error</strong></p>
<p>Consideramos un bosque aleatorio <span class="math inline">\(T_{ba}\)</span>con árboles <span class="math inline">\(T_1^*, T_2^*, \ldots, T_B^*\)</span>, y conjunto de
entrenamiento original
<span class="math inline">\({\mathcal L} =\{(x^{(i)}, y^{(i)}\}_{i=1}^n\)</span>.
Para cada caso de entrenamiento <span class="math inline">\((x^{(i)}, y^{(i)})\)</span> consideramos todos los árboles
que <strong>no</strong> usaron este caso para construirse, y construimos un bosque <span class="math inline">\(T_{ba}^{(i)}\)</span>
basado solamente en esos árboles.
La predicción OOB de <span class="math inline">\(T_{ba}^{(i)}\)</span> para <span class="math inline">\((x^{(i)}, y^{(i)})\)</span> es
<span class="math display">\[y_{oob}^{(i)} = T_{ba}^{(i)}(x^{(i)})\]</span>
El error OOB del árbol <span class="math inline">\(T_{ba}\)</span> está dado por
1. Regresión (error cuadrático medio)
<span class="math display">\[\hat{Err}_{oob} = \frac{1}{n} \sum_{i=1}^n (y^{(i)} - y_{oob}^{(i)})^2\]</span>
2. Clasificación (error de clasificación)
<span class="math display">\[\hat{Err}_{oob} = \frac{1}{n}\sum_{i=1}^n I(y^{(i)} = y_{oob}^{(i)})\]</span></p>
</div>

<ul>
<li>Para cada dato de entrenamiento, hacemos predicciones usando solamente los árboles
que no consideraron ese dato en su construcción. Estas predicciones son las que evaluamos</li>
<li>Es una especie de validación cruzada (se puede demostrar que es similar a
validacion cruzada leave-one-out), pero es barata en términos computacionales.</li>
<li>Como discutimos en validación cruzada, esto hace de OOB una buena medida de error para
afinar los parámetros del modelo (principalmente el número <span class="math inline">\(m\)</span> de variables que se escogen
en cada corte).</li>
</ul>
</div>
<div id="ejempo" class="section level4 unnumbered">
<h4>Ejempo</h4>
<p>Para el ejemplo de spam, podemos ver el error OOB ( y matriz de confusión también OOB):</p>
<pre class="sourceCode r"><code class="sourceCode r">bosque_spam</code></pre>
<pre><code>## 
## Call:
##  randomForest(formula = factor(spam) ~ ., data = spam_entrena,      ntree = 1500, mtry = 6, importance = TRUE) 
##                Type of random forest: classification
##                      Number of trees: 1500
## No. of variables tried at each split: 6
## 
##         OOB estimate of  error rate: 4.96%
## Confusion matrix:
##      0    1 class.error
## 0 1807   54  0.02901666
## 1   98 1108  0.08126036</code></pre>
<p>Que comparamos con</p>
<pre class="sourceCode r"><code class="sourceCode r">probas &lt;-<span class="st"> </span><span class="kw">predict</span>(bosque_spam, <span class="dt">newdata =</span> spam_prueba, <span class="dt">type=</span><span class="st">&#39;prob&#39;</span>)
prop_bosque &lt;-<span class="st"> </span>probas[,<span class="dv">2</span>]
tab &lt;-<span class="st"> </span><span class="kw">table</span>(prop_bosque<span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, spam_prueba<span class="op">$</span>spam) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">prop.table</span>(<span class="dv">2</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">round</span>(<span class="dv">3</span>)
<span class="dv">1</span><span class="op">-</span><span class="kw">diag</span>(tab)</code></pre>
<pre><code>## [1] 0.029 0.092</code></pre>
<p>Podemos comparar con el cálculo de entrenamiento, que como sabemos típicamente
es una mala estimación del error de predicción:</p>
<pre class="sourceCode r"><code class="sourceCode r">probas &lt;-<span class="st"> </span><span class="kw">predict</span>(bosque_spam, <span class="dt">newdata =</span> spam_entrena, <span class="dt">type=</span><span class="st">&#39;prob&#39;</span>)
prop_bosque &lt;-<span class="st"> </span>probas[,<span class="dv">2</span>]
<span class="kw">table</span>(prop_bosque<span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, spam_entrena<span class="op">$</span>spam)</code></pre>
<pre><code>##        
##            0    1
##   FALSE 1861   11
##   TRUE     0 1195</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">tab &lt;-<span class="st"> </span><span class="kw">table</span>(prop_bosque<span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, spam_entrena<span class="op">$</span>spam) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">prop.table</span>(<span class="dv">2</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">round</span>(<span class="dv">3</span>)
<span class="dv">1</span><span class="op">-</span><span class="kw">diag</span>(tab)</code></pre>
<pre><code>## [1] 0.000 0.009</code></pre>
<p>Podemos también monitorear el error OOB conforme agregamos más árboles. Esta gráfica
es útil para entender qué tanto esta mejorando el bosque dependiendo del número de árboles:</p>
<pre class="sourceCode r"><code class="sourceCode r">err_spam &lt;-<span class="st"> </span>bosque_spam<span class="op">$</span>err.rate <span class="op">%&gt;%</span><span class="st"> </span>as_data_frame <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">ntrees =</span> <span class="kw">row_number</span>()) 
<span class="kw">head</span>(err_spam)</code></pre>
<pre><code>## # A tibble: 6 x 4
##      OOB    `0`   `1` ntrees
##    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;int&gt;
## 1 0.113  0.0851 0.159      1
## 2 0.113  0.0853 0.160      2
## 3 0.109  0.0786 0.158      3
## 4 0.101  0.0744 0.143      4
## 5 0.102  0.0715 0.149      5
## 6 0.0972 0.0712 0.137      6</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">err_spam &lt;-<span class="st"> </span>err_spam <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">gather</span>(métrica, valor, <span class="op">-</span>ntrees)
<span class="kw">ggplot</span>(err_spam, <span class="kw">aes</span>(<span class="dt">x=</span>ntrees, <span class="dt">y=</span>valor, <span class="dt">colour=</span>métrica)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>()</code></pre>
<p><img src="07-arboles_files/figure-html/unnamed-chunk-51-1.png" width="672" /></p>
<p>Además de la estimación OOB del error de clasificación, en la gráfica están las estimaciones OOB
dada cada una de las clases (probabilidad de clasificar correctamente dada la clase: en
problemas binarios son tasa de falsos positivos y tasa de falsos negativos).</p>
<hr />
</div>
</div>
<div id="importancia-de-variables" class="section level3">
<h3><span class="header-section-number">7.3.4</span> Importancia de variables</h3>
<p>Usando muestras bootstrap y error OOB, es posible tener mediciones útiles de la
importancia de una variable en el modelo en un bosque aleatorio (todo esto también
fue inventado por Breiman).</p>
<p>En primer lugar, consideremos qué significa que una variable sea importante
desde el punto predictivo en un modelo. Podemos considerar, por ejemplo:</p>
<ul>
<li><em>Si quitamos una variable, y el error de predicción se degrada, la variable es importante</em>.
Este no es un muy buen enfoque, porque muchas veces tenemos conjuntos de variables correlacionadas.
Aún cuando una variable influya en la predicción, si la quitamos, otras
variable pueden hacer su trabajo, y el modelo no se degrada mucho (piensa
en regresión, en donde incluso esta variable eliminada puede tener un coeficiente grande e influir
mucho en la predicción). También requiere ajustar modelos adicionales.</li>
<li><em>Si las predicciones cambian mucho cuando una variable cambia, entonces la variable es importante</em>.
Este concepto funciona mejor, al menos desde el punto de vista predictivo. Su defecto es que debemos decidir qué cambios queremos medir. Si el modelo es simple
(por ejemplo, lineal), entonces es relativamente fácil usar cambios marginales.
Pero en modelos no lineales cambios marginales no necesariamente evalúan
correctamente el efecto de la variable sobre las predicciones. La situación
se complica adicionalmente si hay interacciones con otras variables, lo que
típicamente sucede en métodos basados en árbles.</li>
</ul>
<p>La idea de Breiman, que intenta atender estas observaciones, es como sigue:</p>
<p>Consideramos un árbol <span class="math inline">\(T^*_j\)</span> del bosque, con muestra bootstrap <span class="math inline">\({\mathcal L}^*_i\)</span>. Calculamos
un tipo de error out-of-bag para el árbol, promediando sobre todos los elementos de <span class="math inline">\({\mathcal L}\)</span> que no están en <span class="math inline">\({\mathcal L}^*_i\)</span></p>
<p><span class="math display">\[\widehat{Err}_{oob}(T^*_j) = \frac{1}{A_j}\sum_{(x^{(i)}, y^{(i)}) \in {\mathcal L} -{\mathcal L}^*_i} L(y^{(i)}, T^*_j(x^{(i)}))\]</span>
donde <span class="math inline">\(A_j\)</span> es el tamaño de <span class="math inline">\({\mathcal L} -{\mathcal L}^*_i\)</span>.</p>
<p>Ahora <em>permutamos</em> al azar la variable <span class="math inline">\(X_k\)</span> en la muestra OOB <span class="math inline">\({\mathcal L} -{\mathcal L}^*_i\)</span>.
Describimos esta operación como <span class="math inline">\(x^{(i)} \to x^{(i)}_k\)</span>.
Calculamos el error nuevamente:</p>
<p><span class="math display">\[\widehat{Err}_{k}(T^*_j) = \frac{1}{A_j}\sum_{(x^{(i)}, y^{(i)}) \in {\mathcal L} -{\mathcal L}^*_i} L(y^{(i)}, T^*_j(x_k^{(i)}))\]</span>
Ahora calculamos la degradación del error out-of-bag debido a la permutación:
<span class="math display">\[ D_k(T_j^*) = \widehat{Err}_{k}(T^*_j) - \widehat{Err}_{oob}(T^*_j) \]</span></p>
<p>Y promediamos sobre el bosque entero
<span class="math display">\[I_k =\frac{1}{B} \sum_{j=1}^B D_k(T^*_j)\]</span>
y a esta cantidad le llamamos la <strong>importancia</strong> (basada en permutaciones) de la variable <span class="math inline">\(k\)</span> en el bosque
aleatorio. Es el decremento promedio de capacidad predictiva cuando “quitamos” la variable
<span class="math inline">\(X_k\)</span>.</p>
<p>Nótese que:</p>
<ul>
<li>No podemos “quitar” la variable durante el entrenamiento de los árboles, pues entonces otras variables
pueden hacer su trabajo, subestimando su importancia.</li>
<li>No podemos “quitar” la variable al medir el error OOB, pues se necesitan todas las
variables para poder clasificar con cada árbol (pues cada árbol usa esa variable, o tiene
probabilidad de usarla).</li>
<li>Pero podemos permutar a la hora calcular el error OOB (y no durante el entrenamiento),
rompiendo la relación que
hay entre <span class="math inline">\(X_k\)</span> y la variable respuesta.</li>
<li>Aunque podríamos usar esta medida para árboles, no es muy buena idea por el problema
de “enmascaramiento”. Este problema se aminora en los bosques aleatorios pues todas las variables tienen oportunidad de aportar cortes en ausencia de otras variables.</li>
</ul>
<p>Otra manera de medir importancia para árboles de regresión y clasificación es mediante el decremento de impureza promedio
sobre el bosque, para cada variable.</p>
<ul>
<li>Cada vez que una variable aporta un corte en un árbol, la impureza del árbol
disminuye.</li>
<li>Sumamos, en cada árbol, todos estos decrementos de impureza (cada vez que aparece
la variable en un corte)</li>
<li>Finalmente, promediamos esta medida de importancia dentro de cada árbol sobre
el bosque completo.</li>
<li>Repetimos para cada variable.</li>
</ul>
<p>Para árboles de clasificación, usualmente se toma la importancia de Gini, que está basada in la impureza de Gini
en lugar de la entropía. La impureza de Gini está dada por
<span class="math display">\[I_G(p_1, \ldots, p_K) = \sum_{k=1}^K p_k(1-p_k),\]</span>
que es similar a la impureza de entropía que discutimos en la construcción
de árboles:</p>
<p><span class="math display">\[I_G(p_1, \ldots, p_K) = \sum_{k=1}^K -p_k\log (p_k),\]</span>
Nótese por ejemplo que ambas toman su valor máximo en <span class="math inline">\(p_k=1/K\)</span> (distribución más
uniforme posible sobre las clases), y que son iguales a cero cuando <span class="math inline">\(p_k=1\)</span> para
alguna <span class="math inline">\(k\)</span>.
#### Ejemplo{-}
En nuestro ejemplo de spam</p>
<pre class="sourceCode r"><code class="sourceCode r">imp &lt;-<span class="st"> </span><span class="kw">importance</span>(bosque_spam, <span class="dt">type=</span><span class="dv">1</span>)
importancia_df &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">variable =</span> <span class="kw">rownames</span>(imp), <span class="dt">MeanDecreaseAccuracy =</span> imp[,<span class="dv">1</span>]) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(MeanDecreaseAccuracy))
importancia_df</code></pre>
<pre><code>## # A tibble: 58 x 2
##    variable   MeanDecreaseAccuracy
##    &lt;chr&gt;                     &lt;dbl&gt;
##  1 cfexc                      73.2
##  2 wfremove                   63.1
##  3 crlaverage                 63.0
##  4 cfdollar                   60.3
##  5 wfhp                       55.2
##  6 crllongest                 54.7
##  7 wffree                     54.3
##  8 crltotal                   52.0
##  9 wfedu                      47.8
## 10 wfyour                     45.3
## # ... with 48 more rows</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">importancia_df &lt;-<span class="st"> </span>importancia_df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">variable =</span> <span class="kw">reorder</span>(variable, MeanDecreaseAccuracy))
<span class="kw">ggplot</span>(importancia_df , <span class="kw">aes</span>(<span class="dt">x=</span>variable, <span class="dt">y=</span> MeanDecreaseAccuracy)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">coord_flip</span>()</code></pre>
<p><img src="07-arboles_files/figure-html/unnamed-chunk-52-1.png" width="672" /></p>
<p><strong>Observación</strong>: en el paquete <em>randomForest</em>, las importancias están escaladas
por su la desviación estándar sobre los árboles - la idea es que puedan ser
interpretados como valores-<span class="math inline">\(z\)</span> (estandarizados). En este caso, nos podríamos
fijar en importancias que están por arriba de <span class="math inline">\(2\)</span>, por ejemplo. Para obtener
los valores no estandarizados (y ver la degradación en desempeño directamente)
podemos calcular</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">importance</span>(bosque_spam, <span class="dt">type =</span> <span class="dv">1</span>, <span class="dt">scale =</span> <span class="ot">FALSE</span>)</code></pre>
<pre><code>##              MeanDecreaseAccuracy
## X                   -3.398063e-04
## wfmake               7.820918e-04
## wfaddress            1.509922e-03
## wfall                3.607607e-03
## wf3d                 6.865265e-05
## wfour                1.415974e-02
## wfover               2.559785e-03
## wfremove             3.466735e-02
## wfinternet           5.113536e-03
## wforder              1.229169e-03
## wfmail               2.388153e-03
## wfreceive            4.081920e-03
## wfwill               3.701128e-03
## wfpeople             6.623610e-04
## wfreport             6.582026e-04
## wfaddresses          8.486423e-04
## wffree               2.377365e-02
## wfbusiness           5.742176e-03
## wfemail              2.207889e-03
## wfyou                1.151620e-02
## wfcredit             3.088054e-03
## wfyour               1.909754e-02
## wffont               1.687250e-03
## wf000                1.358799e-02
## wfmoney              1.171880e-02
## wfhp                 3.220300e-02
## wfhpl                1.321378e-02
## wfgeorge             1.677690e-02
## wf650                3.714641e-03
## wflab                1.205268e-03
## wflabs               2.968365e-03
## wftelnet             1.252834e-03
## wf857                5.362343e-04
## wfdata               8.375305e-04
## wf415                4.832454e-04
## wf85                 2.268768e-03
## wftechnology         1.603492e-03
## wf1999               6.902904e-03
## wfparts              7.131246e-05
## wfpm                 1.255157e-03
## wfdirect             6.650049e-04
## wfcs                 5.034624e-04
## wfmeeting            3.209452e-03
## wforiginal           8.825843e-04
## wfproject            6.864723e-04
## wfre                 4.000573e-03
## wfedu                1.049319e-02
## wftable              2.721772e-05
## wfconference         4.790163e-04
## cfsc                 1.527800e-03
## cfpar                4.051610e-03
## cfbrack              9.434229e-04
## cfexc                3.979771e-02
## cfdollar             3.244195e-02
## cfpound              9.215076e-04
## crlaverage           2.784469e-02
## crllongest           3.617719e-02
## crltotal             3.151068e-02</code></pre>
<hr />
</div>
<div id="ajustando-arboles-aleatorios." class="section level3">
<h3><span class="header-section-number">7.3.5</span> Ajustando árboles aleatorios.</h3>
<ul>
<li>El parámetro más importante de afinar es usualmente <span class="math inline">\(m\)</span>, el número de variables que se escogen
al azar en cada nodo.</li>
<li>A veces podemos obtener algunas ventajas de afinar el número mínimo de observaciones por
nodo terminal y/o el número mínimo de observaciones por nodo para considerar hacer cortes adicionales</li>
<li>Usualmente corremos tantos árboles como podamos (cientos, miles), o hasta que se
estabiliza el error. Aumentar más arboles rara vez producen sobreajuste adicional (aunque esto no
quiere decir que los bosques aleatorios no puedan sobreajustar!)</li>
</ul>
<div id="ejemplo-31" class="section level4 unnumbered">
<h4>Ejemplo</h4>
<p>Consideremos datos de (casas en venta en Ames, Iowa)[<a href="https://ww2.amstat.org/publications/jse/v19n3/decock.pdf" class="uri">https://ww2.amstat.org/publications/jse/v19n3/decock.pdf</a>]. Queremos
predecir el precio listado de una casa en función de las características de las casa.</p>
<p>El análisis completo (desde limpieza y exploración) está en scripts/bosque-housing.Rmd</p>
</div>
</div>
<div id="ventajas-y-desventajas-de-arboles-aleatorios" class="section level3">
<h3><span class="header-section-number">7.3.6</span> Ventajas y desventajas de árboles aleatorios</h3>
<p>Ventajas:</p>
<ul>
<li>Entre los métodos estándar (off-the shelf), son quizá el
mejor método: tienen excelentes tasas de error de predicción.</li>
<li>Los bosques aleatorios son relativamente fáciles de entrenar (usualmente 1 o 2 parámetros)
y rápidos de ajustar.</li>
<li>Heredan las ventajas de los árboles: no hay necesidad de transformar variables o construir interacciones (pues los árboles pueden descubrirlas), son robustos a valores atípicos.</li>
<li>Igual que con los árboles, las predicciones de los bosques siempre están en el rango
de las variables de predicción (no extrapolan)</li>
</ul>
<p>Desventajas:
- Pueden ser lentos en la predicción, pues muchas veces
requieren evaluar grandes cantidades
de árboles.
- No es tan simple adaptarlos a distintos tipos de problemas (por ejemplo, como
redes neuronales, que combinando capas podemos construir modelos ad-hoc a problemas
particulares).
- La falta de extrapolación puede ser también un defecto (por ejemplo, cuando hay
una estructura lineal aproximada).</p>
</div>
<div id="tarea-para-23-de-octubre" class="section level3">
<h3><span class="header-section-number">7.3.7</span> Tarea (para 23 de octubre)</h3>
<ul>
<li>Las instrucciones están en <em>scripts/tarea_arboles_bosques.Rmd</em></li>
</ul>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="diagnostico-y-mejora-de-modelos.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="validacion-de-modelos-problemas-comunes.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/felipegonzalez/aprendizaje-maquina-verano-2018/edit/master/07-arboles.Rmd",
"text": "Edit"
},
"download": ["am-curso-verano.pdf", "am-curso-verano.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
